{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nfrom dataclasses import dataclass, field\nfrom typing import List, Literal, Optional\n\n\n@dataclass\nclass FreezeArguments:\n    r\n\n    freeze_trainable_layers: int = field(\n        default=2,\n        metadata={\n            \"help\": (\n                \"The number of trainable layers for freeze (partial-parameter) fine-tuning. \"\n                \"Positive numbers mean the last n layers are set as trainable, \"\n                \"negative numbers mean the first n layers are set as trainable.\"\n            )\n        },\n    )\n    freeze_trainable_modules: str = field(\n        default=\"all\",\n        metadata={\n            \"help\": (\n                \"Name(s) of trainable modules for freeze (partial-parameter) fine-tuning. \"\n                \"Use commas to separate multiple modules. \"\n                \"Use `all` to specify all the available modules.\"\n            )\n        },\n    )\n    freeze_extra_modules: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Name(s) of modules apart from hidden layers to be set as trainable \"\n                \"for freeze (partial-parameter) fine-tuning. \"\n                \"Use commas to separate multiple modules.\"\n            )\n        },\n    )\n\n\n@dataclass\nclass LoraArguments:\n    r\n\n    additional_target: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Name(s) of modules apart from LoRA layers to be set as trainable \"\n                \"and saved in the final checkpoint. \"\n                \"Use commas to separate multiple modules.\"\n            )\n        },\n    )\n    lora_alpha: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The scale factor for LoRA fine-tuning (default: lora_rank * 2).\"},\n    )\n    lora_dropout: float = field(\n        default=0.0,\n        metadata={\"help\": \"Dropout rate for the LoRA fine-tuning.\"},\n    )\n    lora_rank: int = field(\n        default=8,\n        metadata={\"help\": \"The intrinsic dimension for LoRA fine-tuning.\"},\n    )\n    lora_target: str = field(\n        default=\"all\",\n        metadata={\n            \"help\": (\n                \"Name(s) of target modules to apply LoRA. \"\n                \"Use commas to separate multiple modules. \"\n                \"Use `all` to specify all the linear modules.\"\n            )\n        },\n    )\n    loraplus_lr_ratio: Optional[float] = field(\n        default=None,\n        metadata={\"help\": \"LoRA plus learning rate ratio (lr_B / lr_A).\"},\n    )\n    loraplus_lr_embedding: float = field(\n        default=1e-6,\n        metadata={\"help\": \"LoRA plus learning rate for lora embedding layers.\"},\n    )\n    use_rslora: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether or not to use the rank stabilization scaling factor for LoRA layer.\"},\n    )\n    use_dora: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether or not to use the weight-decomposed lora method (DoRA).\"},\n    )\n    pissa_init: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether or not to initialize a PiSSA adapter.\"},\n    )\n    pissa_iter: int = field(\n        default=16,\n        metadata={\"help\": \"The number of iteration steps performed by FSVD in PiSSA. Use -1 to disable it.\"},\n    )\n    pissa_convert: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether or not to convert the PiSSA adapter to a normal LoRA adapter.\"},\n    )\n    create_new_adapter: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether or not to create a new adapter with randomly initialized weight.\"},\n    )\n\n\n@dataclass\nclass RLHFArguments:\n    r\n\n    pref_beta: float = field(\n        default=0.1,\n        metadata={\"help\": \"The beta parameter in the preference loss.\"},\n    )\n    pref_ftx: float = field(\n        default=0.0,\n        metadata={\"help\": \"The supervised fine-tuning loss coefficient in DPO training.\"},\n    )\n    pref_loss: Literal[\"sigmoid\", \"hinge\", \"ipo\", \"kto_pair\", \"orpo\", \"simpo\"] = field(\n        default=\"sigmoid\",\n        metadata={\"help\": \"The type of DPO loss to use.\"},\n    )\n    dpo_label_smoothing: float = field(\n        default=0.0,\n        metadata={\"help\": \"The robust DPO label smoothing parameter in cDPO that should be between 0 and 0.5.\"},\n    )\n    kto_chosen_weight: float = field(\n        default=1.0,\n        metadata={\"help\": \"The weight factor of the desirable losses in KTO training.\"},\n    )\n    kto_rejected_weight: float = field(\n        default=1.0,\n        metadata={\"help\": \"The weight factor of the undesirable losses in KTO training.\"},\n    )\n    simpo_gamma: float = field(\n        default=0.5,\n        metadata={\"help\": \"The target reward margin term in SimPO loss.\"},\n    )\n    ppo_buffer_size: int = field(\n        default=1,\n        metadata={\"help\": \"The number of mini-batches to make experience buffer in a PPO optimization step.\"},\n    )\n    ppo_epochs: int = field(\n        default=4,\n        metadata={\"help\": \"The number of epochs to perform in a PPO optimization step.\"},\n    )\n    ppo_score_norm: bool = field(\n        default=False,\n        metadata={\"help\": \"Use score normalization in PPO training.\"},\n    )\n    ppo_target: float = field(\n        default=6.0,\n        metadata={\"help\": \"Target KL value for adaptive KL control in PPO training.\"},\n    )\n    ppo_whiten_rewards: bool = field(\n        default=False,\n        metadata={\"help\": \"Whiten the rewards before compute advantages in PPO training.\"},\n    )\n    ref_model: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Path to the reference model used for the PPO or DPO training.\"},\n    )\n    ref_model_adapters: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Path to the adapters of the reference model.\"},\n    )\n    ref_model_quantization_bit: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of bits to quantize the reference model.\"},\n    )\n    reward_model: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Path to the reward model used for the PPO training.\"},\n    )\n    reward_model_adapters: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Path to the adapters of the reward model.\"},\n    )\n    reward_model_quantization_bit: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of bits to quantize the reward model.\"},\n    )\n    reward_model_type: Literal[\"lora\", \"full\", \"api\"] = field(\n        default=\"lora\",\n        metadata={\"help\": \"The type of the reward model in PPO training. Lora model only supports lora training.\"},\n    )\n\n\n@dataclass\nclass GaloreArguments:\n    r\n\n    use_galore: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether or not to use the gradient low-Rank projection (GaLore).\"},\n    )\n    galore_target: str = field(\n        default=\"all\",\n        metadata={\n            \"help\": (\n                \"Name(s) of modules to apply GaLore. Use commas to separate multiple modules. \"\n                \"Use `all` to specify all the linear modules.\"\n            )\n        },\n    )\n    galore_rank: int = field(\n        default=16,\n        metadata={\"help\": \"The rank of GaLore gradients.\"},\n    )\n    galore_update_interval: int = field(\n        default=200,\n        metadata={\"help\": \"Number of steps to update the GaLore projection.\"},\n    )\n    galore_scale: float = field(\n        default=0.25,\n        metadata={\"help\": \"GaLore scaling coefficient.\"},\n    )\n    galore_proj_type: Literal[\"std\", \"reverse_std\", \"right\", \"left\", \"full\"] = field(\n        default=\"std\",\n        metadata={\"help\": \"Type of GaLore projection.\"},\n    )\n    galore_layerwise: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether or not to enable layer-wise update to further save memory.\"},\n    )\n\n\n@dataclass\nclass BAdamArgument:\n    r\n\n    use_badam: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether or not to use the BAdam optimizer.\"},\n    )\n    badam_mode: Literal[\"layer\", \"ratio\"] = field(\n        default=\"layer\",\n        metadata={\"help\": \"Whether to use layer-wise or ratio-wise BAdam optimizer.\"},\n    )\n    badam_start_block: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The starting block index for layer-wise BAdam.\"},\n    )\n    badam_switch_mode: Optional[Literal[\"ascending\", \"descending\", \"random\", \"fixed\"]] = field(\n        default=\"ascending\",\n        metadata={\"help\": \"the strategy of picking block to update for layer-wise BAdam.\"},\n    )\n    badam_switch_interval: Optional[int] = field(\n        default=50,\n        metadata={\n            \"help\": \"Number of steps to update the block for layer-wise BAdam. Use -1 to disable the block update.\"\n        },\n    )\n    badam_update_ratio: float = field(\n        default=0.05,\n        metadata={\"help\": \"The ratio of the update for ratio-wise BAdam.\"},\n    )\n    badam_mask_mode: Literal[\"adjacent\", \"scatter\"] = field(\n        default=\"adjacent\",\n        metadata={\n            \"help\": (\n                \"The mode of the mask for BAdam optimizer. \"\n                \"`adjacent` means that the trainable parameters are adjacent to each other, \"\n                \"`scatter` means that trainable parameters are randomly choosed from the weight.\"\n            )\n        },\n    )\n    badam_verbose: int = field(\n        default=0,\n        metadata={\n            \"help\": (\n                \"The verbosity level of BAdam optimizer. \"\n                \"0 for no print, 1 for print the block prefix, 2 for print trainable parameters.\"\n            )\n        },\n    )\n\n\n@dataclass\nclass FinetuningArguments(FreezeArguments, LoraArguments, RLHFArguments, GaloreArguments, BAdamArgument):\n    r\n\n    pure_bf16: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether or not to train model in purely bf16 precision (without AMP).\"},\n    )\n    stage: Literal[\"pt\", \"sft\", \"rm\", \"ppo\", \"dpo\", \"kto\"] = field(\n        default=\"sft\",\n        metadata={\"help\": \"Which stage will be performed in training.\"},\n    )\n    finetuning_type: Literal[\"lora\", \"freeze\", \"full\"] = field(\n        default=\"lora\",\n        metadata={\"help\": \"Which fine-tuning method to use.\"},\n    )\n    use_llama_pro: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether or not to make only the parameters in the expanded blocks trainable.\"},\n    )\n    freeze_vision_tower: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether ot not to freeze vision tower in MLLM training.\"},\n    )\n    train_mm_proj_only: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether or not to train the multimodal projector for MLLM only.\"},\n    )\n    plot_loss: bool = field(\n        default=False,\n        metadata={\"help\": \"Whether or not to save the training loss curves.\"},\n    )\n\n    def __post_init__(self):\n        def split_arg(arg):\n            if isinstance(arg, str):\n                return [item.strip() for item in arg.split(\",\")]\n            return arg\n\n        self.freeze_trainable_modules: List[str] = split_arg(self.freeze_trainable_modules)\n        self.freeze_extra_modules: Optional[List[str]] = split_arg(self.freeze_extra_modules)\n        self.lora_alpha: int = self.lora_alpha or self.lora_rank * 2\n        self.lora_target: List[str] = split_arg(self.lora_target)\n        self.additional_target: Optional[List[str]] = split_arg(self.additional_target)\n        self.galore_target: List[str] = split_arg(self.galore_target)\n        self.freeze_vision_tower = self.freeze_vision_tower or self.train_mm_proj_only\n        self.use_ref_model = self.stage == \"dpo\" and self.pref_loss not in [\"orpo\", \"simpo\"]\n\n        assert self.finetuning_type in [\"lora\", \"freeze\", \"full\"], \"Invalid fine-tuning method.\"\n        assert self.ref_model_quantization_bit in [None, 8, 4], \"We only accept 4-bit or 8-bit quantization.\"\n        assert self.reward_model_quantization_bit in [None, 8, 4], \"We only accept 4-bit or 8-bit quantization.\"\n\n        if self.stage == \"ppo\" and self.reward_model is None:\n            raise ValueError(\"`reward_model` is necessary for PPO training.\")\n\n        if self.stage == \"ppo\" and self.reward_model_type == \"lora\" and self.finetuning_type != \"lora\":\n            raise ValueError(\"`reward_model_type` cannot be lora for Freeze/Full PPO training.\")\n\n        if self.stage == \"dpo\" and self.pref_loss != \"sigmoid\" and self.dpo_label_smoothing > 1e-6:\n            raise ValueError(\"`dpo_label_smoothing` is only valid for sigmoid loss function.\")\n\n        if self.use_llama_pro and self.finetuning_type == \"full\":\n            raise ValueError(\"`use_llama_pro` is only valid for Freeze or LoRA training.\")\n\n        if self.finetuning_type == \"lora\" and (self.use_galore or self.use_badam):\n            raise ValueError(\"Cannot use LoRA with GaLore or BAdam together.\")\n\n        if self.use_galore and self.use_badam:\n            raise ValueError(\"Cannot use GaLore with BAdam together.\")\n\n        if self.pissa_init and (self.stage in [\"ppo\", \"kto\"] or self.use_ref_model):\n            raise ValueError(\"Cannot use PiSSA for current training stage.\")\n\n        if self.train_mm_proj_only and self.finetuning_type != \"full\":\n            raise ValueError(\"`train_mm_proj_only` is only valid for full training.\")\n\n        if self.finetuning_type != \"lora\":\n            if self.loraplus_lr_ratio is not None:\n                raise ValueError(\"`loraplus_lr_ratio` is only valid for LoRA training.\")\n\n            if self.use_rslora:\n                raise ValueError(\"`use_rslora` is only valid for LoRA training.\")\n\n            if self.use_dora:\n                raise ValueError(\"`use_dora` is only valid for LoRA training.\")\n\n            if self.pissa_init:\n                raise ValueError(\"`pissa_init` is only valid for LoRA training.\")\n```"
        }
    ],
    "temperature": 0
}