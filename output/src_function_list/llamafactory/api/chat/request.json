{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nimport base64\nimport io\nimport json\nimport os\nimport uuid\nfrom typing import TYPE_CHECKING, AsyncGenerator, Dict, List, Optional, Tuple\n\nfrom ..data import Role as DataRole\nfrom ..extras.logging import get_logger\nfrom ..extras.packages import is_fastapi_available, is_pillow_available, is_requests_available\nfrom .common import dictify, jsonify\nfrom .protocol import (\n    ChatCompletionMessage,\n    ChatCompletionResponse,\n    ChatCompletionResponseChoice,\n    ChatCompletionResponseUsage,\n    ChatCompletionStreamResponse,\n    ChatCompletionStreamResponseChoice,\n    Finish,\n    Function,\n    FunctionCall,\n    Role,\n    ScoreEvaluationResponse,\n)\n\n\nif is_fastapi_available():\n    from fastapi import HTTPException, status\n\n\nif is_pillow_available():\n    from PIL import Image\n\n\nif is_requests_available():\n    import requests\n\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\n    from ..chat import ChatModel\n    from .protocol import ChatCompletionRequest, ScoreEvaluationRequest\n\n\nlogger = get_logger(__name__)\nROLE_MAPPING = {\n    Role.USER: DataRole.USER.value,\n    Role.ASSISTANT: DataRole.ASSISTANT.value,\n    Role.SYSTEM: DataRole.SYSTEM.value,\n    Role.FUNCTION: DataRole.FUNCTION.value,\n    Role.TOOL: DataRole.OBSERVATION.value,\n}\n\n\ndef _process_request(\n    request: \"ChatCompletionRequest\",\n) -> Tuple[List[Dict[str, str]], Optional[str], Optional[str], Optional[\"NDArray\"]]:\n    logger.info(\"==== request ====\\n{}\".format(json.dumps(dictify(request), indent=2, ensure_ascii=False)))\n\n    if len(request.messages) == 0:\n        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"Invalid length\")\n\n    if request.messages[0].role == Role.SYSTEM:\n        system = request.messages.pop(0).content\n    else:\n        system = None\n\n    if len(request.messages) % 2 == 0:\n        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"Only supports u/a/u/a/u...\")\n\n    input_messages = []\n    image = None\n    for i, message in enumerate(request.messages):\n        if i % 2 == 0 and message.role not in [Role.USER, Role.TOOL]:\n            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"Invalid role\")\n        elif i % 2 == 1 and message.role not in [Role.ASSISTANT, Role.FUNCTION]:\n            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"Invalid role\")\n\n        if message.role == Role.ASSISTANT and isinstance(message.tool_calls, list) and len(message.tool_calls):\n            tool_calls = [\n                {\"name\": tool_call.function.name, \"arguments\": tool_call.function.arguments}\n                for tool_call in message.tool_calls\n            ]\n            content = json.dumps(tool_calls, ensure_ascii=False)\n            input_messages.append({\"role\": ROLE_MAPPING[Role.FUNCTION], \"content\": content})\n        elif isinstance(message.content, list):\n            for input_item in message.content:\n                if input_item.type == \"text\":\n                    input_messages.append({\"role\": ROLE_MAPPING[message.role], \"content\": input_item.text})\n                else:\n                    image_url = input_item.image_url.url\n                    if image_url.startswith(\"data:image\"):  \n                        image_data = base64.b64decode(image_url.split(\",\", maxsplit=1)[1])\n                        image_path = io.BytesIO(image_data)\n                    elif os.path.isfile(image_url):  \n                        image_path = open(image_url, \"rb\")\n                    else:  \n                        image_path = requests.get(image_url, stream=True).raw\n\n                    image = Image.open(image_path).convert(\"RGB\")\n        else:\n            input_messages.append({\"role\": ROLE_MAPPING[message.role], \"content\": message.content})\n\n    tool_list = request.tools\n    if isinstance(tool_list, list) and len(tool_list):\n        try:\n            tools = json.dumps([dictify(tool.function) for tool in tool_list], ensure_ascii=False)\n        except json.JSONDecodeError:\n            raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"Invalid tools\")\n    else:\n        tools = None\n\n    return input_messages, system, tools, image\n\n\ndef _create_stream_chat_completion_chunk(\n    completion_id: str,\n    model: str,\n    delta: \"ChatCompletionMessage\",\n    index: Optional[int] = 0,\n    finish_reason: Optional[\"Finish\"] = None,\n) -> str:\n    choice_data = ChatCompletionStreamResponseChoice(index=index, delta=delta, finish_reason=finish_reason)\n    chunk = ChatCompletionStreamResponse(id=completion_id, model=model, choices=[choice_data])\n    return jsonify(chunk)\n\n\nasync def create_chat_completion_response(\n    request: \"ChatCompletionRequest\", chat_model: \"ChatModel\"\n) -> \"ChatCompletionResponse\":\n    completion_id = \"chatcmpl-{}\".format(uuid.uuid4().hex)\n    input_messages, system, tools, image = _process_request(request)\n    responses = await chat_model.achat(\n        input_messages,\n        system,\n        tools,\n        image,\n        do_sample=request.do_sample,\n        temperature=request.temperature,\n        top_p=request.top_p,\n        max_new_tokens=request.max_tokens,\n        num_return_sequences=request.n,\n        stop=request.stop,\n    )\n\n    prompt_length, response_length = 0, 0\n    choices = []\n    for i, response in enumerate(responses):\n        if tools:\n            result = chat_model.engine.template.extract_tool(response.response_text)\n        else:\n            result = response.response_text\n\n        if isinstance(result, list):\n            tool_calls = []\n            for tool in result:\n                function = Function(name=tool[0], arguments=tool[1])\n                tool_calls.append(FunctionCall(id=\"call_{}\".format(uuid.uuid4().hex), function=function))\n\n            response_message = ChatCompletionMessage(role=Role.ASSISTANT, tool_calls=tool_calls)\n            finish_reason = Finish.TOOL\n        else:\n            response_message = ChatCompletionMessage(role=Role.ASSISTANT, content=result)\n            finish_reason = Finish.STOP if response.finish_reason == \"stop\" else Finish.LENGTH\n\n        choices.append(ChatCompletionResponseChoice(index=i, message=response_message, finish_reason=finish_reason))\n        prompt_length = response.prompt_length\n        response_length += response.response_length\n\n    usage = ChatCompletionResponseUsage(\n        prompt_tokens=prompt_length,\n        completion_tokens=response_length,\n        total_tokens=prompt_length + response_length,\n    )\n\n    return ChatCompletionResponse(id=completion_id, model=request.model, choices=choices, usage=usage)\n\n\nasync def create_stream_chat_completion_response(\n    request: \"ChatCompletionRequest\", chat_model: \"ChatModel\"\n) -> AsyncGenerator[str, None]:\n    completion_id = \"chatcmpl-{}\".format(uuid.uuid4().hex)\n    input_messages, system, tools, image = _process_request(request)\n    if tools:\n        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"Cannot stream function calls.\")\n\n    if request.n > 1:\n        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"Cannot stream multiple responses.\")\n\n    yield _create_stream_chat_completion_chunk(\n        completion_id=completion_id, model=request.model, delta=ChatCompletionMessage(role=Role.ASSISTANT, content=\"\")\n    )\n    async for new_token in chat_model.astream_chat(\n        input_messages,\n        system,\n        tools,\n        image,\n        do_sample=request.do_sample,\n        temperature=request.temperature,\n        top_p=request.top_p,\n        max_new_tokens=request.max_tokens,\n        stop=request.stop,\n    ):\n        if len(new_token) != 0:\n            yield _create_stream_chat_completion_chunk(\n                completion_id=completion_id, model=request.model, delta=ChatCompletionMessage(content=new_token)\n            )\n\n    yield _create_stream_chat_completion_chunk(\n        completion_id=completion_id, model=request.model, delta=ChatCompletionMessage(), finish_reason=Finish.STOP\n    )\n    yield \"[DONE]\"\n\n\nasync def create_score_evaluation_response(\n    request: \"ScoreEvaluationRequest\", chat_model: \"ChatModel\"\n) -> \"ScoreEvaluationResponse\":\n    if len(request.messages) == 0:\n        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=\"Invalid request\")\n\n    scores = await chat_model.aget_scores(request.messages, max_length=request.max_length)\n    return ScoreEvaluationResponse(model=request.model, scores=scores)\n```"
        }
    ],
    "temperature": 0
}