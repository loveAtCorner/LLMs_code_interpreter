{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nimport asyncio\nfrom threading import Thread\nfrom typing import TYPE_CHECKING, Any, AsyncGenerator, Dict, Generator, List, Optional, Sequence\n\nfrom ..extras.misc import torch_gc\nfrom ..hparams import get_infer_args\nfrom .hf_engine import HuggingfaceEngine\nfrom .vllm_engine import VllmEngine\n\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n\n    from .base_engine import BaseEngine, Response\n\n\ndef _start_background_loop(loop: \"asyncio.AbstractEventLoop\") -> None:\n    asyncio.set_event_loop(loop)\n    loop.run_forever()\n\n\nclass ChatModel:\n    def __init__(self, args: Optional[Dict[str, Any]] = None) -> None:\n        model_args, data_args, finetuning_args, generating_args = get_infer_args(args)\n        if model_args.infer_backend == \"huggingface\":\n            self.engine: \"BaseEngine\" = HuggingfaceEngine(model_args, data_args, finetuning_args, generating_args)\n        elif model_args.infer_backend == \"vllm\":\n            self.engine: \"BaseEngine\" = VllmEngine(model_args, data_args, finetuning_args, generating_args)\n        else:\n            raise NotImplementedError(\"Unknown backend: {}\".format(model_args.infer_backend))\n\n        self._loop = asyncio.new_event_loop()\n        self._thread = Thread(target=_start_background_loop, args=(self._loop,), daemon=True)\n        self._thread.start()\n\n    def chat(\n        self,\n        messages: Sequence[Dict[str, str]],\n        system: Optional[str] = None,\n        tools: Optional[str] = None,\n        image: Optional[\"NDArray\"] = None,\n        **input_kwargs,\n    ) -> List[\"Response\"]:\n        task = asyncio.run_coroutine_threadsafe(self.achat(messages, system, tools, image, **input_kwargs), self._loop)\n        return task.result()\n\n    async def achat(\n        self,\n        messages: Sequence[Dict[str, str]],\n        system: Optional[str] = None,\n        tools: Optional[str] = None,\n        image: Optional[\"NDArray\"] = None,\n        **input_kwargs,\n    ) -> List[\"Response\"]:\n        return await self.engine.chat(messages, system, tools, image, **input_kwargs)\n\n    def stream_chat(\n        self,\n        messages: Sequence[Dict[str, str]],\n        system: Optional[str] = None,\n        tools: Optional[str] = None,\n        image: Optional[\"NDArray\"] = None,\n        **input_kwargs,\n    ) -> Generator[str, None, None]:\n        generator = self.astream_chat(messages, system, tools, image, **input_kwargs)\n        while True:\n            try:\n                task = asyncio.run_coroutine_threadsafe(generator.__anext__(), self._loop)\n                yield task.result()\n            except StopAsyncIteration:\n                break\n\n    async def astream_chat(\n        self,\n        messages: Sequence[Dict[str, str]],\n        system: Optional[str] = None,\n        tools: Optional[str] = None,\n        image: Optional[\"NDArray\"] = None,\n        **input_kwargs,\n    ) -> AsyncGenerator[str, None]:\n        async for new_token in self.engine.stream_chat(messages, system, tools, image, **input_kwargs):\n            yield new_token\n\n    def get_scores(\n        self,\n        batch_input: List[str],\n        **input_kwargs,\n    ) -> List[float]:\n        task = asyncio.run_coroutine_threadsafe(self.aget_scores(batch_input, **input_kwargs), self._loop)\n        return task.result()\n\n    async def aget_scores(\n        self,\n        batch_input: List[str],\n        **input_kwargs,\n    ) -> List[float]:\n        return await self.engine.get_scores(batch_input, **input_kwargs)\n\n\ndef run_chat() -> None:\n    try:\n        import platform\n\n        if platform.system() != \"Windows\":\n            import readline  \n    except ImportError:\n        print(\"Install `readline` for a better experience.\")\n\n    chat_model = ChatModel()\n    messages = []\n    print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n\n    while True:\n        try:\n            query = input(\"\\nUser: \")\n        except UnicodeDecodeError:\n            print(\"Detected decoding error at the inputs, please set the terminal encoding to utf-8.\")\n            continue\n        except Exception:\n            raise\n\n        if query.strip() == \"exit\":\n            break\n\n        if query.strip() == \"clear\":\n            messages = []\n            torch_gc()\n            print(\"History has been removed.\")\n            continue\n\n        messages.append({\"role\": \"user\", \"content\": query})\n        print(\"Assistant: \", end=\"\", flush=True)\n\n        response = \"\"\n        for new_text in chat_model.stream_chat(messages):\n            print(new_text, end=\"\", flush=True)\n            response += new_text\n        print()\n        messages.append({\"role\": \"assistant\", \"content\": response})\n```"
        }
    ],
    "temperature": 0
}