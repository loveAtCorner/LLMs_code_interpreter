{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nimport re\nfrom typing import TYPE_CHECKING\n\nimport torch\nfrom peft import LoraConfig, LoraModel, PeftModel, TaskType, get_peft_model\nfrom transformers.integrations import is_deepspeed_zero3_enabled\nfrom transformers.modeling_utils import is_fsdp_enabled\n\nfrom ..extras.logging import get_logger\nfrom .model_utils.misc import find_all_linear_modules, find_expanded_modules\nfrom .model_utils.quantization import QuantizationMethod\nfrom .model_utils.unsloth import get_unsloth_peft_model, load_unsloth_peft_model\n\n\nif TYPE_CHECKING:\n    from transformers import PretrainedConfig, PreTrainedModel\n\n    from ..hparams import FinetuningArguments, ModelArguments\n\n\nlogger = get_logger(__name__)\n\n\ndef _setup_full_tuning(\n    model: \"PreTrainedModel\",\n    model_args: \"ModelArguments\",\n    finetuning_args: \"FinetuningArguments\",\n    is_trainable: bool,\n    cast_trainable_params_to_fp32: bool,\n) -> None:\n    if not is_trainable:\n        return\n\n    logger.info(\"Fine-tuning method: Full\")\n    forbidden_modules = set()\n    if model_args.visual_inputs and finetuning_args.freeze_vision_tower:\n        forbidden_modules.add(\"vision_tower\")\n\n    if model_args.visual_inputs and finetuning_args.train_mm_proj_only:\n        forbidden_modules.add(\"language_model\")\n\n    for name, param in model.named_parameters():\n        if not any(forbidden_module in name for forbidden_module in forbidden_modules):\n            if cast_trainable_params_to_fp32:\n                param.data = param.data.to(torch.float32)\n        else:\n            param.requires_grad_(False)\n\n\ndef _setup_freeze_tuning(\n    model: \"PreTrainedModel\",\n    model_args: \"ModelArguments\",\n    finetuning_args: \"FinetuningArguments\",\n    is_trainable: bool,\n    cast_trainable_params_to_fp32: bool,\n) -> None:\n    if not is_trainable:\n        return\n\n    logger.info(\"Fine-tuning method: Freeze\")\n    if model_args.visual_inputs:\n        config = model.config.text_config\n    else:\n        config = model.config\n\n    num_layers = (\n        getattr(config, \"num_hidden_layers\", None)\n        or getattr(config, \"num_layers\", None)\n        or getattr(config, \"n_layer\", None)\n    )\n    if not num_layers:\n        raise ValueError(\"Current model does not support freeze tuning.\")\n\n    if finetuning_args.use_llama_pro:\n        if num_layers % finetuning_args.freeze_trainable_layers != 0:\n            raise ValueError(\n                \"`num_layers` {} should be divisible by `num_layer_trainable` {}.\".format(\n                    num_layers, finetuning_args.freeze_trainable_layers\n                )\n            )\n\n        stride = num_layers // finetuning_args.freeze_trainable_layers\n        trainable_layer_ids = range(stride - 1, num_layers + stride - 1, stride)\n    elif finetuning_args.freeze_trainable_layers > 0:  \n        trainable_layer_ids = range(max(0, num_layers - finetuning_args.freeze_trainable_layers), num_layers)\n    else:  \n        trainable_layer_ids = range(min(-finetuning_args.freeze_trainable_layers, num_layers))\n\n    hidden_modules = set()\n    non_hidden_modules = set()\n    for name, _ in model.named_parameters():\n        if \".0.\" in name:\n            hidden_modules.add(name.split(\".0.\")[-1].split(\".\")[0])\n        elif \".1.\" in name:  \n            hidden_modules.add(name.split(\".1.\")[-1].split(\".\")[0])\n\n        if re.search(r\"\\.\\d+\\.\", name) is None:\n            non_hidden_modules.add(name.split(\".\")[-2])\n\n    trainable_layers = []\n    for module_name in finetuning_args.freeze_trainable_modules:\n        if module_name != \"all\" and module_name not in hidden_modules:\n            raise ValueError(\n                \"Module {} is not found, please choose from {}\".format(module_name, \", \".join(hidden_modules))\n            )\n\n        for idx in trainable_layer_ids:\n            trainable_layers.append(\".{:d}.{}\".format(idx, module_name if module_name != \"all\" else \"\"))\n\n    if finetuning_args.freeze_extra_modules:\n        for module_name in finetuning_args.freeze_extra_modules:\n            if module_name not in non_hidden_modules:\n                raise ValueError(\n                    \"Module {} is not found, please choose from {}\".format(module_name, \", \".join(non_hidden_modules))\n                )\n\n            trainable_layers.append(module_name)\n\n    forbidden_modules = set()\n    if model_args.visual_inputs and finetuning_args.freeze_vision_tower:\n        forbidden_modules.add(\"vision_tower\")\n\n    for name, param in model.named_parameters():\n        if any(trainable_layer in name for trainable_layer in trainable_layers) and not any(\n            forbidden_module in name for forbidden_module in forbidden_modules\n        ):\n            if cast_trainable_params_to_fp32:\n                param.data = param.data.to(torch.float32)\n        else:\n            param.requires_grad_(False)\n\n    logger.info(\"Set trainable layers: {}\".format(\",\".join(trainable_layers)))\n\n\ndef _setup_lora_tuning(\n    config: \"PretrainedConfig\",\n    model: \"PreTrainedModel\",\n    model_args: \"ModelArguments\",\n    finetuning_args: \"FinetuningArguments\",\n    is_trainable: bool,\n    cast_trainable_params_to_fp32: bool,\n) -> \"PeftModel\":\n    if is_trainable:\n        logger.info(\"Fine-tuning method: {}\".format(\"DoRA\" if finetuning_args.use_dora else \"LoRA\"))\n\n    adapter_to_resume = None\n\n    if model_args.adapter_name_or_path is not None:\n        is_mergeable = True\n        if getattr(model, \"quantization_method\", None):  \n            assert len(model_args.adapter_name_or_path) == 1, \"Quantized model only accepts a single adapter.\"\n            is_mergeable = False\n\n        if is_deepspeed_zero3_enabled():\n            assert len(model_args.adapter_name_or_path) == 1, \"Cannot use multiple adapters in DeepSpeed ZeRO-3.\"\n            is_mergeable = False\n\n        if model_args.use_unsloth:\n            assert len(model_args.adapter_name_or_path) == 1, \"Unsloth model only accepts a single adapter.\"\n            is_mergeable = False\n\n        if (is_trainable and not finetuning_args.create_new_adapter) or (not is_mergeable):\n            adapter_to_merge = model_args.adapter_name_or_path[:-1]\n            adapter_to_resume = model_args.adapter_name_or_path[-1]\n        else:\n            adapter_to_merge = model_args.adapter_name_or_path\n\n        init_kwargs = {\n            \"subfolder\": model_args.adapter_folder,\n            \"offload_folder\": model_args.offload_folder,\n            \"cache_dir\": model_args.cache_dir,\n            \"revision\": model_args.model_revision,\n            \"token\": model_args.hf_hub_token,\n        }\n\n        for adapter in adapter_to_merge:\n            model: \"LoraModel\" = PeftModel.from_pretrained(model, adapter, **init_kwargs)\n            model = model.merge_and_unload()\n\n        if len(adapter_to_merge) > 0:\n            logger.info(\"Merged {} adapter(s).\".format(len(adapter_to_merge)))\n\n        if adapter_to_resume is not None:  \n            if model_args.use_unsloth:\n                model = load_unsloth_peft_model(config, model_args, is_trainable=is_trainable)\n            else:\n                model = PeftModel.from_pretrained(model, adapter_to_resume, is_trainable=is_trainable, **init_kwargs)\n\n        logger.info(\"Loaded adapter(s): {}\".format(\",\".join(model_args.adapter_name_or_path)))\n\n    if is_trainable and adapter_to_resume is None:  \n        if len(finetuning_args.lora_target) == 1 and finetuning_args.lora_target[0] == \"all\":\n            target_modules = find_all_linear_modules(model, finetuning_args.freeze_vision_tower)\n        else:\n            target_modules = finetuning_args.lora_target\n\n        if finetuning_args.use_llama_pro:\n            target_modules = find_expanded_modules(model, target_modules, finetuning_args.freeze_trainable_layers)\n\n        if model_args.visual_inputs and finetuning_args.freeze_vision_tower:\n            target_modules = \"^(?!.*vision_tower).*(?:{}).*\".format(\"|\".join(target_modules))\n\n        if (\n            finetuning_args.use_dora\n            and getattr(model, \"quantization_method\", None) is not None\n            and getattr(model, \"quantization_method\", None) != QuantizationMethod.BITS_AND_BYTES\n        ):\n            raise ValueError(\"DoRA is not compatible with PTQ-quantized models.\")\n\n        if model_args.resize_vocab and finetuning_args.additional_target is None:\n            input_embeddings = model.get_input_embeddings()\n            output_embeddings = model.get_output_embeddings()\n            module_names = set()\n            for name, module in model.named_modules():\n                if module in [input_embeddings, output_embeddings]:\n                    module_names.add(name.split(\".\")[-1])\n\n            finetuning_args.additional_target = module_names\n            logger.warning(\"Vocab has been resized, add {} to trainable params.\".format(\",\".join(module_names)))\n\n        peft_kwargs = {\n            \"r\": finetuning_args.lora_rank,\n            \"target_modules\": target_modules,\n            \"lora_alpha\": finetuning_args.lora_alpha,\n            \"lora_dropout\": finetuning_args.lora_dropout,\n            \"use_rslora\": finetuning_args.use_rslora,\n            \"use_dora\": finetuning_args.use_dora,\n            \"modules_to_save\": finetuning_args.additional_target,\n        }\n\n        if model_args.use_unsloth:\n            model = get_unsloth_peft_model(model, model_args, peft_kwargs)\n        else:\n            if finetuning_args.pissa_init:\n                if finetuning_args.pissa_iter == -1:\n                    logger.info(\"Using PiSSA initialization.\")\n                    peft_kwargs[\"init_lora_weights\"] = \"pissa\"\n                else:\n                    logger.info(\"Using PiSSA initialization with FSVD steps {}.\".format(finetuning_args.pissa_iter))\n                    peft_kwargs[\"init_lora_weights\"] = \"pissa_niter_{}\".format(finetuning_args.pissa_iter)\n\n            lora_config = LoraConfig(\n                task_type=TaskType.CAUSAL_LM,\n                inference_mode=False,\n                **peft_kwargs,\n            )\n            model = get_peft_model(model, lora_config)\n\n    if is_trainable and cast_trainable_params_to_fp32:\n        for param in filter(lambda p: p.requires_grad, model.parameters()):\n            param.data = param.data.to(torch.float32)\n\n    return model\n\n\ndef init_adapter(\n    config: \"PretrainedConfig\",\n    model: \"PreTrainedModel\",\n    model_args: \"ModelArguments\",\n    finetuning_args: \"FinetuningArguments\",\n    is_trainable: bool,\n) -> \"PreTrainedModel\":\n    r\n    if is_trainable and getattr(model, \"quantization_method\", None) is not None:\n        if finetuning_args.finetuning_type != \"lora\":\n            raise ValueError(\"Quantized models can only be used for the LoRA tuning.\")\n\n        if finetuning_args.pissa_init:\n            raise ValueError(\"Cannot initialize PiSSA adapter on quantized models.\")\n\n    \n    \n    \n    cast_trainable_params_to_fp32 = False\n    if not is_trainable:\n        pass\n    elif finetuning_args.pure_bf16 or finetuning_args.use_badam:\n        logger.info(\"Pure bf16 / BAdam detected, remaining trainable params in half precision.\")\n    elif model_args.quantization_bit is None and (is_deepspeed_zero3_enabled() or is_fsdp_enabled()):\n        logger.info(\"ZeRO3 / FSDP detected, remaining trainable params in float32.\")\n    else:\n        logger.info(\"Upcasting trainable params to float32.\")\n        cast_trainable_params_to_fp32 = True\n\n    if finetuning_args.finetuning_type == \"full\":\n        _setup_full_tuning(model, model_args, finetuning_args, is_trainable, cast_trainable_params_to_fp32)\n    elif finetuning_args.finetuning_type == \"freeze\":\n        _setup_freeze_tuning(model, model_args, finetuning_args, is_trainable, cast_trainable_params_to_fp32)\n    elif finetuning_args.finetuning_type == \"lora\":\n        model = _setup_lora_tuning(\n            config, model, model_args, finetuning_args, is_trainable, cast_trainable_params_to_fp32\n        )\n    else:\n        raise NotImplementedError(\"Unknown finetuning type: {}.\".format(finetuning_args.finetuning_type))\n\n    return model\n```"
        }
    ],
    "temperature": 0
}