{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nimport math\nfrom typing import TYPE_CHECKING\n\nfrom ...extras.logging import get_logger\n\n\nif TYPE_CHECKING:\n    from transformers import PretrainedConfig\n\n    from ...hparams import ModelArguments\n\n\nlogger = get_logger(__name__)\n\n\ndef configure_rope(config: \"PretrainedConfig\", model_args: \"ModelArguments\", is_trainable: bool) -> None:\n    if model_args.rope_scaling is None:\n        return\n\n    if not hasattr(config, \"rope_scaling\"):\n        logger.warning(\"Current model does not support RoPE scaling.\")\n        return\n\n    if model_args.model_max_length is not None:\n        if is_trainable and model_args.rope_scaling == \"dynamic\":\n            logger.warning(\n                \"Dynamic NTK scaling may not work well with fine-tuning. \"\n                \"See: https://github.com/huggingface/transformers/pull/24653\"\n            )\n\n        current_max_length = getattr(config, \"max_position_embeddings\", None)\n        if current_max_length and model_args.model_max_length > current_max_length:\n            logger.info(\n                \"Enlarge max model length from {} to {}.\".format(current_max_length, model_args.model_max_length)\n            )\n            setattr(config, \"max_position_embeddings\", model_args.model_max_length)\n            scaling_factor = float(math.ceil(model_args.model_max_length / current_max_length))\n        else:\n            logger.warning(\"Input length is smaller than max length. Consider increase input length.\")\n            scaling_factor = 1.0\n    else:\n        scaling_factor = 2.0\n\n    setattr(config, \"rope_scaling\", {\"type\": model_args.rope_scaling, \"factor\": scaling_factor})\n    logger.info(\n        \"Using {} scaling strategy and setting scaling factor to {}\".format(model_args.rope_scaling, scaling_factor)\n    )\n```"
        }
    ],
    "temperature": 0
}