{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nfrom typing import TYPE_CHECKING, Dict\n\nimport torch\nfrom transformers.utils import cached_file\n\nfrom ...extras.constants import V_HEAD_SAFE_WEIGHTS_NAME, V_HEAD_WEIGHTS_NAME\nfrom ...extras.logging import get_logger\n\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedModel\n\n    from ...hparams import ModelArguments\n\n\nlogger = get_logger(__name__)\n\n\ndef load_valuehead_params(path_or_repo_id: str, model_args: \"ModelArguments\") -> Dict[str, torch.Tensor]:\n    r\n    kwargs = {\"path_or_repo_id\": path_or_repo_id, \"cache_dir\": model_args.cache_dir, \"token\": model_args.hf_hub_token}\n    err_text = \"\"\n\n    try:\n        from safetensors import safe_open\n\n        vhead_file = cached_file(filename=V_HEAD_SAFE_WEIGHTS_NAME, **kwargs)\n        with safe_open(vhead_file, framework=\"pt\", device=\"cpu\") as f:\n            return {key: f.get_tensor(key) for key in f.keys()}\n    except Exception as err:\n        err_text = str(err)\n\n    try:\n        vhead_file = cached_file(filename=V_HEAD_WEIGHTS_NAME, **kwargs)\n        return torch.load(vhead_file, map_location=\"cpu\")\n    except Exception as err:\n        err_text = str(err)\n\n    logger.info(\"Provided path ({}) does not contain value head weights: {}.\".format(path_or_repo_id, err_text))\n    logger.info(\"Ignore the above message if you are not resuming the training of a value head model.\")\n    return None\n\n\ndef prepare_valuehead_model(model: \"PreTrainedModel\") -> None:\n    if getattr(model.config, \"model_type\", None) == \"llava\":\n        setattr(model, \"lm_head\", model.language_model.get_output_embeddings())\n        setattr(model, \"_keys_to_ignore_on_save\", [\"lm_head.weight\"])\n\n    if getattr(model.config, \"model_type\", None) == \"chatglm\":\n        setattr(model, \"lm_head\", model.transformer.output_layer)\n        setattr(model, \"_keys_to_ignore_on_save\", [\"lm_head.weight\"])\n\n    if getattr(model.config, \"model_type\", None) == \"internlm2\":\n        setattr(model, \"lm_head\", model.output)\n        setattr(model, \"_keys_to_ignore_on_save\", [\"lm_head.weight\"])\n```"
        }
    ],
    "temperature": 0
}