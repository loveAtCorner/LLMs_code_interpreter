{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nfrom typing import TYPE_CHECKING, List\n\nfrom ...extras.logging import get_logger\n\n\nif TYPE_CHECKING:\n    from transformers import PretrainedConfig, PreTrainedModel, PreTrainedTokenizer\n\n\nlogger = get_logger(__name__)\n\n\ndef find_all_linear_modules(model: \"PreTrainedModel\", freeze_vision_tower: bool) -> List[str]:\n    r\n    forbidden_modules = {\"lm_head\"}\n\n    if model.config.model_type == \"chatglm\":\n        forbidden_modules.add(\"output_layer\")\n    elif model.config.model_type == \"internlm2\":\n        forbidden_modules.add(\"output\")\n    elif model.config.model_type in [\"llava\", \"paligemma\"]:\n        forbidden_modules.add(\"multi_modal_projector\")\n\n    if freeze_vision_tower:\n        forbidden_modules.add(\"vision_tower\")\n\n    module_names = set()\n    for name, module in model.named_modules():\n        if any(forbidden_module in name for forbidden_module in forbidden_modules):\n            continue\n\n        if \"Linear\" in module.__class__.__name__ and \"Embedding\" not in module.__class__.__name__:\n            module_names.add(name.split(\".\")[-1])\n\n    logger.info(\"Found linear modules: {}\".format(\",\".join(module_names)))\n    return list(module_names)\n\n\ndef find_expanded_modules(model: \"PreTrainedModel\", target_modules: List[str], num_layer_trainable: int) -> List[str]:\n    r\n    num_layers = getattr(model.config, \"num_hidden_layers\", None)\n    if not num_layers:\n        raise ValueError(\"Model was not supported.\")\n\n    if num_layers % num_layer_trainable != 0:\n        raise ValueError(\n            \"`num_layers` {} should be divisible by `num_layer_trainable` {}.\".format(num_layers, num_layer_trainable)\n        )\n\n    stride = num_layers // num_layer_trainable\n    trainable_layer_ids = range(stride - 1, num_layers + stride - 1, stride)\n    trainable_layers = [\".{:d}.\".format(idx) for idx in trainable_layer_ids]\n    module_names = []\n    for name, _ in model.named_modules():\n        if any(target_module in name for target_module in target_modules) and any(\n            trainable_layer in name for trainable_layer in trainable_layers\n        ):\n            module_names.append(name)\n\n    logger.info(\"Apply lora to layers: {}\".format(\",\".join(map(str, trainable_layer_ids))))\n    return module_names\n\n\ndef register_autoclass(config: \"PretrainedConfig\", model: \"PreTrainedModel\", tokenizer: \"PreTrainedTokenizer\"):\n    if \"AutoConfig\" in getattr(config, \"auto_map\", {}):\n        config.__class__.register_for_auto_class()\n    if \"AutoModelForCausalLM\" in getattr(config, \"auto_map\", {}):\n        model.__class__.register_for_auto_class()\n    if \"AutoTokenizer\" in tokenizer.init_kwargs.get(\"auto_map\", {}):\n        tokenizer.__class__.register_for_auto_class()\n```"
        }
    ],
    "temperature": 0
}