{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nfrom typing import TYPE_CHECKING, Sequence\n\nimport torch\nfrom transformers.integrations import is_deepspeed_zero3_enabled\nfrom transformers.utils.versions import require_version\n\n\nif TYPE_CHECKING:\n    from transformers import PretrainedConfig, PreTrainedModel\n\n    from ...hparams import ModelArguments\n\n\ndef _set_z3_leaf_modules(model: \"PreTrainedModel\", leaf_modules: Sequence[\"torch.nn.Module\"]) -> None:\n    require_version(\"deepspeed>=0.13.0\", \"To fix: pip install deepspeed>=0.13.0\")\n    from deepspeed.utils import set_z3_leaf_modules  \n\n    set_z3_leaf_modules(model, leaf_modules)\n\n\ndef add_z3_leaf_module(model: \"PreTrainedModel\") -> None:\n    r\n    if not is_deepspeed_zero3_enabled():\n        return\n\n    if getattr(model.config, \"model_type\", None) == \"dbrx\":\n        from transformers.models.dbrx.modeling_dbrx import DbrxFFN\n\n        _set_z3_leaf_modules(model, [DbrxFFN])\n\n    if getattr(model.config, \"model_type\", None) == \"jamba\":\n        from transformers.models.jamba.modeling_jamba import JambaSparseMoeBlock\n\n        _set_z3_leaf_modules(model, [JambaSparseMoeBlock])\n\n    if getattr(model.config, \"model_type\", None) == \"jetmoe\":\n        from transformers.models.jetmoe.modeling_jetmoe import JetMoeMoA, JetMoeMoE\n\n        _set_z3_leaf_modules(model, [JetMoeMoA, JetMoeMoE])\n\n    if getattr(model.config, \"model_type\", None) == \"mixtral\":\n        from transformers.models.mixtral.modeling_mixtral import MixtralSparseMoeBlock\n\n        _set_z3_leaf_modules(model, [MixtralSparseMoeBlock])\n\n    if getattr(model.config, \"model_type\", None) == \"qwen2moe\":\n        from transformers.models.qwen2_moe.modeling_qwen2_moe import Qwen2MoeSparseMoeBlock\n\n        _set_z3_leaf_modules(model, [Qwen2MoeSparseMoeBlock])\n\n\ndef configure_moe(config: \"PretrainedConfig\", model_args: \"ModelArguments\", is_trainable: bool) -> None:\n    if model_args.moe_aux_loss_coef is not None:\n        if getattr(config, \"model_type\", None) in [\"jamba\", \"mixtral\", \"qwen2_moe\"]:\n            setattr(config, \"router_aux_loss_coef\", model_args.moe_aux_loss_coef)\n\n        elif getattr(config, \"model_type\", None) == \"deepseek\":\n            setattr(config, \"aux_loss_alpha\", model_args.moe_aux_loss_coef)\n\n        elif getattr(config, \"model_type\", None) == \"jetmoe\":\n            setattr(config, \"aux_loss_coef\", model_args.moe_aux_loss_coef)\n\n    if getattr(config, \"model_type\", None) in [\"dbrx\", \"jamba\", \"jetmoe\", \"mixtral\", \"qwen2_moe\"]:\n        setattr(config, \"output_router_logits\", is_trainable)\n```"
        }
    ],
    "temperature": 0
}