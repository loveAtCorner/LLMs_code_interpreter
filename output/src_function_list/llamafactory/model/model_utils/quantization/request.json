{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nimport os\nimport random\nfrom enum import Enum, unique\nfrom typing import TYPE_CHECKING, Any, Dict, List\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import BitsAndBytesConfig, EetqConfig, GPTQConfig, HqqConfig\nfrom transformers.integrations import is_deepspeed_zero3_enabled\nfrom transformers.modeling_utils import is_fsdp_enabled\nfrom transformers.utils.versions import require_version\n\nfrom ...extras.constants import FILEEXT2TYPE\nfrom ...extras.logging import get_logger\nfrom ...extras.misc import get_current_device\n\n\nif TYPE_CHECKING:\n    from transformers import PretrainedConfig, PreTrainedTokenizer\n\n    from ...hparams import ModelArguments\n\n\nlogger = get_logger(__name__)\n\n\n@unique\nclass QuantizationMethod(str, Enum):\n    r\n\n    BITS_AND_BYTES = \"bitsandbytes\"\n    GPTQ = \"gptq\"\n    AWQ = \"awq\"\n    AQLM = \"aqlm\"\n    QUANTO = \"quanto\"\n    EETQ = \"eetq\"\n    HQQ = \"hqq\"\n\n\ndef _get_quantization_dataset(tokenizer: \"PreTrainedTokenizer\", model_args: \"ModelArguments\") -> List[Dict[str, Any]]:\n    r\n    if os.path.isfile(model_args.export_quantization_dataset):\n        data_path = FILEEXT2TYPE.get(model_args.export_quantization_dataset.split(\".\")[-1], None)\n        data_files = model_args.export_quantization_dataset\n    else:\n        data_path = model_args.export_quantization_dataset\n        data_files = None\n\n    dataset = load_dataset(\n        path=data_path,\n        data_files=data_files,\n        split=\"train\",\n        cache_dir=model_args.cache_dir,\n        token=model_args.hf_hub_token,\n    )\n\n    samples = []\n    maxlen = model_args.export_quantization_maxlen\n    for _ in range(model_args.export_quantization_nsamples):\n        n_try = 0\n        while True:\n            if n_try > 100:\n                raise ValueError(\"Cannot find satisfying example, considering decrease `export_quantization_maxlen`.\")\n\n            sample_idx = random.randint(0, len(dataset) - 1)\n            sample: Dict[str, \"torch.Tensor\"] = tokenizer(dataset[sample_idx][\"text\"], return_tensors=\"pt\")\n            n_try += 1\n            if sample[\"input_ids\"].size(1) > maxlen:\n                break  \n\n        word_idx = random.randint(0, sample[\"input_ids\"].size(1) - maxlen - 1)\n        input_ids = sample[\"input_ids\"][:, word_idx : word_idx + maxlen]\n        attention_mask = sample[\"attention_mask\"][:, word_idx : word_idx + maxlen]\n        samples.append({\"input_ids\": input_ids.tolist(), \"attention_mask\": attention_mask.tolist()})\n\n    return samples\n\n\ndef configure_quantization(\n    config: \"PretrainedConfig\",\n    tokenizer: \"PreTrainedTokenizer\",\n    model_args: \"ModelArguments\",\n    init_kwargs: Dict[str, Any],\n) -> None:\n    r\n    if getattr(config, \"quantization_config\", None):  \n        if model_args.quantization_bit is not None:\n            logger.warning(\"`quantization_bit` will not affect on the PTQ-quantized models.\")\n\n        if is_deepspeed_zero3_enabled() or is_fsdp_enabled():\n            raise ValueError(\"DeepSpeed ZeRO-3 or FSDP is incompatible with PTQ-quantized models.\")\n\n        quantization_config: Dict[str, Any] = getattr(config, \"quantization_config\", None)\n        quant_method = quantization_config.get(\"quant_method\", \"\")\n\n        if quant_method == QuantizationMethod.GPTQ:\n            require_version(\"auto_gptq>=0.5.0\", \"To fix: pip install auto_gptq>=0.5.0\")\n            quantization_config.pop(\"disable_exllama\", None)  \n            quantization_config[\"use_exllama\"] = False  \n\n        if quant_method == QuantizationMethod.AWQ:\n            require_version(\"autoawq\", \"To fix: pip install autoawq\")\n\n        if quant_method == QuantizationMethod.AQLM:\n            require_version(\"transformers>=4.39.0\", \"To fix: pip install transformers>=4.39.0\")\n            require_version(\"aqlm>=1.1.0\", \"To fix: pip install aqlm[gpu]>=1.1.0\")\n            quantization_config[\"bits\"] = 2\n\n        quant_bits = quantization_config.get(\"bits\", \"?\")\n        logger.info(\"Loading {}-bit {}-quantized model.\".format(quant_bits, quant_method.upper()))\n\n    elif model_args.export_quantization_bit is not None:  \n        if model_args.export_quantization_bit not in [8, 4, 3, 2]:\n            raise ValueError(\"AutoGPTQ only accepts 2/3/4/8-bit quantization.\")\n\n        require_version(\"optimum>=1.17.0\", \"To fix: pip install optimum>=1.17.0\")\n        require_version(\"auto_gptq>=0.5.0\", \"To fix: pip install auto_gptq>=0.5.0\")\n        from accelerate.utils import get_max_memory\n\n        if getattr(config, \"model_type\", None) == \"chatglm\":\n            raise ValueError(\"ChatGLM model is not supported yet.\")\n\n        init_kwargs[\"quantization_config\"] = GPTQConfig(\n            bits=model_args.export_quantization_bit,\n            dataset=_get_quantization_dataset(tokenizer, model_args),\n        )\n        init_kwargs[\"device_map\"] = \"auto\"\n        init_kwargs[\"max_memory\"] = get_max_memory()\n        logger.info(\"Quantizing model to {} bit with AutoGPTQ.\".format(model_args.export_quantization_bit))\n\n    elif model_args.quantization_bit is not None:  \n        if model_args.quantization_method == QuantizationMethod.BITS_AND_BYTES.value:\n            if model_args.quantization_bit == 8:\n                require_version(\"bitsandbytes>=0.37.0\", \"To fix: pip install bitsandbytes>=0.37.0\")\n                init_kwargs[\"quantization_config\"] = BitsAndBytesConfig(load_in_8bit=True)\n            elif model_args.quantization_bit == 4:\n                require_version(\"bitsandbytes>=0.39.0\", \"To fix: pip install bitsandbytes>=0.39.0\")\n                init_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n                    load_in_4bit=True,\n                    bnb_4bit_compute_dtype=model_args.compute_dtype,\n                    bnb_4bit_use_double_quant=model_args.double_quantization,\n                    bnb_4bit_quant_type=model_args.quantization_type,\n                    bnb_4bit_quant_storage=model_args.compute_dtype,  \n                )\n            else:\n                raise ValueError(\"Bitsandbytes only accepts 4-bit or 8-bit quantization.\")\n\n            \n            \n            \n            if is_deepspeed_zero3_enabled() or is_fsdp_enabled() or model_args.quantization_device_map == \"auto\":\n                if model_args.quantization_bit != 4:\n                    raise ValueError(\"Only 4-bit quantized model can use fsdp+qlora or auto device map.\")\n\n                require_version(\"bitsandbytes>=0.43.0\", \"To fix: pip install bitsandbytes>=0.43.0\")\n            else:\n                init_kwargs[\"device_map\"] = {\"\": get_current_device()}  \n\n            logger.info(\"Quantizing model to {} bit with bitsandbytes.\".format(model_args.quantization_bit))\n        elif model_args.quantization_method == QuantizationMethod.HQQ.value:\n            if model_args.quantization_bit not in [8, 6, 5, 4, 3, 2, 1]:\n                raise ValueError(\"HQQ only accepts 1/2/3/4/5/6/8-bit quantization.\")\n\n            if is_deepspeed_zero3_enabled() or is_fsdp_enabled():\n                raise ValueError(\"HQQ quantization is incompatible with DeepSpeed ZeRO-3 or FSDP.\")\n\n            require_version(\"hqq\", \"To fix: pip install hqq\")\n            init_kwargs[\"quantization_config\"] = HqqConfig(\n                nbits=model_args.quantization_bit, quant_zero=False, quant_scale=False, axis=0\n            )  \n            logger.info(\"Quantizing model to {} bit with HQQ.\".format(model_args.quantization_bit))\n        elif model_args.quantization_method == QuantizationMethod.EETQ.value:\n            if model_args.quantization_bit != 8:\n                raise ValueError(\"EETQ only accepts 8-bit quantization.\")\n\n            if is_deepspeed_zero3_enabled() or is_fsdp_enabled():\n                raise ValueError(\"EETQ quantization is incompatible with DeepSpeed ZeRO-3 or FSDP.\")\n\n            require_version(\"eetq\", \"To fix: pip install eetq\")\n            init_kwargs[\"quantization_config\"] = EetqConfig()\n            logger.info(\"Quantizing model to {} bit with EETQ.\".format(model_args.quantization_bit))\n```"
        }
    ],
    "temperature": 0
}