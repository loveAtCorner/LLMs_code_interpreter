{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nfrom typing import TYPE_CHECKING, Any, Dict, Optional, TypedDict\n\nimport torch\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoModelForVision2Seq, AutoProcessor, AutoTokenizer\nfrom trl import AutoModelForCausalLMWithValueHead\n\nfrom ..extras.logging import get_logger\nfrom ..extras.misc import count_parameters, skip_check_imports, try_download_model_from_ms\nfrom .adapter import init_adapter\nfrom .model_utils.misc import register_autoclass\nfrom .model_utils.mod import convert_pretrained_model_to_mod, load_mod_pretrained_model\nfrom .model_utils.unsloth import load_unsloth_pretrained_model\nfrom .model_utils.valuehead import load_valuehead_params\nfrom .patcher import patch_config, patch_model, patch_tokenizer, patch_valuehead_model\n\n\nif TYPE_CHECKING:\n    from transformers import PretrainedConfig, PreTrainedModel, PreTrainedTokenizer, ProcessorMixin\n\n    from ..hparams import FinetuningArguments, ModelArguments\n\n\nlogger = get_logger(__name__)\n\n\nclass TokenizerModule(TypedDict):\n    tokenizer: \"PreTrainedTokenizer\"\n    processor: Optional[\"ProcessorMixin\"]\n\n\ndef _get_init_kwargs(model_args: \"ModelArguments\") -> Dict[str, Any]:\n    r\n    skip_check_imports()\n    model_args.model_name_or_path = try_download_model_from_ms(model_args)\n    return {\n        \"trust_remote_code\": True,\n        \"cache_dir\": model_args.cache_dir,\n        \"revision\": model_args.model_revision,\n        \"token\": model_args.hf_hub_token,\n    }\n\n\ndef load_tokenizer(model_args: \"ModelArguments\") -> \"TokenizerModule\":\n    r\n    init_kwargs = _get_init_kwargs(model_args)\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_args.model_name_or_path,\n            use_fast=model_args.use_fast_tokenizer,\n            split_special_tokens=model_args.split_special_tokens,\n            padding_side=\"right\",\n            **init_kwargs,\n        )\n    except ValueError:  \n        tokenizer = AutoTokenizer.from_pretrained(\n            model_args.model_name_or_path,\n            use_fast=True,\n            padding_side=\"right\",\n            **init_kwargs,\n        )\n\n    if model_args.new_special_tokens is not None:\n        num_added_tokens = tokenizer.add_special_tokens(\n            dict(additional_special_tokens=model_args.new_special_tokens),\n            replace_additional_special_tokens=False,\n        )\n        logger.info(\"Add {} to special tokens.\".format(\",\".join(model_args.new_special_tokens)))\n        if num_added_tokens > 0 and not model_args.resize_vocab:\n            model_args.resize_vocab = True\n            logger.warning(\"New tokens have been added, changed `resize_vocab` to True.\")\n\n    patch_tokenizer(tokenizer)\n\n    if model_args.visual_inputs:\n        try:\n            processor = AutoProcessor.from_pretrained(model_args.model_name_or_path, **init_kwargs)\n            setattr(processor, \"tokenizer\", tokenizer)\n        except Exception:\n            raise ValueError(\n                \"This multimodal LLM is not supported.\\n\"\n                \"Download LLaVA-1.5 models from: https://huggingface.co/llava-hf\\n\"\n                \"Download Yi-VL models from: https://huggingface.co/BUAADreamer\"\n            )\n    else:\n        processor = None\n\n    return {\"tokenizer\": tokenizer, \"processor\": processor}\n\n\ndef load_config(model_args: \"ModelArguments\") -> \"PretrainedConfig\":\n    r\n    init_kwargs = _get_init_kwargs(model_args)\n    return AutoConfig.from_pretrained(model_args.model_name_or_path, **init_kwargs)\n\n\ndef load_model(\n    tokenizer: \"PreTrainedTokenizer\",\n    model_args: \"ModelArguments\",\n    finetuning_args: \"FinetuningArguments\",\n    is_trainable: bool = False,\n    add_valuehead: bool = False,\n) -> \"PreTrainedModel\":\n    r\n    init_kwargs = _get_init_kwargs(model_args)\n    config = load_config(model_args)\n    patch_config(config, tokenizer, model_args, init_kwargs, is_trainable)\n\n    model = None\n    lazy_load = False\n    if model_args.use_unsloth:\n        if model_args.adapter_name_or_path is not None:\n            lazy_load = True\n        elif is_trainable:\n            model = load_unsloth_pretrained_model(config, model_args)\n\n    if model is None and not lazy_load:\n        init_kwargs[\"config\"] = config\n        init_kwargs[\"pretrained_model_name_or_path\"] = model_args.model_name_or_path\n\n        if model_args.mixture_of_depths == \"load\":\n            model = load_mod_pretrained_model(**init_kwargs)\n        elif model_args.visual_inputs:\n            model = AutoModelForVision2Seq.from_pretrained(**init_kwargs)\n        elif model_args.train_from_scratch:\n            model = AutoModelForCausalLM.from_config(config)\n        else:\n            model = AutoModelForCausalLM.from_pretrained(**init_kwargs)\n\n        if model_args.mixture_of_depths == \"convert\":\n            model = convert_pretrained_model_to_mod(model, config, model_args)\n\n    if not lazy_load:\n        patch_model(model, tokenizer, model_args, is_trainable, add_valuehead)\n        register_autoclass(config, model, tokenizer)\n\n    model = init_adapter(config, model, model_args, finetuning_args, is_trainable)\n\n    if add_valuehead:\n        model = AutoModelForCausalLMWithValueHead.from_pretrained(model)\n        patch_valuehead_model(model)\n\n        if model_args.adapter_name_or_path is not None:\n            vhead_path = model_args.adapter_name_or_path[-1]\n        else:\n            vhead_path = model_args.model_name_or_path\n\n        vhead_params = load_valuehead_params(vhead_path, model_args)\n        if vhead_params is not None:\n            model.load_state_dict(vhead_params, strict=False)\n            logger.info(\"Loaded valuehead from checkpoint: {}\".format(vhead_path))\n\n    if not is_trainable:\n        model.requires_grad_(False)\n        for param in model.parameters():\n            if param.data.dtype == torch.float32 and model_args.compute_dtype != torch.float32:\n                param.data = param.data.to(model_args.compute_dtype)\n\n        model.eval()\n    else:\n        model.train()\n\n    trainable_params, all_param = count_parameters(model)\n    if is_trainable:\n        param_stats = \"trainable params: {:,} || all params: {:,} || trainable%: {:.4f}\".format(\n            trainable_params, all_param, 100 * trainable_params / all_param\n        )\n    else:\n        param_stats = \"all params: {:,}\".format(all_param)\n\n    logger.info(param_stats)\n\n    if model_args.print_param_status:\n        for name, param in model.named_parameters():\n            print(\n                \"name: {}, dtype: {}, device: {}, trainable: {}\".format(\n                    name, param.dtype, param.device, param.requires_grad\n                )\n            )\n\n    return model\n```"
        }
    ],
    "temperature": 0
}