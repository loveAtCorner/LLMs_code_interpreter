{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nfrom typing import TYPE_CHECKING, Dict, Generator, List, Union\n\nfrom ...extras.constants import PEFT_METHODS\nfrom ...extras.misc import torch_gc\nfrom ...extras.packages import is_gradio_available\nfrom ...train.tuner import export_model\nfrom ..common import GPTQ_BITS, get_save_dir\nfrom ..locales import ALERTS\n\n\nif is_gradio_available():\n    import gradio as gr\n\n\nif TYPE_CHECKING:\n    from gradio.components import Component\n\n    from ..engine import Engine\n\n\ndef can_quantize(checkpoint_path: Union[str, List[str]]) -> \"gr.Dropdown\":\n    if isinstance(checkpoint_path, list) and len(checkpoint_path) != 0:\n        return gr.Dropdown(value=\"none\", interactive=False)\n    else:\n        return gr.Dropdown(interactive=True)\n\n\ndef save_model(\n    lang: str,\n    model_name: str,\n    model_path: str,\n    finetuning_type: str,\n    checkpoint_path: Union[str, List[str]],\n    template: str,\n    visual_inputs: bool,\n    export_size: int,\n    export_quantization_bit: int,\n    export_quantization_dataset: str,\n    export_device: str,\n    export_legacy_format: bool,\n    export_dir: str,\n    export_hub_model_id: str,\n) -> Generator[str, None, None]:\n    error = \"\"\n    if not model_name:\n        error = ALERTS[\"err_no_model\"][lang]\n    elif not model_path:\n        error = ALERTS[\"err_no_path\"][lang]\n    elif not export_dir:\n        error = ALERTS[\"err_no_export_dir\"][lang]\n    elif export_quantization_bit in GPTQ_BITS and not export_quantization_dataset:\n        error = ALERTS[\"err_no_dataset\"][lang]\n    elif export_quantization_bit not in GPTQ_BITS and not checkpoint_path:\n        error = ALERTS[\"err_no_adapter\"][lang]\n    elif export_quantization_bit in GPTQ_BITS and isinstance(checkpoint_path, list):\n        error = ALERTS[\"err_gptq_lora\"][lang]\n\n    if error:\n        gr.Warning(error)\n        yield error\n        return\n\n    args = dict(\n        model_name_or_path=model_path,\n        finetuning_type=finetuning_type,\n        template=template,\n        visual_inputs=visual_inputs,\n        export_dir=export_dir,\n        export_hub_model_id=export_hub_model_id or None,\n        export_size=export_size,\n        export_quantization_bit=int(export_quantization_bit) if export_quantization_bit in GPTQ_BITS else None,\n        export_quantization_dataset=export_quantization_dataset,\n        export_device=export_device,\n        export_legacy_format=export_legacy_format,\n    )\n\n    if checkpoint_path:\n        if finetuning_type in PEFT_METHODS:  \n            args[\"adapter_name_or_path\"] = \",\".join(\n                [get_save_dir(model_name, finetuning_type, adapter) for adapter in checkpoint_path]\n            )\n        else:  \n            args[\"model_name_or_path\"] = get_save_dir(model_name, finetuning_type, checkpoint_path)\n\n    yield ALERTS[\"info_exporting\"][lang]\n    export_model(args)\n    torch_gc()\n    yield ALERTS[\"info_exported\"][lang]\n\n\ndef create_export_tab(engine: \"Engine\") -> Dict[str, \"Component\"]:\n    with gr.Row():\n        export_size = gr.Slider(minimum=1, maximum=100, value=1, step=1)\n        export_quantization_bit = gr.Dropdown(choices=[\"none\"] + GPTQ_BITS, value=\"none\")\n        export_quantization_dataset = gr.Textbox(value=\"data/c4_demo.json\")\n        export_device = gr.Radio(choices=[\"cpu\", \"auto\"], value=\"cpu\")\n        export_legacy_format = gr.Checkbox()\n\n    with gr.Row():\n        export_dir = gr.Textbox()\n        export_hub_model_id = gr.Textbox()\n\n    checkpoint_path: gr.Dropdown = engine.manager.get_elem_by_id(\"top.checkpoint_path\")\n    checkpoint_path.change(can_quantize, [checkpoint_path], [export_quantization_bit], queue=False)\n\n    export_btn = gr.Button()\n    info_box = gr.Textbox(show_label=False, interactive=False)\n\n    export_btn.click(\n        save_model,\n        [\n            engine.manager.get_elem_by_id(\"top.lang\"),\n            engine.manager.get_elem_by_id(\"top.model_name\"),\n            engine.manager.get_elem_by_id(\"top.model_path\"),\n            engine.manager.get_elem_by_id(\"top.finetuning_type\"),\n            engine.manager.get_elem_by_id(\"top.checkpoint_path\"),\n            engine.manager.get_elem_by_id(\"top.template\"),\n            engine.manager.get_elem_by_id(\"top.visual_inputs\"),\n            export_size,\n            export_quantization_bit,\n            export_quantization_dataset,\n            export_device,\n            export_legacy_format,\n            export_dir,\n            export_hub_model_id,\n        ],\n        [info_box],\n    )\n\n    return dict(\n        export_size=export_size,\n        export_quantization_bit=export_quantization_bit,\n        export_quantization_dataset=export_quantization_dataset,\n        export_device=export_device,\n        export_legacy_format=export_legacy_format,\n        export_dir=export_dir,\n        export_hub_model_id=export_hub_model_id,\n        export_btn=export_btn,\n        info_box=info_box,\n    )\n```"
        }
    ],
    "temperature": 0
}