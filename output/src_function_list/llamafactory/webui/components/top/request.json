{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nfrom typing import TYPE_CHECKING, Dict\n\nfrom ...data import TEMPLATES\nfrom ...extras.constants import METHODS, SUPPORTED_MODELS\nfrom ...extras.packages import is_gradio_available\nfrom ..common import get_model_info, list_checkpoints, save_config\nfrom ..utils import can_quantize, can_quantize_to\n\n\nif is_gradio_available():\n    import gradio as gr\n\n\nif TYPE_CHECKING:\n    from gradio.components import Component\n\n\ndef create_top() -> Dict[str, \"Component\"]:\n    available_models = list(SUPPORTED_MODELS.keys()) + [\"Custom\"]\n\n    with gr.Row():\n        lang = gr.Dropdown(choices=[\"en\", \"ru\", \"zh\"], scale=1)\n        model_name = gr.Dropdown(choices=available_models, scale=3)\n        model_path = gr.Textbox(scale=3)\n\n    with gr.Row():\n        finetuning_type = gr.Dropdown(choices=METHODS, value=\"lora\", scale=1)\n        checkpoint_path = gr.Dropdown(multiselect=True, allow_custom_value=True, scale=6)\n\n    with gr.Accordion(open=False) as advanced_tab:\n        with gr.Row():\n            quantization_bit = gr.Dropdown(choices=[\"none\", \"8\", \"4\"], value=\"none\", allow_custom_value=True, scale=1)\n            quantization_method = gr.Dropdown(choices=[\"bitsandbytes\", \"hqq\", \"eetq\"], value=\"bitsandbytes\", scale=1)\n            template = gr.Dropdown(choices=list(TEMPLATES.keys()), value=\"default\", scale=1)\n            rope_scaling = gr.Radio(choices=[\"none\", \"linear\", \"dynamic\"], value=\"none\", scale=2)\n            booster = gr.Radio(choices=[\"auto\", \"flashattn2\", \"unsloth\"], value=\"auto\", scale=2)\n            visual_inputs = gr.Checkbox(scale=1)\n\n    model_name.change(get_model_info, [model_name], [model_path, template, visual_inputs], queue=False).then(\n        list_checkpoints, [model_name, finetuning_type], [checkpoint_path], queue=False\n    )\n    model_name.input(save_config, inputs=[lang, model_name], queue=False)\n    model_path.input(save_config, inputs=[lang, model_name, model_path], queue=False)\n    finetuning_type.change(can_quantize, [finetuning_type], [quantization_bit], queue=False).then(\n        list_checkpoints, [model_name, finetuning_type], [checkpoint_path], queue=False\n    )\n    checkpoint_path.focus(list_checkpoints, [model_name, finetuning_type], [checkpoint_path], queue=False)\n    quantization_method.change(can_quantize_to, [quantization_method], [quantization_bit], queue=False)\n\n    return dict(\n        lang=lang,\n        model_name=model_name,\n        model_path=model_path,\n        finetuning_type=finetuning_type,\n        checkpoint_path=checkpoint_path,\n        advanced_tab=advanced_tab,\n        quantization_bit=quantization_bit,\n        quantization_method=quantization_method,\n        template=template,\n        rope_scaling=rope_scaling,\n        booster=booster,\n        visual_inputs=visual_inputs,\n    )\n```"
        }
    ],
    "temperature": 0
}