{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nimport json\nimport os\nfrom typing import TYPE_CHECKING, Dict, Generator, List, Optional, Sequence, Tuple\n\nfrom numpy.typing import NDArray\n\nfrom ..chat import ChatModel\nfrom ..data import Role\nfrom ..extras.constants import PEFT_METHODS\nfrom ..extras.misc import torch_gc\nfrom ..extras.packages import is_gradio_available\nfrom .common import QUANTIZATION_BITS, get_save_dir\nfrom .locales import ALERTS\n\n\nif TYPE_CHECKING:\n    from ..chat import BaseEngine\n    from .manager import Manager\n\n\nif is_gradio_available():\n    import gradio as gr\n\n\nclass WebChatModel(ChatModel):\n    def __init__(self, manager: \"Manager\", demo_mode: bool = False, lazy_init: bool = True) -> None:\n        self.manager = manager\n        self.demo_mode = demo_mode\n        self.engine: Optional[\"BaseEngine\"] = None\n\n        if not lazy_init:  \n            super().__init__()\n\n        if demo_mode and os.environ.get(\"DEMO_MODEL\") and os.environ.get(\"DEMO_TEMPLATE\"):  \n            model_name_or_path = os.environ.get(\"DEMO_MODEL\")\n            template = os.environ.get(\"DEMO_TEMPLATE\")\n            infer_backend = os.environ.get(\"DEMO_BACKEND\", \"huggingface\")\n            super().__init__(\n                dict(model_name_or_path=model_name_or_path, template=template, infer_backend=infer_backend)\n            )\n\n    @property\n    def loaded(self) -> bool:\n        return self.engine is not None\n\n    def load_model(self, data) -> Generator[str, None, None]:\n        get = lambda elem_id: data[self.manager.get_elem_by_id(elem_id)]\n        lang, model_name, model_path = get(\"top.lang\"), get(\"top.model_name\"), get(\"top.model_path\")\n        finetuning_type, checkpoint_path = get(\"top.finetuning_type\"), get(\"top.checkpoint_path\")\n        error = \"\"\n        if self.loaded:\n            error = ALERTS[\"err_exists\"][lang]\n        elif not model_name:\n            error = ALERTS[\"err_no_model\"][lang]\n        elif not model_path:\n            error = ALERTS[\"err_no_path\"][lang]\n        elif self.demo_mode:\n            error = ALERTS[\"err_demo\"][lang]\n\n        if error:\n            gr.Warning(error)\n            yield error\n            return\n\n        if get(\"top.quantization_bit\") in QUANTIZATION_BITS:\n            quantization_bit = int(get(\"top.quantization_bit\"))\n        else:\n            quantization_bit = None\n\n        yield ALERTS[\"info_loading\"][lang]\n        args = dict(\n            model_name_or_path=model_path,\n            finetuning_type=finetuning_type,\n            quantization_bit=quantization_bit,\n            quantization_method=get(\"top.quantization_method\"),\n            template=get(\"top.template\"),\n            flash_attn=\"fa2\" if get(\"top.booster\") == \"flashattn2\" else \"auto\",\n            use_unsloth=(get(\"top.booster\") == \"unsloth\"),\n            visual_inputs=get(\"top.visual_inputs\"),\n            rope_scaling=get(\"top.rope_scaling\") if get(\"top.rope_scaling\") in [\"linear\", \"dynamic\"] else None,\n            infer_backend=get(\"infer.infer_backend\"),\n            infer_dtype=get(\"infer.infer_dtype\"),\n        )\n\n        if checkpoint_path:\n            if finetuning_type in PEFT_METHODS:  \n                args[\"adapter_name_or_path\"] = \",\".join(\n                    [get_save_dir(model_name, finetuning_type, adapter) for adapter in checkpoint_path]\n                )\n            else:  \n                args[\"model_name_or_path\"] = get_save_dir(model_name, finetuning_type, checkpoint_path)\n\n        super().__init__(args)\n        yield ALERTS[\"info_loaded\"][lang]\n\n    def unload_model(self, data) -> Generator[str, None, None]:\n        lang = data[self.manager.get_elem_by_id(\"top.lang\")]\n\n        if self.demo_mode:\n            gr.Warning(ALERTS[\"err_demo\"][lang])\n            yield ALERTS[\"err_demo\"][lang]\n            return\n\n        yield ALERTS[\"info_unloading\"][lang]\n        self.engine = None\n        torch_gc()\n        yield ALERTS[\"info_unloaded\"][lang]\n\n    def append(\n        self,\n        chatbot: List[List[Optional[str]]],\n        messages: Sequence[Dict[str, str]],\n        role: str,\n        query: str,\n    ) -> Tuple[List[List[Optional[str]]], List[Dict[str, str]], str]:\n        return chatbot + [[query, None]], messages + [{\"role\": role, \"content\": query}], \"\"\n\n    def stream(\n        self,\n        chatbot: List[List[Optional[str]]],\n        messages: Sequence[Dict[str, str]],\n        system: str,\n        tools: str,\n        image: Optional[NDArray],\n        max_new_tokens: int,\n        top_p: float,\n        temperature: float,\n    ) -> Generator[Tuple[List[List[Optional[str]]], List[Dict[str, str]]], None, None]:\n        chatbot[-1][1] = \"\"\n        response = \"\"\n        for new_text in self.stream_chat(\n            messages, system, tools, image, max_new_tokens=max_new_tokens, top_p=top_p, temperature=temperature\n        ):\n            response += new_text\n            if tools:\n                result = self.engine.template.extract_tool(response)\n            else:\n                result = response\n\n            if isinstance(result, list):\n                tool_calls = [{\"name\": tool[0], \"arguments\": json.loads(tool[1])} for tool in result]\n                tool_calls = json.dumps(tool_calls, indent=4, ensure_ascii=False)\n                output_messages = messages + [{\"role\": Role.FUNCTION.value, \"content\": tool_calls}]\n                bot_text = \"```json\\n\" + tool_calls + \"\\n```\"\n            else:\n                output_messages = messages + [{\"role\": Role.ASSISTANT.value, \"content\": result}]\n                bot_text = result\n\n            chatbot[-1][1] = bot_text\n            yield chatbot, output_messages\n```"
        }
    ],
    "temperature": 0
}