{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nfrom typing import TYPE_CHECKING, Callable, Dict, List, Optional, Tuple, Union\n\nimport torch\nfrom transformers import Trainer\nfrom transformers.integrations import is_deepspeed_zero3_enabled\nfrom transformers.optimization import get_scheduler\nfrom transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\nfrom transformers.trainer_pt_utils import get_parameter_names\n\nfrom ..extras.constants import IGNORE_INDEX\nfrom ..extras.logging import get_logger\nfrom ..extras.packages import is_galore_available\nfrom ..hparams import FinetuningArguments, ModelArguments\nfrom ..model import find_all_linear_modules, load_model, load_tokenizer, load_valuehead_params\n\n\nif is_galore_available():\n    from galore_torch import GaLoreAdafactor, GaLoreAdamW, GaLoreAdamW8bit\n\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedModel, Seq2SeqTrainingArguments\n    from trl import AutoModelForCausalLMWithValueHead\n\n    from ..hparams import DataArguments\n\n\nlogger = get_logger(__name__)\n\n\nclass DummyOptimizer(torch.optim.Optimizer):\n    r\n\n    def __init__(\n        self, lr: float = 1e-3, optimizer_dict: Optional[Dict[\"torch.nn.Parameter\", \"torch.optim.Optimizer\"]] = None\n    ) -> None:\n        dummy_tensor = torch.randn(1, 1)\n        self.optimizer_dict = optimizer_dict\n        super().__init__([dummy_tensor], {\"lr\": lr})\n\n    def zero_grad(self, set_to_none: bool = True) -> None:\n        pass\n\n    def step(self, closure: Optional[Callable[[], float]] = None) -> Optional[float]:\n        pass\n\n\ndef create_modelcard_and_push(\n    trainer: \"Trainer\",\n    model_args: \"ModelArguments\",\n    data_args: \"DataArguments\",\n    training_args: \"Seq2SeqTrainingArguments\",\n    finetuning_args: \"FinetuningArguments\",\n) -> None:\n    kwargs = {\n        \"tasks\": \"text-generation\",\n        \"finetuned_from\": model_args.model_name_or_path,\n        \"tags\": [\"llama-factory\", finetuning_args.finetuning_type],\n    }\n    if data_args.dataset is not None:\n        kwargs[\"dataset\"] = [dataset.strip() for dataset in data_args.dataset.split(\",\")]\n\n    if model_args.use_unsloth:\n        kwargs[\"tags\"] = kwargs[\"tags\"] + [\"unsloth\"]\n\n    if not training_args.do_train:\n        pass\n    elif training_args.push_to_hub:\n        trainer.push_to_hub(**kwargs)\n    else:\n        trainer.create_model_card(license=\"other\", **kwargs)  \n\n\ndef create_ref_model(\n    model_args: \"ModelArguments\", finetuning_args: \"FinetuningArguments\", add_valuehead: bool = False\n) -> Optional[Union[\"PreTrainedModel\", \"AutoModelForCausalLMWithValueHead\"]]:\n    r\n    if finetuning_args.ref_model is not None:\n        ref_model_args = ModelArguments.copyfrom(\n            model_args,\n            model_name_or_path=finetuning_args.ref_model,\n            adapter_name_or_path=finetuning_args.ref_model_adapters,\n            quantization_bit=finetuning_args.ref_model_quantization_bit,\n        )\n        ref_finetuning_args = FinetuningArguments()\n        tokenizer = load_tokenizer(ref_model_args)[\"tokenizer\"]\n        ref_model = load_model(\n            tokenizer, ref_model_args, ref_finetuning_args, is_trainable=False, add_valuehead=add_valuehead\n        )\n        logger.info(\"Created reference model from {}\".format(finetuning_args.ref_model))\n    else:\n        if finetuning_args.finetuning_type == \"lora\":\n            ref_model = None\n        else:\n            ref_model_args = ModelArguments.copyfrom(model_args)\n            ref_finetuning_args = FinetuningArguments()\n            tokenizer = load_tokenizer(ref_model_args)[\"tokenizer\"]\n            ref_model = load_model(\n                tokenizer, ref_model_args, ref_finetuning_args, is_trainable=False, add_valuehead=add_valuehead\n            )\n            logger.info(\"Created reference model from the model itself.\")\n\n    return ref_model\n\n\ndef create_reward_model(\n    model: \"AutoModelForCausalLMWithValueHead\", model_args: \"ModelArguments\", finetuning_args: \"FinetuningArguments\"\n) -> Optional[\"AutoModelForCausalLMWithValueHead\"]:\n    r\n    if finetuning_args.reward_model_type == \"api\":\n        assert finetuning_args.reward_model.startswith(\"http\"), \"Please provide full url.\"\n        logger.info(\"Use reward server {}\".format(finetuning_args.reward_model))\n        return finetuning_args.reward_model\n    elif finetuning_args.reward_model_type == \"lora\":\n        model.pretrained_model.load_adapter(finetuning_args.reward_model, \"reward\")\n        for name, param in model.named_parameters():  \n            if \"default\" in name:\n                param.data = param.data.to(torch.float32)  \n        vhead_params = load_valuehead_params(finetuning_args.reward_model, model_args)\n        assert vhead_params is not None, \"Reward model is not correctly loaded.\"\n        model.register_buffer(\"reward_head_weight\", vhead_params[\"v_head.summary.weight\"], persistent=False)\n        model.register_buffer(\"reward_head_bias\", vhead_params[\"v_head.summary.bias\"], persistent=False)\n        model.register_buffer(\n            \"default_head_weight\", torch.zeros_like(vhead_params[\"v_head.summary.weight\"]), persistent=False\n        )\n        model.register_buffer(\n            \"default_head_bias\", torch.zeros_like(vhead_params[\"v_head.summary.bias\"]), persistent=False\n        )\n        logger.info(\"Loaded adapter weights of reward model from {}\".format(finetuning_args.reward_model))\n        return None\n    else:\n        reward_model_args = ModelArguments.copyfrom(\n            model_args,\n            model_name_or_path=finetuning_args.reward_model,\n            adapter_name_or_path=finetuning_args.reward_model_adapters,\n            quantization_bit=finetuning_args.reward_model_quantization_bit,\n        )\n        reward_finetuning_args = FinetuningArguments()\n        tokenizer = load_tokenizer(reward_model_args)[\"tokenizer\"]\n        reward_model = load_model(\n            tokenizer, reward_model_args, reward_finetuning_args, is_trainable=False, add_valuehead=True\n        )\n        logger.info(\"Loaded full weights of reward model from {}\".format(finetuning_args.reward_model))\n        logger.warning(\"Please ensure the ppo model and reward model share SAME tokenizer and vocabulary.\")\n        return reward_model\n\n\ndef _get_decay_parameter_names(model: \"PreTrainedModel\") -> List[str]:\n    r\n    decay_parameters = get_parameter_names(model, ALL_LAYERNORM_LAYERS)\n    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n    return decay_parameters\n\n\ndef _create_galore_optimizer(\n    model: \"PreTrainedModel\",\n    training_args: \"Seq2SeqTrainingArguments\",\n    finetuning_args: \"FinetuningArguments\",\n) -> \"torch.optim.Optimizer\":\n    if len(finetuning_args.galore_target) == 1 and finetuning_args.galore_target[0] == \"all\":\n        galore_targets = find_all_linear_modules(model, finetuning_args.freeze_vision_tower)\n    else:\n        galore_targets = finetuning_args.galore_target\n\n    galore_params: List[\"torch.nn.Parameter\"] = []\n    for name, module in model.named_modules():\n        if isinstance(module, torch.nn.Linear) and any(target in name for target in galore_targets):\n            for param in module.parameters():\n                if param.requires_grad and len(param.shape) > 1:\n                    galore_params.append(param)\n\n    galore_kwargs = {\n        \"rank\": finetuning_args.galore_rank,\n        \"update_proj_gap\": finetuning_args.galore_update_interval,\n        \"scale\": finetuning_args.galore_scale,\n        \"proj_type\": finetuning_args.galore_proj_type,\n    }\n\n    id_galore_params = {id(param) for param in galore_params}\n    decay_params, nodecay_params = [], []  \n    trainable_params: List[\"torch.nn.Parameter\"] = []  \n    decay_param_names = _get_decay_parameter_names(model)\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            trainable_params.append(param)\n            if id(param) not in id_galore_params:\n                if name in decay_param_names:\n                    decay_params.append(param)\n                else:\n                    nodecay_params.append(param)\n\n    _, optim_kwargs = Trainer.get_optimizer_cls_and_kwargs(training_args)\n\n    if training_args.optim == \"adamw_torch\":\n        optim_class = GaLoreAdamW\n    elif training_args.optim in [\"adamw_bnb_8bit\", \"adamw_8bit\", \"paged_adamw_8bit\"]:\n        optim_class = GaLoreAdamW8bit\n    elif training_args.optim == \"adafactor\":\n        optim_class = GaLoreAdafactor\n    else:\n        raise NotImplementedError(\"Unknow optim: {}\".format(training_args.optim))\n\n    if finetuning_args.galore_layerwise:\n        if training_args.gradient_accumulation_steps != 1:\n            raise ValueError(\"Per-layer GaLore does not support gradient accumulation.\")\n\n        optimizer_dict: Dict[\"torch.Tensor\", \"torch.optim.Optimizer\"] = {}\n        for param in nodecay_params:\n            param_groups = [dict(params=[param], weight_decay=0.0)]\n            optimizer_dict[param] = optim_class(param_groups, **optim_kwargs)\n        for param in decay_params:\n            param_groups = [dict(params=[param], weight_decay=training_args.weight_decay)]\n            optimizer_dict[param] = optim_class(param_groups, **optim_kwargs)\n        for param in galore_params:  \n            param_groups = [dict(params=[param], weight_decay=training_args.weight_decay, **galore_kwargs)]\n            optimizer_dict[param] = optim_class(param_groups, **optim_kwargs)\n\n        def optimizer_hook(param: \"torch.nn.Parameter\"):\n            if param.grad is not None:\n                optimizer_dict[param].step()\n                optimizer_dict[param].zero_grad()\n\n        for param in trainable_params:\n            param.register_post_accumulate_grad_hook(optimizer_hook)\n\n        optimizer = DummyOptimizer(lr=training_args.learning_rate, optimizer_dict=optimizer_dict)\n    else:\n        param_groups = [\n            dict(params=nodecay_params, weight_decay=0.0),\n            dict(params=decay_params, weight_decay=training_args.weight_decay),\n            dict(params=galore_params, weight_decay=training_args.weight_decay, **galore_kwargs),\n        ]\n        optimizer = optim_class(param_groups, **optim_kwargs)\n\n    logger.info(\"Using GaLore optimizer, may cause hanging at the start of training, wait patiently.\")\n    return optimizer\n\n\ndef _create_loraplus_optimizer(\n    model: \"PreTrainedModel\",\n    training_args: \"Seq2SeqTrainingArguments\",\n    finetuning_args: \"FinetuningArguments\",\n) -> \"torch.optim.Optimizer\":\n    default_lr = training_args.learning_rate\n    loraplus_lr = training_args.learning_rate * finetuning_args.loraplus_lr_ratio\n    embedding_lr = finetuning_args.loraplus_lr_embedding\n\n    decay_param_names = _get_decay_parameter_names(model)\n    param_dict: Dict[str, List[\"torch.nn.Parameter\"]] = {\n        \"lora_a\": [],\n        \"lora_b\": [],\n        \"lora_b_nodecay\": [],\n        \"embedding\": [],\n    }\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            if \"lora_embedding_B\" in name:\n                param_dict[\"embedding\"].append(param)\n            elif \"lora_B\" in name or param.ndim == 1:\n                if name in decay_param_names:\n                    param_dict[\"lora_b\"].append(param)\n                else:\n                    param_dict[\"lora_b_nodecay\"].append(param)\n            else:\n                param_dict[\"lora_a\"].append(param)\n\n    optim_class, optim_kwargs = Trainer.get_optimizer_cls_and_kwargs(training_args)\n    param_groups = [\n        dict(params=param_dict[\"lora_a\"], lr=default_lr, weight_decay=training_args.weight_decay),\n        dict(params=param_dict[\"lora_b\"], lr=loraplus_lr, weight_decay=training_args.weight_decay),\n        dict(params=param_dict[\"lora_b_nodecay\"], lr=loraplus_lr, weight_decay=0.0),\n        dict(params=param_dict[\"embedding\"], lr=embedding_lr, weight_decay=training_args.weight_decay),\n    ]\n    optimizer = optim_class(param_groups, **optim_kwargs)\n    logger.info(\"Using LoRA+ optimizer with loraplus lr ratio {:.2f}.\".format(finetuning_args.loraplus_lr_ratio))\n    return optimizer\n\n\ndef _create_badam_optimizer(\n    model: \"PreTrainedModel\",\n    training_args: \"Seq2SeqTrainingArguments\",\n    finetuning_args: \"FinetuningArguments\",\n) -> \"torch.optim.Optimizer\":\n    decay_params, nodecay_params = [], []\n    decay_param_names = _get_decay_parameter_names(model)\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            if name in decay_param_names:\n                decay_params.append(param)\n            else:\n                nodecay_params.append(param)\n\n    optim_class, optim_kwargs = Trainer.get_optimizer_cls_and_kwargs(training_args)\n    param_groups = [\n        dict(params=nodecay_params, weight_decay=0.0),\n        dict(params=decay_params, weight_decay=training_args.weight_decay),\n    ]\n\n    if finetuning_args.badam_mode == \"layer\":\n        from badam import BlockOptimizer\n\n        base_optimizer = optim_class(param_groups, **optim_kwargs)\n        optimizer = BlockOptimizer(\n            base_optimizer=base_optimizer,\n            named_parameters_list=list(model.named_parameters()),\n            block_prefix_list=None,\n            switch_block_every=finetuning_args.badam_switch_interval,\n            start_block=finetuning_args.badam_start_block,\n            switch_mode=finetuning_args.badam_switch_mode,\n            verbose=finetuning_args.badam_verbose,\n            ds_zero3_enabled=is_deepspeed_zero3_enabled(),\n        )\n        logger.info(\n            f\"Using BAdam optimizer with layer-wise update, switch mode is {finetuning_args.badam_switch_mode}, \"\n            f\"switch block every {finetuning_args.badam_switch_interval} steps, \"\n            f\"default start block is {finetuning_args.badam_start_block}\"\n        )\n\n    elif finetuning_args.badam_mode == \"ratio\":\n        from badam import BlockOptimizerRatio\n\n        assert finetuning_args.badam_update_ratio > 1e-6\n        optimizer = BlockOptimizerRatio(\n            param_groups=param_groups,\n            named_parameters_list=list(model.named_parameters()),\n            update_ratio=finetuning_args.badam_update_ratio,\n            mask_mode=finetuning_args.badam_mask_mode,\n            verbose=finetuning_args.badam_verbose,\n            include_embedding=False,\n            **optim_kwargs,\n        )\n        logger.info(\n            f\"Using BAdam optimizer with ratio-based update, update ratio is {finetuning_args.badam_update_ratio}, \"\n            f\"mask mode is {finetuning_args.badam_mask_mode}\"\n        )\n\n    return optimizer\n\n\ndef create_custom_optimzer(\n    model: \"PreTrainedModel\",\n    training_args: \"Seq2SeqTrainingArguments\",\n    finetuning_args: \"FinetuningArguments\",\n) -> Optional[\"torch.optim.Optimizer\"]:\n    if finetuning_args.use_galore:\n        return _create_galore_optimizer(model, training_args, finetuning_args)\n\n    if finetuning_args.loraplus_lr_ratio is not None:\n        return _create_loraplus_optimizer(model, training_args, finetuning_args)\n\n    if finetuning_args.use_badam:\n        return _create_badam_optimizer(model, training_args, finetuning_args)\n\n\ndef create_custom_scheduler(\n    training_args: \"Seq2SeqTrainingArguments\",\n    num_training_steps: int,\n    optimizer: Optional[\"torch.optim.Optimizer\"] = None,\n) -> None:\n    if optimizer is not None and isinstance(optimizer, DummyOptimizer):\n        optimizer_dict = optimizer.optimizer_dict\n        scheduler_dict: Dict[\"torch.nn.Parameter\", \"torch.optim.lr_scheduler.LRScheduler\"] = {}\n\n        for param in optimizer_dict.keys():\n            scheduler_dict[param] = get_scheduler(\n                training_args.lr_scheduler_type,\n                optimizer=optimizer_dict[param],\n                num_warmup_steps=training_args.get_warmup_steps(num_training_steps),\n                num_training_steps=num_training_steps,\n                scheduler_specific_kwargs=training_args.lr_scheduler_kwargs,\n            )\n\n        def scheduler_hook(param: \"torch.nn.Parameter\"):\n            scheduler_dict[param].step()\n\n        for param in optimizer_dict.keys():\n            param.register_post_accumulate_grad_hook(scheduler_hook)\n\n\ndef get_batch_logps(\n    logits: \"torch.Tensor\", labels: \"torch.Tensor\", label_pad_token_id: int = IGNORE_INDEX\n) -> Tuple[\"torch.Tensor\", \"torch.Tensor\"]:\n    r\n    if logits.shape[:-1] != labels.shape:\n        raise ValueError(\"Logits (batchsize x seqlen) and labels must have the same shape.\")\n\n    labels = labels[:, 1:].clone()\n    logits = logits[:, :-1, :]\n    loss_mask = labels != label_pad_token_id\n    labels[labels == label_pad_token_id] = 0  \n    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n    return (per_token_logps * loss_mask).sum(-1), loss_mask.sum(-1)\n```"
        }
    ],
    "temperature": 0
}