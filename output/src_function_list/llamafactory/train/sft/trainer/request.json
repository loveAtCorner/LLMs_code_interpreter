{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nimport json\nimport os\nfrom types import MethodType\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom transformers import Seq2SeqTrainer\n\nfrom ...extras.constants import IGNORE_INDEX\nfrom ...extras.logging import get_logger\nfrom ..callbacks import PissaConvertCallback, SaveProcessorCallback\nfrom ..trainer_utils import create_custom_optimzer, create_custom_scheduler\n\n\nif TYPE_CHECKING:\n    from torch.utils.data import Dataset\n    from transformers import ProcessorMixin\n    from transformers.trainer import PredictionOutput\n\n    from ...hparams import FinetuningArguments\n\n\nlogger = get_logger(__name__)\n\n\nclass CustomSeq2SeqTrainer(Seq2SeqTrainer):\n    r\n\n    def __init__(\n        self, finetuning_args: \"FinetuningArguments\", processor: Optional[\"ProcessorMixin\"], **kwargs\n    ) -> None:\n        super().__init__(**kwargs)\n        self.finetuning_args = finetuning_args\n\n        if processor is not None:\n            self.add_callback(SaveProcessorCallback(processor))\n\n        if finetuning_args.pissa_convert:\n            self.add_callback(PissaConvertCallback)\n\n        if finetuning_args.use_badam:\n            from badam import BAdamCallback, clip_grad_norm_old_version\n\n            self.accelerator.clip_grad_norm_ = MethodType(clip_grad_norm_old_version, self.accelerator)\n            self.add_callback(BAdamCallback)\n\n    def create_optimizer(self) -> \"torch.optim.Optimizer\":\n        if self.optimizer is None:\n            self.optimizer = create_custom_optimzer(self.model, self.args, self.finetuning_args)\n        return super().create_optimizer()\n\n    def create_scheduler(\n        self, num_training_steps: int, optimizer: Optional[\"torch.optim.Optimizer\"] = None\n    ) -> \"torch.optim.lr_scheduler.LRScheduler\":\n        create_custom_scheduler(self.args, num_training_steps, optimizer)\n        return super().create_scheduler(num_training_steps, optimizer)\n\n    def prediction_step(\n        self,\n        model: \"torch.nn.Module\",\n        inputs: Dict[str, Union[torch.Tensor, Any]],\n        prediction_loss_only: bool,\n        ignore_keys: Optional[List[str]] = None,\n    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n        r\n        labels = inputs[\"labels\"].detach().clone() if \"labels\" in inputs else None  \n        if self.args.predict_with_generate:\n            assert self.tokenizer.padding_side == \"left\", \"This method only accepts left-padded tensor.\"\n            prompt_len, label_len = inputs[\"input_ids\"].size(-1), inputs[\"labels\"].size(-1)\n            if prompt_len > label_len:\n                inputs[\"labels\"] = self._pad_tensors_to_target_len(inputs[\"labels\"], inputs[\"input_ids\"])\n            if label_len > prompt_len:  \n                inputs[\"labels\"] = inputs[\"labels\"][:, :prompt_len]\n\n        loss, generated_tokens, _ = super().prediction_step(  \n            model, inputs, prediction_loss_only=prediction_loss_only, ignore_keys=ignore_keys\n        )\n        if generated_tokens is not None and self.args.predict_with_generate:\n            generated_tokens[:, :prompt_len] = self.tokenizer.pad_token_id\n            generated_tokens = generated_tokens.contiguous()\n\n        return loss, generated_tokens, labels\n\n    def _pad_tensors_to_target_len(self, src_tensor: torch.Tensor, tgt_tensor: torch.Tensor) -> torch.Tensor:\n        r\n        assert self.tokenizer.pad_token_id is not None, \"Pad token is required.\"\n        padded_tensor = self.tokenizer.pad_token_id * torch.ones_like(tgt_tensor)\n        padded_tensor[:, -src_tensor.shape[-1] :] = src_tensor  \n        return padded_tensor.contiguous()  \n\n    def save_predictions(self, dataset: \"Dataset\", predict_results: \"PredictionOutput\") -> None:\n        r\n        if not self.is_world_process_zero():\n            return\n\n        output_prediction_file = os.path.join(self.args.output_dir, \"generated_predictions.jsonl\")\n        logger.info(f\"Saving prediction results to {output_prediction_file}\")\n\n        labels = np.where(\n            predict_results.label_ids != IGNORE_INDEX, predict_results.label_ids, self.tokenizer.pad_token_id\n        )\n        preds = np.where(\n            predict_results.predictions != IGNORE_INDEX, predict_results.predictions, self.tokenizer.pad_token_id\n        )\n\n        for i in range(len(preds)):\n            pad_len = np.nonzero(preds[i] != self.tokenizer.pad_token_id)[0]\n            if len(pad_len):  \n                preds[i] = np.concatenate((preds[i][pad_len[0] :], preds[i][: pad_len[0]]), axis=-1)\n\n        decoded_inputs = self.tokenizer.batch_decode(dataset[\"input_ids\"], skip_special_tokens=True)\n        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n        decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n        with open(output_prediction_file, \"w\", encoding=\"utf-8\") as writer:\n            res: List[str] = []\n            for text, label, pred in zip(decoded_inputs, decoded_labels, decoded_preds):\n                res.append(json.dumps({\"prompt\": text, \"label\": label, \"predict\": pred}, ensure_ascii=False))\n\n            writer.write(\"\\n\".join(res))\n```"
        }
    ],
    "temperature": 0
}