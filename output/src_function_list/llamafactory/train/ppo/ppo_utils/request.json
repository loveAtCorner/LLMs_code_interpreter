{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nimport json\nfrom contextlib import nullcontext\nfrom typing import TYPE_CHECKING, Dict, List, Literal, Optional\n\nimport torch\nfrom transformers.integrations import is_deepspeed_zero3_enabled\n\nfrom ...extras.packages import is_requests_available\n\n\nif is_requests_available():\n    import requests\n\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedModel\n    from trl import AutoModelForCausalLMWithValueHead\n\n\ndef get_rewards_from_server(server_url: str, messages: List[str]) -> List[torch.Tensor]:\n    r\n    headers = {\"Content-Type\": \"application/json\"}\n    payload = {\"model\": \"model\", \"messages\": messages}\n    response = requests.post(server_url, json=payload, headers=headers)\n    rewards = json.loads(response.text)[\"scores\"]\n    return torch.Tensor(rewards)\n\n\ndef replace_model(model: \"AutoModelForCausalLMWithValueHead\", target: Literal[\"default\", \"reward\"]) -> None:\n    r\n    v_head_layer = model.v_head.summary\n    if is_deepspeed_zero3_enabled():\n        import deepspeed  \n\n        params = [v_head_layer.weight, v_head_layer.bias]\n        context_maybe_zero3 = deepspeed.zero.GatheredParameters(params, modifier_rank=0)\n    else:\n        context_maybe_zero3 = nullcontext()\n\n    model.pretrained_model.set_adapter(target)  \n    with context_maybe_zero3:\n        if target == \"reward\":  \n            setattr(model, \"default_head_weight\", v_head_layer.weight.data.detach().clone())\n            setattr(model, \"default_head_bias\", v_head_layer.bias.data.detach().clone())\n\n        device = v_head_layer.weight.device\n        v_head_layer.weight.data = model.get_buffer(\"{}_head_weight\".format(target)).detach().clone().to(device)\n        v_head_layer.bias.data = model.get_buffer(\"{}_head_bias\".format(target)).detach().clone().to(device)\n\n\ndef dump_layernorm(model: \"PreTrainedModel\") -> Dict[str, torch.Tensor]:\n    r\n    layer_norm_params = {}\n    for name, param in model.named_parameters():\n        if param.data.dtype == torch.float32:\n            layer_norm_params[name] = param.data.detach().clone()\n            param.data = param.data.to(model.config.torch_dtype)\n\n    return layer_norm_params\n\n\ndef restore_layernorm(model: \"PreTrainedModel\", layernorm_params: Optional[Dict[str, torch.Tensor]] = None) -> None:\n    r\n    for name, param in model.named_parameters():\n        if name in layernorm_params:\n            param.data = layernorm_params[name]\n```"
        }
    ],
    "temperature": 0
}