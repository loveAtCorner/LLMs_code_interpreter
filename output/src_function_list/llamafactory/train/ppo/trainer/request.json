{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nimport math\nimport os\nimport sys\nimport warnings\nfrom types import MethodType\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple\n\nimport torch\nfrom accelerate.utils import DistributedDataParallelKwargs\nfrom tqdm import tqdm\nfrom transformers import GenerationConfig, Trainer, TrainerControl, TrainerState\nfrom transformers.optimization import get_scheduler\nfrom transformers.trainer import DEFAULT_CALLBACKS\nfrom transformers.trainer_callback import CallbackHandler\nfrom transformers.trainer_pt_utils import remove_dummy_checkpoint\nfrom transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\nfrom transformers.utils import SAFE_WEIGHTS_NAME, WEIGHTS_NAME\nfrom trl import PPOConfig, PPOTrainer\nfrom trl.core import PPODecorators, logprobs_from_logits\nfrom trl.models.utils import unwrap_model_for_generation\n\nfrom ...extras.logging import get_logger\nfrom ...extras.misc import AverageMeter, count_parameters, get_current_device, get_logits_processor\nfrom ..callbacks import FixValueHeadModelCallback, SaveProcessorCallback\nfrom ..trainer_utils import create_custom_optimzer, create_custom_scheduler\nfrom .ppo_utils import dump_layernorm, get_rewards_from_server, replace_model, restore_layernorm\n\n\nif TYPE_CHECKING:\n    from datasets import Dataset\n    from transformers import (\n        DataCollatorWithPadding,\n        PreTrainedTokenizer,\n        ProcessorMixin,\n        Seq2SeqTrainingArguments,\n        TrainerCallback,\n    )\n    from trl import AutoModelForCausalLMWithValueHead\n\n    from ...hparams import FinetuningArguments, GeneratingArguments, ModelArguments\n\n\nlogger = get_logger(__name__)\n\n\nclass CustomPPOTrainer(PPOTrainer, Trainer):\n    r\n\n    def __init__(\n        self,\n        model_args: \"ModelArguments\",\n        training_args: \"Seq2SeqTrainingArguments\",\n        finetuning_args: \"FinetuningArguments\",\n        generating_args: \"GeneratingArguments\",\n        callbacks: Optional[List[\"TrainerCallback\"]],\n        model: \"AutoModelForCausalLMWithValueHead\",\n        reward_model: Optional[\"AutoModelForCausalLMWithValueHead\"],\n        ref_model: Optional[\"AutoModelForCausalLMWithValueHead\"],\n        tokenizer: \"PreTrainedTokenizer\",\n        processor: Optional[\"ProcessorMixin\"],\n        dataset: \"Dataset\",\n        data_collator: \"DataCollatorWithPadding\",\n    ) -> None:\n        backward_batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n        ppo_config = PPOConfig(\n            model_name=model_args.model_name_or_path,\n            learning_rate=training_args.learning_rate,\n            mini_batch_size=training_args.per_device_train_batch_size,\n            batch_size=backward_batch_size * finetuning_args.ppo_buffer_size,\n            gradient_accumulation_steps=training_args.gradient_accumulation_steps,\n            ppo_epochs=finetuning_args.ppo_epochs,\n            max_grad_norm=training_args.max_grad_norm,\n            seed=training_args.seed,\n            optimize_device_cache=True,\n            target=finetuning_args.ppo_target,\n            use_score_scaling=finetuning_args.ppo_score_norm,\n            use_score_norm=finetuning_args.ppo_score_norm,\n            whiten_rewards=finetuning_args.ppo_whiten_rewards,\n            accelerator_kwargs={\"step_scheduler_with_optimizer\": False},\n            log_with=training_args.report_to[0] if training_args.report_to else None,\n            project_kwargs={\"logging_dir\": training_args.logging_dir},\n        )\n\n        \n        if training_args.deepspeed_plugin is not None:\n            ppo_config.accelerator_kwargs[\"kwargs_handlers\"] = [\n                DistributedDataParallelKwargs(find_unused_parameters=training_args.ddp_find_unused_parameters)\n            ]\n            ppo_config.accelerator_kwargs[\"deepspeed_plugin\"] = training_args.deepspeed_plugin\n            if ppo_config.log_with == \"tensorboard\":  \n                ppo_config.log_with = None\n\n        \n        if training_args.max_steps > 0:\n            num_training_steps = training_args.max_steps\n        else:\n            total_train_batch_size = backward_batch_size * finetuning_args.ppo_buffer_size * training_args.world_size\n            num_training_steps = training_args.num_train_epochs * math.ceil(len(dataset) / total_train_batch_size)\n\n        optimizer = self.create_optimizer(model, training_args, finetuning_args)\n        scheduler = self.create_scheduler(training_args, num_training_steps, optimizer)\n\n        PPOTrainer.__init__(\n            self,\n            config=ppo_config,\n            model=model,\n            ref_model=ref_model,\n            tokenizer=tokenizer,\n            dataset=dataset,\n            data_collator=data_collator,\n            lr_scheduler=scheduler,\n        )\n\n        self.args = training_args\n        self.model_args = model_args\n        self.finetuning_args = finetuning_args\n        self.reward_model = reward_model\n        self.current_device = get_current_device()  \n\n        self.generation_config = GenerationConfig(\n            pad_token_id=self.tokenizer.pad_token_id,\n            eos_token_id=[self.tokenizer.eos_token_id] + self.tokenizer.additional_special_tokens_ids,\n            **generating_args.to_dict(),\n        )\n\n        self.state = TrainerState()\n        self.control = TrainerControl()\n        self.is_deepspeed_enabled = getattr(self.accelerator.state, \"deepspeed_plugin\", None) is not None\n        self.is_fsdp_enabled = getattr(self.accelerator.state, \"fsdp_plugin\", None) is not None\n        callbacks = DEFAULT_CALLBACKS if callbacks is None else DEFAULT_CALLBACKS + callbacks\n        self.callback_handler = CallbackHandler(\n            callbacks, self.accelerator.unwrap_model(self.model), self.tokenizer, self.optimizer, self.lr_scheduler\n        )\n        if self.args.max_steps > 0:\n            logger.info(\"max_steps is given, it will override any value given in num_train_epochs\")\n\n        self.amp_context = torch.autocast(self.current_device.type)\n        warnings.simplefilter(\"ignore\")  \n\n        if finetuning_args.reward_model_type == \"full\":\n            if self.is_deepspeed_enabled:\n                if not (\n                    getattr(reward_model.pretrained_model, \"is_loaded_in_8bit\", False)\n                    or getattr(reward_model.pretrained_model, \"is_loaded_in_4bit\", False)\n                ):  \n                    self.reward_model = self._prepare_deepspeed(self.reward_model)\n            else:\n                self.reward_model = self.accelerator.prepare_model(self.reward_model, evaluation_mode=True)\n\n        self.add_callback(FixValueHeadModelCallback)\n\n        if processor is not None:\n            self.add_callback(SaveProcessorCallback(processor))\n\n        if finetuning_args.use_badam:\n            from badam import BAdamCallback, clip_grad_norm_old_version\n\n            self.accelerator.clip_grad_norm_ = MethodType(clip_grad_norm_old_version, self.accelerator)\n            self.add_callback(BAdamCallback)\n\n    def ppo_train(self, resume_from_checkpoint: Optional[str] = None) -> None:\n        r\n        if resume_from_checkpoint is not None:\n            raise ValueError(\"`resume_from_checkpoint` will be supported in the future version.\")\n\n        total_train_batch_size = (\n            self.args.per_device_train_batch_size\n            * self.args.gradient_accumulation_steps\n            * self.finetuning_args.ppo_buffer_size\n            * self.args.world_size\n        )\n        if self.args.max_steps > 0:\n            num_examples = total_train_batch_size * self.args.max_steps\n            num_train_epochs = sys.maxsize\n            max_steps = self.args.max_steps\n            steps_in_epoch = self.args.max_steps\n        else:\n            len_dataloader = len(self.dataloader)\n            num_examples = len(self.dataset)\n            num_train_epochs = self.args.num_train_epochs\n            max_steps = math.ceil(num_train_epochs * len_dataloader)\n            steps_in_epoch = len_dataloader\n\n        self.state.max_steps = max_steps\n        self.state.num_train_epochs = num_train_epochs\n        self.state.is_local_process_zero = self.is_local_process_zero()\n        self.state.is_world_process_zero = self.is_world_process_zero()\n\n        if self.is_world_process_zero():\n            logger.info(\"***** Running training *****\")\n            logger.info(\"  Num examples = {:,}\".format(num_examples))\n            logger.info(\"  Num Epochs = {:,}\".format(num_train_epochs))\n            logger.info(\"  Instantaneous batch size per device = {:,}\".format(self.args.per_device_train_batch_size))\n            logger.info(\n                \"  Total train batch size (w. parallel, buffer, distributed & accumulation) = {:,}\".format(\n                    total_train_batch_size\n                )\n            )\n            logger.info(\"  Gradient Accumulation steps = {:,}\".format(self.args.gradient_accumulation_steps))\n            logger.info(\"  Num optimization epochs per batch = {:,}\".format(self.finetuning_args.ppo_epochs))\n            logger.info(\"  Total training steps = {:,}\".format(max_steps))\n            logger.info(\"  Number of trainable parameters = {:,}\".format(count_parameters(self.model)[0]))\n\n        dataiter = iter(self.dataloader)\n        loss_meter = AverageMeter()\n        reward_meter = AverageMeter()\n        self.callback_handler.on_train_begin(self.args, self.state, self.control)\n\n        for step in tqdm(range(max_steps), disable=not self.is_local_process_zero()):\n            try:\n                batch = next(dataiter)\n            except StopIteration:\n                dataiter = iter(self.dataloader)\n                batch = next(dataiter)\n\n            \n            self.model.eval()\n            self.tokenizer.padding_side = \"right\"  \n            queries, responses, rewards = [], [], []\n            for idx in range(0, self.config.batch_size, self.config.mini_batch_size):\n                mini_batch_queries, mini_batch_responses = self.get_inputs(\n                    batch[idx : idx + self.config.mini_batch_size]\n                )\n                mini_batch_rewards = self.get_rewards(mini_batch_queries, mini_batch_responses)\n                queries.extend(mini_batch_queries)\n                responses.extend(mini_batch_responses)\n                rewards.extend(mini_batch_rewards)\n\n            \n            self.model.train()\n            stats = self.step(queries, responses, rewards)\n            self.tokenizer.padding_side = \"left\"  \n            loss_meter.update(float(stats[\"ppo/loss/total\"]), n=len(rewards))\n            reward_meter.update(torch.stack(rewards).mean().item(), n=len(rewards))\n\n            if self.config.log_with is not None:\n                try:\n                    batch[\"query\"] = self.tokenizer.batch_decode(queries, skip_special_tokens=True)\n                    batch[\"response\"] = self.tokenizer.batch_decode(responses, skip_special_tokens=True)\n                    self.log_stats(stats, batch, rewards)\n                except Exception:\n                    logger.warning(\"Failed to save stats due to unknown errors.\")\n\n            self.state.global_step += 1\n            self.callback_handler.on_step_end(self.args, self.state, self.control)\n\n            if self.is_local_process_zero() and (step + 1) % self.args.logging_steps == 0:\n                logs = dict(\n                    loss=round(loss_meter.avg, 4),\n                    reward=round(reward_meter.avg, 4),\n                    learning_rate=stats[\"ppo/learning_rate\"],\n                    epoch=round(step / steps_in_epoch, 2),\n                )\n                tqdm.write(str(logs))\n                logs[\"step\"] = step\n                self.state.log_history.append(logs)\n                self.callback_handler.on_log(self.args, self.state, self.control, logs)\n                loss_meter.reset()\n                reward_meter.reset()\n\n            if (step + 1) % self.args.save_steps == 0:  \n                self.save_model(\n                    os.path.join(self.args.output_dir, \"{}-{}\".format(PREFIX_CHECKPOINT_DIR, self.state.global_step))\n                )\n                self.callback_handler.on_save(self.args, self.state, self.control)\n\n            if self.control.should_epoch_stop or self.control.should_training_stop:\n                break\n\n        self.callback_handler.on_train_end(self.args, self.state, self.control)\n\n    def create_optimizer(\n        self,\n        model: \"AutoModelForCausalLMWithValueHead\",\n        training_args: \"Seq2SeqTrainingArguments\",\n        finetuning_args: \"FinetuningArguments\",\n    ) -> \"torch.optim.Optimizer\":\n        optimizer = create_custom_optimzer(model, training_args, finetuning_args)\n        if optimizer is None:\n            decay_params, nodecay_params = [], []\n            decay_param_names = self.get_decay_parameter_names(model)\n            for name, param in model.named_parameters():\n                if param.requires_grad:\n                    if name in decay_param_names:\n                        decay_params.append(param)\n                    else:\n                        nodecay_params.append(param)\n\n            optim_class, optim_kwargs = Trainer.get_optimizer_cls_and_kwargs(training_args)\n            param_groups = [\n                dict(params=nodecay_params),\n                dict(params=decay_params, weight_decay=training_args.weight_decay),\n            ]\n            optimizer = optim_class(param_groups, **optim_kwargs)\n\n        return optimizer\n\n    def create_scheduler(\n        self, training_args: \"Seq2SeqTrainingArguments\", num_training_steps: int, optimizer: \"torch.optim.Optimizer\"\n    ) -> \"torch.optim.lr_scheduler.LRScheduler\":\n        create_custom_scheduler(training_args, num_training_steps, optimizer)\n        lr_scheduler = get_scheduler(\n            training_args.lr_scheduler_type,\n            optimizer=optimizer,\n            num_warmup_steps=training_args.get_warmup_steps(num_training_steps),\n            num_training_steps=num_training_steps,\n        )\n        return lr_scheduler\n\n    @torch.no_grad()\n    def get_inputs(self, batch: Dict[str, \"torch.Tensor\"]) -> Tuple[List[\"torch.Tensor\"], List[\"torch.Tensor\"]]:\n        r\n        if batch[\"input_ids\"].size(0) == 1:  \n            start_index = (batch[\"input_ids\"][0] != self.tokenizer.pad_token_id).nonzero()[0].item()\n            for k, v in batch.items():\n                batch[k] = v[:, start_index:]\n\n        with unwrap_model_for_generation(self.model, self.accelerator) as unwrapped_model:\n            unwrapped_model: \"AutoModelForCausalLMWithValueHead\" = self.accelerator.unwrap_model(self.model)\n            if self.model_args.upcast_layernorm:\n                layernorm_params = dump_layernorm(unwrapped_model)\n\n            generate_output: \"torch.Tensor\" = unwrapped_model.generate(\n                generation_config=self.generation_config, logits_processor=get_logits_processor(), **batch\n            )\n            if self.model_args.upcast_layernorm:\n                restore_layernorm(unwrapped_model, layernorm_params)\n\n        query = batch[\"input_ids\"].detach().cpu()\n        response = generate_output[:, batch[\"input_ids\"].size(-1) :].detach().cpu()\n        queries, responses = [], []\n        for i in range(len(query)):\n            query_start_index = (query[i] != self.tokenizer.pad_token_id).nonzero()[0].item()\n            response_indexes = (response[i] != self.tokenizer.pad_token_id).nonzero()\n\n            if len(response_indexes) == 0:  \n                response_length = 1\n            elif self.tokenizer.eos_token_id == self.tokenizer.pad_token_id:  \n                response_length = response_indexes[-1].item() + 2\n            else:\n                response_length = response_indexes[-1].item() + 1\n\n            queries.append(query[i, query_start_index:])  \n            responses.append(response[i, :response_length])  \n\n        return queries, responses\n\n    @torch.no_grad()\n    def get_rewards(\n        self,\n        queries: List[\"torch.Tensor\"],\n        responses: List[\"torch.Tensor\"],\n    ) -> List[\"torch.Tensor\"]:\n        r\n        if self.finetuning_args.reward_model_type == \"api\":\n            token_ids = [torch.cat((q, r), dim=-1).tolist() for q, r in zip(queries, responses)]\n            messages = self.tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n            return get_rewards_from_server(self.reward_model, messages)\n\n        batch: Dict[str, \"torch.Tensor\"] = self.prepare_model_inputs(queries, responses)\n        unwrapped_model: \"AutoModelForCausalLMWithValueHead\" = self.accelerator.unwrap_model(self.model)\n\n        if self.finetuning_args.reward_model_type == \"lora\":\n            replace_model(unwrapped_model, target=\"reward\")\n            reward_model = self.model\n        else:\n            reward_model = self.reward_model\n\n        with unwrap_model_for_generation(reward_model, self.accelerator), self.amp_context:  \n            _, _, values = reward_model(**batch, return_dict=True, use_cache=False)\n\n        if self.finetuning_args.reward_model_type == \"lora\":\n            replace_model(unwrapped_model, target=\"default\")\n\n        rewards = values.gather(dim=-1, index=(batch[\"attention_mask\"].sum(dim=-1, keepdim=True) - 1))\n        return rewards.float().detach()  \n\n    @PPODecorators.empty_device_cache()\n    def batched_forward_pass(\n        self,\n        model: \"AutoModelForCausalLMWithValueHead\",\n        queries: \"torch.Tensor\",\n        responses: \"torch.Tensor\",\n        model_inputs: Dict[str, Any],\n        return_logits: bool = False,\n        response_masks: Optional[\"torch.Tensor\"] = None,\n    ) -> Tuple[\"torch.Tensor\", Optional[\"torch.Tensor\"], \"torch.Tensor\", \"torch.Tensor\"]:\n        r\n        bs = len(queries)\n        fbs = self.config.mini_batch_size\n        all_logprobs = []\n        all_logits = []\n        all_masks = []\n        all_values = []\n\n        for i in range(math.ceil(bs / fbs)):\n            input_kwargs = {key: value[i * fbs : (i + 1) * fbs] for key, value in model_inputs.items()}\n            query_batch = queries[i * fbs : (i + 1) * fbs]\n            response_batch = responses[i * fbs : (i + 1) * fbs]\n            if response_masks is not None:\n                response_masks_batch = response_masks[i * fbs : (i + 1) * fbs]\n            input_ids = input_kwargs[\"input_ids\"]\n            attention_mask = input_kwargs[\"attention_mask\"]\n\n            with self.amp_context:  \n                logits, _, values = model(**input_kwargs, return_dict=True, use_cache=False)\n\n            logprobs = logprobs_from_logits(logits[:, :-1, :], input_ids[:, 1:])\n            masks = torch.zeros_like(attention_mask)\n            masks[:, :-1] = attention_mask[:, 1:]\n\n            for j in range(len(query_batch)):\n                start = len(query_batch[j]) - 1\n                if attention_mask[j, 0] == 0:  \n                    start += attention_mask[j, :].nonzero()[0].item()\n                end = start + len(response_batch[j])\n\n                if response_masks is not None:\n                    response_masks_batch = torch.cat((torch.zeros_like(query_batch[j]), response_masks_batch[j]))[1:]\n\n                masks[j, :start] = 0\n                masks[j, end:] = 0\n                if response_masks is not None:\n                    masks[j, start:end] = masks[j, start:end] * response_masks_batch[j][start:end]\n\n            if return_logits:\n                all_logits.append(logits)\n            else:\n                del logits\n\n            all_values.append(values)\n            all_logprobs.append(logprobs)\n            all_masks.append(masks)\n\n        return (\n            torch.cat(all_logprobs),\n            torch.cat(all_logits)[:, :-1] if return_logits else None,\n            torch.cat(all_values)[:, :-1],\n            torch.cat(all_masks)[:, :-1],\n        )\n\n    def save_model(self, output_dir: Optional[str] = None) -> None:\n        r\n        if output_dir is None:\n            output_dir = self.args.output_dir\n\n        if self.is_fsdp_enabled or self.is_deepspeed_enabled:\n            try:\n                state_dict = self.accelerator.get_state_dict(self.model)  \n                if self.args.should_save:\n                    self._save(output_dir, state_dict=state_dict)\n            except ValueError:\n                logger.warning(\n                    \" stage3_gather_16bit_weights_on_model_save=false. Saving the full checkpoint instead,\"\n                    \" use zero_to_fp32.py to recover weights\"\n                )\n                if self.args.should_save:\n                    self._save(output_dir, state_dict={})\n                \n                remove_dummy_checkpoint(self.args.should_save, output_dir, [WEIGHTS_NAME, SAFE_WEIGHTS_NAME])\n                self.model.save_checkpoint(output_dir)\n\n        elif self.args.should_save:\n            unwrapped_model: \"AutoModelForCausalLMWithValueHead\" = self.accelerator.unwrap_model(self.model)\n            self._save(output_dir, state_dict=unwrapped_model.state_dict())\n```"
        }
    ],
    "temperature": 0
}