{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nimport json\nimport logging\nimport os\nimport signal\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\nfrom datetime import timedelta\nfrom typing import TYPE_CHECKING, Any, Dict, Optional\n\nimport torch\nimport transformers\nfrom peft import PeftModel\nfrom transformers import PreTrainedModel, ProcessorMixin, TrainerCallback\nfrom transformers.trainer_utils import PREFIX_CHECKPOINT_DIR, has_length\nfrom transformers.utils import (\n    SAFE_WEIGHTS_NAME,\n    WEIGHTS_NAME,\n    is_safetensors_available,\n)\n\nfrom ..extras.constants import TRAINER_LOG, V_HEAD_SAFE_WEIGHTS_NAME, V_HEAD_WEIGHTS_NAME\nfrom ..extras.logging import LoggerHandler, get_logger\n\n\nif is_safetensors_available():\n    from safetensors import safe_open\n    from safetensors.torch import save_file\n\nif TYPE_CHECKING:\n    from transformers import TrainerControl, TrainerState, TrainingArguments\n    from trl import AutoModelForCausalLMWithValueHead\n\n\nlogger = get_logger(__name__)\n\n\ndef fix_valuehead_checkpoint(\n    model: \"AutoModelForCausalLMWithValueHead\", output_dir: str, safe_serialization: bool\n) -> None:\n    r\n    if not isinstance(model.pretrained_model, (PreTrainedModel, PeftModel)):\n        return\n\n    if safe_serialization:\n        path_to_checkpoint = os.path.join(output_dir, SAFE_WEIGHTS_NAME)\n        with safe_open(path_to_checkpoint, framework=\"pt\", device=\"cpu\") as f:\n            state_dict: Dict[str, torch.Tensor] = {key: f.get_tensor(key) for key in f.keys()}\n    else:\n        path_to_checkpoint = os.path.join(output_dir, WEIGHTS_NAME)\n        state_dict: Dict[str, torch.Tensor] = torch.load(path_to_checkpoint, map_location=\"cpu\")\n\n    decoder_state_dict = {}\n    v_head_state_dict = {}\n    for name, param in state_dict.items():\n        if name.startswith(\"v_head.\"):\n            v_head_state_dict[name] = param\n        else:\n            decoder_state_dict[name.replace(\"pretrained_model.\", \"\", count=1)] = param\n\n    model.pretrained_model.save_pretrained(\n        output_dir, state_dict=decoder_state_dict or None, safe_serialization=safe_serialization\n    )\n\n    if safe_serialization:\n        save_file(v_head_state_dict, os.path.join(output_dir, V_HEAD_SAFE_WEIGHTS_NAME), metadata={\"format\": \"pt\"})\n    else:\n        torch.save(v_head_state_dict, os.path.join(output_dir, V_HEAD_WEIGHTS_NAME))\n\n    os.remove(path_to_checkpoint)\n    logger.info(\"Value head model saved at: {}\".format(output_dir))\n\n\nclass FixValueHeadModelCallback(TrainerCallback):\n    def on_save(self, args: \"TrainingArguments\", state: \"TrainerState\", control: \"TrainerControl\", **kwargs):\n        r\n        if args.should_save:\n            fix_valuehead_checkpoint(\n                model=kwargs.pop(\"model\"),\n                output_dir=os.path.join(args.output_dir, \"{}-{}\".format(PREFIX_CHECKPOINT_DIR, state.global_step)),\n                safe_serialization=args.save_safetensors,\n            )\n\n\nclass SaveProcessorCallback(TrainerCallback):\n    def __init__(self, processor: \"ProcessorMixin\") -> None:\n        r\n        self.processor = processor\n\n    def on_train_end(self, args: \"TrainingArguments\", state: \"TrainerState\", control: \"TrainerControl\", **kwargs):\n        r\n        if args.should_save:\n            getattr(self.processor, \"image_processor\").save_pretrained(args.output_dir)\n\n\nclass PissaConvertCallback(TrainerCallback):\n    r\n\n    def on_train_begin(self, args: \"TrainingArguments\", state: \"TrainerState\", control: \"TrainerControl\", **kwargs):\n        r\n        if args.should_save:\n            model = kwargs.pop(\"model\")\n            pissa_init_dir = os.path.join(args.output_dir, \"pissa_init\")\n            logger.info(\"Initial PiSSA adatper will be saved at: {}.\".format(pissa_init_dir))\n            if isinstance(model, PeftModel):\n                init_lora_weights = getattr(model.peft_config[\"default\"], \"init_lora_weights\")\n                setattr(model.peft_config[\"default\"], \"init_lora_weights\", True)\n                model.save_pretrained(pissa_init_dir, safe_serialization=args.save_safetensors)\n                setattr(model.peft_config[\"default\"], \"init_lora_weights\", init_lora_weights)\n\n    def on_train_end(self, args: \"TrainingArguments\", state: \"TrainerState\", control: \"TrainerControl\", **kwargs):\n        r\n        if args.should_save:\n            model = kwargs.pop(\"model\")\n            pissa_init_dir = os.path.join(args.output_dir, \"pissa_init\")\n            pissa_backup_dir = os.path.join(args.output_dir, \"pissa_backup\")\n            pissa_convert_dir = os.path.join(args.output_dir, \"pissa_converted\")\n            logger.info(\"Converted PiSSA adapter will be saved at: {}.\".format(pissa_convert_dir))\n            \n            \n            \n            \n            if isinstance(model, PeftModel):\n                init_lora_weights = getattr(model.peft_config[\"default\"], \"init_lora_weights\")\n                setattr(model.peft_config[\"default\"], \"init_lora_weights\", True)\n                model.save_pretrained(pissa_backup_dir, safe_serialization=args.save_safetensors)\n                setattr(model.peft_config[\"default\"], \"init_lora_weights\", init_lora_weights)\n                model.save_pretrained(\n                    pissa_convert_dir, safe_serialization=args.save_safetensors, convert_pissa_to_lora=pissa_init_dir\n                )\n                model.load_adapter(pissa_backup_dir, \"default\", is_trainable=True)\n                model.set_adapter(\"default\")\n                model.delete_adapter(\"pissa_init\")\n                setattr(model.peft_config[\"default\"], \"init_lora_weights\", init_lora_weights)\n\n\nclass LogCallback(TrainerCallback):\n    def __init__(self) -> None:\n        r\n        \n        self.start_time = 0\n        self.cur_steps = 0\n        self.max_steps = 0\n        self.elapsed_time = \"\"\n        self.remaining_time = \"\"\n        self.thread_pool: Optional[\"ThreadPoolExecutor\"] = None\n        \n        self.aborted = False\n        self.do_train = False\n        \n        self.webui_mode = os.environ.get(\"LLAMABOARD_ENABLED\", \"0\").lower() in [\"true\", \"1\"]\n        if self.webui_mode:\n            signal.signal(signal.SIGABRT, self._set_abort)\n            self.logger_handler = LoggerHandler(os.environ.get(\"LLAMABOARD_WORKDIR\"))\n            logging.root.addHandler(self.logger_handler)\n            transformers.logging.add_handler(self.logger_handler)\n\n    def _set_abort(self, signum, frame) -> None:\n        self.aborted = True\n\n    def _reset(self, max_steps: int = 0) -> None:\n        self.start_time = time.time()\n        self.cur_steps = 0\n        self.max_steps = max_steps\n        self.elapsed_time = \"\"\n        self.remaining_time = \"\"\n\n    def _timing(self, cur_steps: int) -> None:\n        cur_time = time.time()\n        elapsed_time = cur_time - self.start_time\n        avg_time_per_step = elapsed_time / cur_steps if cur_steps != 0 else 0\n        remaining_time = (self.max_steps - cur_steps) * avg_time_per_step\n        self.cur_steps = cur_steps\n        self.elapsed_time = str(timedelta(seconds=int(elapsed_time)))\n        self.remaining_time = str(timedelta(seconds=int(remaining_time)))\n\n    def _write_log(self, output_dir: str, logs: Dict[str, Any]) -> None:\n        with open(os.path.join(output_dir, TRAINER_LOG), \"a\", encoding=\"utf-8\") as f:\n            f.write(json.dumps(logs) + \"\\n\")\n\n    def _create_thread_pool(self, output_dir: str) -> None:\n        os.makedirs(output_dir, exist_ok=True)\n        self.thread_pool = ThreadPoolExecutor(max_workers=1)\n\n    def _close_thread_pool(self) -> None:\n        if self.thread_pool is not None:\n            self.thread_pool.shutdown(wait=True)\n            self.thread_pool = None\n\n    def on_init_end(self, args: \"TrainingArguments\", state: \"TrainerState\", control: \"TrainerControl\", **kwargs):\n        r\n        if (\n            args.should_save\n            and os.path.exists(os.path.join(args.output_dir, TRAINER_LOG))\n            and args.overwrite_output_dir\n        ):\n            logger.warning(\"Previous trainer log in this folder will be deleted.\")\n            os.remove(os.path.join(args.output_dir, TRAINER_LOG))\n\n    def on_train_begin(self, args: \"TrainingArguments\", state: \"TrainerState\", control: \"TrainerControl\", **kwargs):\n        r\n        if args.should_save:\n            self.do_train = True\n            self._reset(max_steps=state.max_steps)\n            self._create_thread_pool(output_dir=args.output_dir)\n\n    def on_train_end(self, args: \"TrainingArguments\", state: \"TrainerState\", control: \"TrainerControl\", **kwargs):\n        r\n        self._close_thread_pool()\n\n    def on_substep_end(self, args: \"TrainingArguments\", state: \"TrainerState\", control: \"TrainerControl\", **kwargs):\n        r\n        if self.aborted:\n            control.should_epoch_stop = True\n            control.should_training_stop = True\n\n    def on_step_end(self, args: \"TrainingArguments\", state: \"TrainerState\", control: \"TrainerControl\", **kwargs):\n        r\n        if self.aborted:\n            control.should_epoch_stop = True\n            control.should_training_stop = True\n\n    def on_evaluate(self, args: \"TrainingArguments\", state: \"TrainerState\", control: \"TrainerControl\", **kwargs):\n        r\n        if not self.do_train:\n            self._close_thread_pool()\n\n    def on_predict(self, args: \"TrainingArguments\", state: \"TrainerState\", control: \"TrainerControl\", **kwargs):\n        r\n        if not self.do_train:\n            self._close_thread_pool()\n\n    def on_log(self, args: \"TrainingArguments\", state: \"TrainerState\", control: \"TrainerControl\", **kwargs):\n        r\n        if not args.should_save:\n            return\n\n        self._timing(cur_steps=state.global_step)\n        logs = dict(\n            current_steps=self.cur_steps,\n            total_steps=self.max_steps,\n            loss=state.log_history[-1].get(\"loss\", None),\n            eval_loss=state.log_history[-1].get(\"eval_loss\", None),\n            predict_loss=state.log_history[-1].get(\"predict_loss\", None),\n            reward=state.log_history[-1].get(\"reward\", None),\n            accuracy=state.log_history[-1].get(\"rewards/accuracies\", None),\n            learning_rate=state.log_history[-1].get(\"learning_rate\", None),\n            epoch=state.log_history[-1].get(\"epoch\", None),\n            percentage=round(self.cur_steps / self.max_steps * 100, 2) if self.max_steps != 0 else 100,\n            elapsed_time=self.elapsed_time,\n            remaining_time=self.remaining_time,\n            throughput=\"{:.2f}\".format(state.num_input_tokens_seen / (time.time() - self.start_time)),\n            total_tokens=state.num_input_tokens_seen,\n        )\n        logs = {k: v for k, v in logs.items() if v is not None}\n        if self.webui_mode and all(key in logs for key in [\"loss\", \"learning_rate\", \"epoch\"]):\n            logger.info(\n                \"{{'loss': {:.4f}, 'learning_rate': {:2.4e}, 'epoch': {:.2f}, 'throughput': {}}}\".format(\n                    logs[\"loss\"], logs[\"learning_rate\"], logs[\"epoch\"], logs[\"throughput\"]\n                )\n            )\n\n        if self.thread_pool is not None:\n            self.thread_pool.submit(self._write_log, args.output_dir, logs)\n\n    def on_prediction_step(\n        self, args: \"TrainingArguments\", state: \"TrainerState\", control: \"TrainerControl\", **kwargs\n    ):\n        r\n        if self.do_train:\n            return\n\n        if self.aborted:\n            sys.exit(0)\n\n        if not args.should_save:\n            return\n\n        eval_dataloader = kwargs.pop(\"eval_dataloader\", None)\n        if has_length(eval_dataloader):\n            if self.max_steps == 0:\n                self._reset(max_steps=len(eval_dataloader))\n                self._create_thread_pool(output_dir=args.output_dir)\n\n            self._timing(cur_steps=self.cur_steps + 1)\n            if self.cur_steps % 5 == 0 and self.thread_pool is not None:\n                logs = dict(\n                    current_steps=self.cur_steps,\n                    total_steps=self.max_steps,\n                    percentage=round(self.cur_steps / self.max_steps * 100, 2) if self.max_steps != 0 else 100,\n                    elapsed_time=self.elapsed_time,\n                    remaining_time=self.remaining_time,\n                )\n                self.thread_pool.submit(self._write_log, args.output_dir, logs)\n```"
        }
    ],
    "temperature": 0
}