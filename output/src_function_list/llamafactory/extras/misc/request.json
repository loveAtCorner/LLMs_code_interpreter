{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nimport gc\nimport os\nfrom typing import TYPE_CHECKING, Tuple\n\nimport torch\nimport transformers.dynamic_module_utils\nfrom transformers import InfNanRemoveLogitsProcessor, LogitsProcessorList\nfrom transformers.dynamic_module_utils import get_relative_imports\nfrom transformers.utils import (\n    is_torch_bf16_gpu_available,\n    is_torch_cuda_available,\n    is_torch_mps_available,\n    is_torch_npu_available,\n    is_torch_xpu_available,\n)\nfrom transformers.utils.versions import require_version\n\nfrom .logging import get_logger\n\n\n_is_fp16_available = is_torch_npu_available() or is_torch_cuda_available()\ntry:\n    _is_bf16_available = is_torch_bf16_gpu_available()\nexcept Exception:\n    _is_bf16_available = False\n\n\nif TYPE_CHECKING:\n    from ..hparams import ModelArguments\n\n\nlogger = get_logger(__name__)\n\n\nclass AverageMeter:\n    r\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef check_dependencies() -> None:\n    r\n    if os.environ.get(\"DISABLE_VERSION_CHECK\", \"0\").lower() in [\"true\", \"1\"]:\n        logger.warning(\"Version checking has been disabled, may lead to unexpected behaviors.\")\n    else:\n        require_version(\"transformers>=4.41.2\", \"To fix: pip install transformers>=4.41.2\")\n        require_version(\"datasets>=2.16.0\", \"To fix: pip install datasets>=2.16.0\")\n        require_version(\"accelerate>=0.30.1\", \"To fix: pip install accelerate>=0.30.1\")\n        require_version(\"peft>=0.11.1\", \"To fix: pip install peft>=0.11.1\")\n        require_version(\"trl>=0.8.6\", \"To fix: pip install trl>=0.8.6\")\n\n\ndef count_parameters(model: \"torch.nn.Module\") -> Tuple[int, int]:\n    r\n    trainable_params, all_param = 0, 0\n    for param in model.parameters():\n        num_params = param.numel()\n        \n        if num_params == 0 and hasattr(param, \"ds_numel\"):\n            num_params = param.ds_numel\n\n        \n        if param.__class__.__name__ == \"Params4bit\":\n            if hasattr(param, \"quant_storage\") and hasattr(param.quant_storage, \"itemsize\"):\n                num_bytes = param.quant_storage.itemsize\n            elif hasattr(param, \"element_size\"):  \n                num_bytes = param.element_size()\n            else:\n                num_bytes = 1\n\n            num_params = num_params * 2 * num_bytes\n\n        all_param += num_params\n        if param.requires_grad:\n            trainable_params += num_params\n\n    return trainable_params, all_param\n\n\ndef get_current_device() -> \"torch.device\":\n    r\n    if is_torch_xpu_available():\n        device = \"xpu:{}\".format(os.environ.get(\"LOCAL_RANK\", \"0\"))\n    elif is_torch_npu_available():\n        device = \"npu:{}\".format(os.environ.get(\"LOCAL_RANK\", \"0\"))\n    elif is_torch_mps_available():\n        device = \"mps:{}\".format(os.environ.get(\"LOCAL_RANK\", \"0\"))\n    elif is_torch_cuda_available():\n        device = \"cuda:{}\".format(os.environ.get(\"LOCAL_RANK\", \"0\"))\n    else:\n        device = \"cpu\"\n\n    return torch.device(device)\n\n\ndef get_device_count() -> int:\n    r\n    if is_torch_npu_available():\n        return torch.npu.device_count()\n    elif is_torch_cuda_available():\n        return torch.cuda.device_count()\n    else:\n        return 0\n\n\ndef get_logits_processor() -> \"LogitsProcessorList\":\n    r\n    logits_processor = LogitsProcessorList()\n    logits_processor.append(InfNanRemoveLogitsProcessor())\n    return logits_processor\n\n\ndef has_tokenized_data(path: \"os.PathLike\") -> bool:\n    r\n    return os.path.isdir(path) and len(os.listdir(path)) > 0\n\n\ndef infer_optim_dtype(model_dtype: \"torch.dtype\") -> \"torch.dtype\":\n    r\n    if _is_bf16_available and model_dtype == torch.bfloat16:\n        return torch.bfloat16\n    elif _is_fp16_available:\n        return torch.float16\n    else:\n        return torch.float32\n\n\ndef is_gpu_or_npu_available() -> bool:\n    r\n    return is_torch_npu_available() or is_torch_cuda_available()\n\n\ndef skip_check_imports() -> None:\n    if os.environ.get(\"FORCE_CHECK_IMPORTS\", \"0\").lower() not in [\"true\", \"1\"]:\n        transformers.dynamic_module_utils.check_imports = get_relative_imports\n\n\ndef torch_gc() -> None:\n    r\n    gc.collect()\n    if is_torch_xpu_available():\n        torch.xpu.empty_cache()\n    elif is_torch_npu_available():\n        torch.npu.empty_cache()\n    elif is_torch_mps_available():\n        torch.mps.empty_cache()\n    elif is_torch_cuda_available():\n        torch.cuda.empty_cache()\n\n\ndef try_download_model_from_ms(model_args: \"ModelArguments\") -> str:\n    if not use_modelscope() or os.path.exists(model_args.model_name_or_path):\n        return model_args.model_name_or_path\n\n    try:\n        from modelscope import snapshot_download\n\n        revision = \"master\" if model_args.model_revision == \"main\" else model_args.model_revision\n        return snapshot_download(model_args.model_name_or_path, revision=revision, cache_dir=model_args.cache_dir)\n    except ImportError:\n        raise ImportError(\"Please install modelscope via `pip install modelscope -U`\")\n\n\ndef use_modelscope() -> bool:\n    return os.environ.get(\"USE_MODELSCOPE_HUB\", \"0\").lower() in [\"true\", \"1\"]\n```"
        }
    ],
    "temperature": 0
}