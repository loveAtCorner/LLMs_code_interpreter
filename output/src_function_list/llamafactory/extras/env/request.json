{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nimport platform\n\nimport accelerate\nimport datasets\nimport peft\nimport torch\nimport transformers\nimport trl\nfrom transformers.utils import is_torch_cuda_available, is_torch_npu_available\n\n\nVERSION = \"0.8.3.dev0\"\n\n\ndef print_env() -> None:\n    info = {\n        \"`llamafactory` version\": VERSION,\n        \"Platform\": platform.platform(),\n        \"Python version\": platform.python_version(),\n        \"PyTorch version\": torch.__version__,\n        \"Transformers version\": transformers.__version__,\n        \"Datasets version\": datasets.__version__,\n        \"Accelerate version\": accelerate.__version__,\n        \"PEFT version\": peft.__version__,\n        \"TRL version\": trl.__version__,\n    }\n\n    if is_torch_cuda_available():\n        info[\"PyTorch version\"] += \" (GPU)\"\n        info[\"GPU type\"] = torch.cuda.get_device_name()\n\n    if is_torch_npu_available():\n        info[\"PyTorch version\"] += \" (NPU)\"\n        info[\"NPU type\"] = torch.npu.get_device_name()\n        info[\"CANN version\"] = torch.version.cann\n\n    try:\n        import deepspeed  \n\n        info[\"DeepSpeed version\"] = deepspeed.__version__\n    except Exception:\n        pass\n\n    try:\n        import bitsandbytes\n\n        info[\"Bitsandbytes version\"] = bitsandbytes.__version__\n    except Exception:\n        pass\n\n    try:\n        import vllm\n\n        info[\"vLLM version\"] = vllm.__version__\n    except Exception:\n        pass\n\n    print(\"\\n\" + \"\\n\".join([\"- {}: {}\".format(key, value) for key, value in info.items()]) + \"\\n\")\n```"
        }
    ],
    "temperature": 0
}