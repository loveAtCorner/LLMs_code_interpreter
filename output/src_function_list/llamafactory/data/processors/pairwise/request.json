{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, Tuple\n\nfrom ...extras.constants import IGNORE_INDEX\nfrom ...extras.logging import get_logger\nfrom .processor_utils import get_paligemma_token_type_ids, get_pixel_values, infer_seqlen\n\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedTokenizer, ProcessorMixin\n\n    from ...hparams import DataArguments\n    from ..template import Template\n\n\nlogger = get_logger(__name__)\n\n\ndef _encode_pairwise_example(\n    prompt: Sequence[Dict[str, str]],\n    response: Sequence[Dict[str, str]],\n    system: Optional[str],\n    tools: Optional[str],\n    template: \"Template\",\n    tokenizer: \"PreTrainedTokenizer\",\n    processor: Optional[\"ProcessorMixin\"],\n    data_args: \"DataArguments\",\n) -> Tuple[List[int], List[int], List[int], List[int]]:\n    if processor is not None and not hasattr(processor, \"image_seq_length\"):  \n        prompt[0][\"content\"] = template.image_token + prompt[0][\"content\"]\n\n    chosen_messages = prompt + [response[0]]\n    rejected_messages = prompt + [response[1]]\n    prompt_ids, chosen_ids = template.encode_oneturn(tokenizer, chosen_messages, system, tools)\n    _, rejected_ids = template.encode_oneturn(tokenizer, rejected_messages, system, tools)\n\n    if template.efficient_eos:\n        chosen_ids += [tokenizer.eos_token_id]\n        rejected_ids += [tokenizer.eos_token_id]\n\n    if processor is not None and hasattr(processor, \"image_seq_length\"):  \n        image_token_id = tokenizer.convert_tokens_to_ids(template.image_token)\n        prompt_ids = [image_token_id] * getattr(processor, \"image_seq_length\") + prompt_ids\n\n    source_len, target_len = infer_seqlen(\n        len(prompt_ids), max(len(chosen_ids), len(rejected_ids)), data_args.cutoff_len\n    )  \n    prompt_ids = prompt_ids[:source_len]\n    chosen_ids = chosen_ids[:target_len]\n    rejected_ids = rejected_ids[:target_len]\n\n    chosen_input_ids = prompt_ids + chosen_ids\n    chosen_labels = [IGNORE_INDEX] * source_len + chosen_ids\n    rejected_input_ids = prompt_ids + rejected_ids\n    rejected_labels = [IGNORE_INDEX] * source_len + rejected_ids\n\n    return chosen_input_ids, chosen_labels, rejected_input_ids, rejected_labels\n\n\ndef preprocess_pairwise_dataset(\n    examples: Dict[str, List[Any]],\n    template: \"Template\",\n    tokenizer: \"PreTrainedTokenizer\",\n    processor: Optional[\"ProcessorMixin\"],\n    data_args: \"DataArguments\",\n) -> Dict[str, List[List[int]]]:\n    \n    model_inputs = {\n        \"chosen_input_ids\": [],\n        \"chosen_attention_mask\": [],\n        \"chosen_labels\": [],\n        \"rejected_input_ids\": [],\n        \"rejected_attention_mask\": [],\n        \"rejected_labels\": [],\n    }\n    if processor is not None:\n        model_inputs[\"pixel_values\"] = []\n        if hasattr(processor, \"image_seq_length\"):  \n            model_inputs[\"chosen_token_type_ids\"] = []\n            model_inputs[\"rejected_token_type_ids\"] = []\n\n    for i in range(len(examples[\"prompt\"])):\n        if len(examples[\"prompt\"][i]) % 2 != 1 or len(examples[\"response\"][i]) < 2:\n            logger.warning(\"Dropped invalid example: {}\".format(examples[\"prompt\"][i] + examples[\"response\"][i]))\n            continue\n\n        chosen_input_ids, chosen_labels, rejected_input_ids, rejected_labels = _encode_pairwise_example(\n            prompt=examples[\"prompt\"][i],\n            response=examples[\"response\"][i],\n            system=examples[\"system\"][i],\n            tools=examples[\"tools\"][i],\n            template=template,\n            tokenizer=tokenizer,\n            processor=processor,\n            data_args=data_args,\n        )\n        model_inputs[\"chosen_input_ids\"].append(chosen_input_ids)\n        model_inputs[\"chosen_attention_mask\"].append([1] * len(chosen_input_ids))\n        model_inputs[\"chosen_labels\"].append(chosen_labels)\n        model_inputs[\"rejected_input_ids\"].append(rejected_input_ids)\n        model_inputs[\"rejected_attention_mask\"].append([1] * len(rejected_input_ids))\n        model_inputs[\"rejected_labels\"].append(rejected_labels)\n        if processor is not None:\n            model_inputs[\"pixel_values\"].append(get_pixel_values(examples[\"images\"][i], processor))\n            if hasattr(processor, \"image_seq_length\"):  \n                model_inputs[\"chosen_token_type_ids\"].append(\n                    get_paligemma_token_type_ids(len(chosen_input_ids), processor)\n                )\n                model_inputs[\"rejected_token_type_ids\"].append(\n                    get_paligemma_token_type_ids(len(rejected_input_ids), processor)\n                )\n\n    return model_inputs\n\n\ndef print_pairwise_dataset_example(example: Dict[str, List[int]], tokenizer: \"PreTrainedTokenizer\") -> None:\n    valid_chosen_labels = list(filter(lambda x: x != IGNORE_INDEX, example[\"chosen_labels\"]))\n    valid_rejected_labels = list(filter(lambda x: x != IGNORE_INDEX, example[\"rejected_labels\"]))\n    print(\"chosen_input_ids:\\n{}\".format(example[\"chosen_input_ids\"]))\n    print(\"chosen_inputs:\\n{}\".format(tokenizer.decode(example[\"chosen_input_ids\"], skip_special_tokens=False)))\n    print(\"chosen_label_ids:\\n{}\".format(example[\"chosen_labels\"]))\n    print(\"chosen_labels:\\n{}\".format(tokenizer.decode(valid_chosen_labels, skip_special_tokens=False)))\n    print(\"rejected_input_ids:\\n{}\".format(example[\"rejected_input_ids\"]))\n    print(\"rejected_inputs:\\n{}\".format(tokenizer.decode(example[\"rejected_input_ids\"], skip_special_tokens=False)))\n    print(\"rejected_label_ids:\\n{}\".format(example[\"rejected_labels\"]))\n    print(\"rejected_labels:\\n{}\".format(tokenizer.decode(valid_rejected_labels, skip_special_tokens=False)))\n```"
        }
    ],
    "temperature": 0
}