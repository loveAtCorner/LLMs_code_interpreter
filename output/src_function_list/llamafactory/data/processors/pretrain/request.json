{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "返回下面这个脚本当中所有函数或者类的名称，及其对应的功能\n返回内容的格式需要遵循markdown表格的形式\n\n```python\nfrom itertools import chain\nfrom typing import TYPE_CHECKING, Any, Dict, List\n\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedTokenizer\n\n    from ...hparams import DataArguments\n\n\ndef preprocess_pretrain_dataset(\n    examples: Dict[str, List[Any]], tokenizer: \"PreTrainedTokenizer\", data_args: \"DataArguments\"\n) -> Dict[str, List[List[int]]]:\n    \n    eos_token = \"<|end_of_text|>\" if data_args.template == \"llama3\" else tokenizer.eos_token\n    text_examples = [messages[0][\"content\"] + eos_token for messages in examples[\"prompt\"]]\n\n    if not data_args.packing:\n        if data_args.template == \"gemma\":\n            text_examples = [tokenizer.bos_token + example for example in text_examples]\n\n        result = tokenizer(text_examples, add_special_tokens=False, max_length=data_args.cutoff_len, truncation=True)\n    else:\n        tokenized_examples = tokenizer(text_examples, add_special_tokens=False)\n        concatenated_examples = {k: list(chain(*tokenized_examples[k])) for k in tokenized_examples.keys()}\n        total_length = len(concatenated_examples[list(concatenated_examples.keys())[0]])\n        block_size = data_args.cutoff_len\n        total_length = (total_length // block_size) * block_size\n        result = {\n            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n            for k, t in concatenated_examples.items()\n        }\n        if data_args.template == \"gemma\":\n            for i in range(len(result[\"input_ids\"])):\n                result[\"input_ids\"][i][0] = tokenizer.bos_token_id\n\n    return result\n```"
        }
    ],
    "temperature": 0
}