{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nimport json\nimport os\nimport signal\nfrom datetime import datetime\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport psutil\nfrom transformers.trainer_utils import get_last_checkpoint\nfrom yaml import safe_dump, safe_load\n\nfrom ..extras.constants import PEFT_METHODS, RUNNING_LOG, TRAINER_LOG, TRAINING_ARGS, TRAINING_STAGES\nfrom ..extras.packages import is_gradio_available, is_matplotlib_available\nfrom ..extras.ploting import gen_loss_plot\nfrom ..model import QuantizationMethod\nfrom .common import DEFAULT_CACHE_DIR, DEFAULT_CONFIG_DIR, get_save_dir\nfrom .locales import ALERTS\n\n\nif is_gradio_available():\n    import gradio as gr\n\n\ndef abort_process(pid: int) -> None:\n    r\n    try:\n        children = psutil.Process(pid).children()\n        if children:\n            for child in children:\n                abort_process(child.pid)\n\n        os.kill(pid, signal.SIGABRT)\n    except Exception:\n        pass\n\n\ndef can_quantize(finetuning_type: str) -> \"gr.Dropdown\":\n    r\n    if finetuning_type not in PEFT_METHODS:\n        return gr.Dropdown(value=\"none\", interactive=False)\n    else:\n        return gr.Dropdown(interactive=True)\n\n\ndef can_quantize_to(quantization_method: str) -> \"gr.Dropdown\":\n    r\n    if quantization_method == QuantizationMethod.BITS_AND_BYTES.value:\n        available_bits = [\"none\", \"8\", \"4\"]\n    elif quantization_method == QuantizationMethod.HQQ.value:\n        available_bits = [\"none\", \"8\", \"6\", \"5\", \"4\", \"3\", \"2\", \"1\"]\n    elif quantization_method == QuantizationMethod.EETQ.value:\n        available_bits = [\"none\", \"8\"]\n\n    return gr.Dropdown(choices=available_bits)\n\n\ndef change_stage(training_stage: str = list(TRAINING_STAGES.keys())[0]) -> Tuple[List[str], bool]:\n    r\n    return [], TRAINING_STAGES[training_stage] == \"pt\"\n\n\ndef check_json_schema(text: str, lang: str) -> None:\n    r\n    try:\n        tools = json.loads(text)\n        if tools:\n            assert isinstance(tools, list)\n            for tool in tools:\n                if \"name\" not in tool:\n                    raise NotImplementedError(\"Name not found.\")\n    except NotImplementedError:\n        gr.Warning(ALERTS[\"err_tool_name\"][lang])\n    except Exception:\n        gr.Warning(ALERTS[\"err_json_schema\"][lang])\n\n\ndef clean_cmd(args: Dict[str, Any]) -> Dict[str, Any]:\n    r\n    no_skip_keys = [\"packing\"]\n    return {k: v for k, v in args.items() if (k in no_skip_keys) or (v is not None and v is not False and v != \"\")}\n\n\ndef gen_cmd(args: Dict[str, Any]) -> str:\n    r\n    cmd_lines = [\"llamafactory-cli train \"]\n    for k, v in clean_cmd(args).items():\n        cmd_lines.append(\"    --{} {} \".format(k, str(v)))\n\n    cmd_text = \"\\\\\\n\".join(cmd_lines)\n    cmd_text = \"```bash\\n{}\\n```\".format(cmd_text)\n    return cmd_text\n\n\ndef save_cmd(args: Dict[str, Any]) -> str:\n    r\n    output_dir = args[\"output_dir\"]\n    os.makedirs(output_dir, exist_ok=True)\n\n    with open(os.path.join(output_dir, TRAINING_ARGS), \"w\", encoding=\"utf-8\") as f:\n        safe_dump(clean_cmd(args), f)\n\n    return os.path.join(output_dir, TRAINING_ARGS)\n\n\ndef get_eval_results(path: os.PathLike) -> str:\n    r\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        result = json.dumps(json.load(f), indent=4)\n    return \"```json\\n{}\\n```\\n\".format(result)\n\n\ndef get_time() -> str:\n    r\n    return datetime.now().strftime(r\"%Y-%m-%d-%H-%M-%S\")\n\n\ndef get_trainer_info(output_path: os.PathLike, do_train: bool) -> Tuple[str, \"gr.Slider\", Optional[\"gr.Plot\"]]:\n    r\n    running_log = \"\"\n    running_progress = gr.Slider(visible=False)\n    running_loss = None\n\n    running_log_path = os.path.join(output_path, RUNNING_LOG)\n    if os.path.isfile(running_log_path):\n        with open(running_log_path, \"r\", encoding=\"utf-8\") as f:\n            running_log = f.read()\n\n    trainer_log_path = os.path.join(output_path, TRAINER_LOG)\n    if os.path.isfile(trainer_log_path):\n        trainer_log: List[Dict[str, Any]] = []\n        with open(trainer_log_path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                trainer_log.append(json.loads(line))\n\n        if len(trainer_log) != 0:\n            latest_log = trainer_log[-1]\n            percentage = latest_log[\"percentage\"]\n            label = \"Running {:d}/{:d}: {} < {}\".format(\n                latest_log[\"current_steps\"],\n                latest_log[\"total_steps\"],\n                latest_log[\"elapsed_time\"],\n                latest_log[\"remaining_time\"],\n            )\n            running_progress = gr.Slider(label=label, value=percentage, visible=True)\n\n            if do_train and is_matplotlib_available():\n                running_loss = gr.Plot(gen_loss_plot(trainer_log))\n\n    return running_log, running_progress, running_loss\n\n\ndef load_args(config_path: str) -> Optional[Dict[str, Any]]:\n    r\n    try:\n        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n            return safe_load(f)\n    except Exception:\n        return None\n\n\ndef save_args(config_path: str, config_dict: Dict[str, Any]):\n    r\n    with open(config_path, \"w\", encoding=\"utf-8\") as f:\n        safe_dump(config_dict, f)\n\n\ndef list_config_paths(current_time: str) -> \"gr.Dropdown\":\n    r\n    config_files = [\"{}.yaml\".format(current_time)]\n    if os.path.isdir(DEFAULT_CONFIG_DIR):\n        for file_name in os.listdir(DEFAULT_CONFIG_DIR):\n            if file_name.endswith(\".yaml\") and file_name not in config_files:\n                config_files.append(file_name)\n\n    return gr.Dropdown(choices=config_files)\n\n\ndef list_output_dirs(model_name: Optional[str], finetuning_type: str, current_time: str) -> \"gr.Dropdown\":\n    r\n    output_dirs = [\"train_{}\".format(current_time)]\n    if model_name:\n        save_dir = get_save_dir(model_name, finetuning_type)\n        if save_dir and os.path.isdir(save_dir):\n            for folder in os.listdir(save_dir):\n                output_dir = os.path.join(save_dir, folder)\n                if os.path.isdir(output_dir) and get_last_checkpoint(output_dir) is not None:\n                    output_dirs.append(folder)\n\n    return gr.Dropdown(choices=output_dirs)\n\n\ndef create_ds_config() -> None:\n    r\n    os.makedirs(DEFAULT_CACHE_DIR, exist_ok=True)\n    ds_config = {\n        \"train_batch_size\": \"auto\",\n        \"train_micro_batch_size_per_gpu\": \"auto\",\n        \"gradient_accumulation_steps\": \"auto\",\n        \"gradient_clipping\": \"auto\",\n        \"zero_allow_untested_optimizer\": True,\n        \"fp16\": {\n            \"enabled\": \"auto\",\n            \"loss_scale\": 0,\n            \"loss_scale_window\": 1000,\n            \"initial_scale_power\": 16,\n            \"hysteresis\": 2,\n            \"min_loss_scale\": 1,\n        },\n        \"bf16\": {\"enabled\": \"auto\"},\n    }\n    offload_config = {\n        \"device\": \"cpu\",\n        \"pin_memory\": True,\n    }\n    ds_config[\"zero_optimization\"] = {\n        \"stage\": 2,\n        \"allgather_partitions\": True,\n        \"allgather_bucket_size\": 5e8,\n        \"overlap_comm\": True,\n        \"reduce_scatter\": True,\n        \"reduce_bucket_size\": 5e8,\n        \"contiguous_gradients\": True,\n        \"round_robin_gradients\": True,\n    }\n    with open(os.path.join(DEFAULT_CACHE_DIR, \"ds_z2_config.json\"), \"w\", encoding=\"utf-8\") as f:\n        json.dump(ds_config, f, indent=2)\n\n    ds_config[\"zero_optimization\"][\"offload_optimizer\"] = offload_config\n    with open(os.path.join(DEFAULT_CACHE_DIR, \"ds_z2_offload_config.json\"), \"w\", encoding=\"utf-8\") as f:\n        json.dump(ds_config, f, indent=2)\n\n    ds_config[\"zero_optimization\"] = {\n        \"stage\": 3,\n        \"overlap_comm\": True,\n        \"contiguous_gradients\": True,\n        \"sub_group_size\": 1e9,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_16bit_weights_on_model_save\": True,\n    }\n    with open(os.path.join(DEFAULT_CACHE_DIR, \"ds_z3_config.json\"), \"w\", encoding=\"utf-8\") as f:\n        json.dump(ds_config, f, indent=2)\n\n    ds_config[\"zero_optimization\"][\"offload_optimizer\"] = offload_config\n    ds_config[\"zero_optimization\"][\"offload_param\"] = offload_config\n    with open(os.path.join(DEFAULT_CACHE_DIR, \"ds_z3_offload_config.json\"), \"w\", encoding=\"utf-8\") as f:\n        json.dump(ds_config, f, indent=2)\n```"
        }
    ],
    "temperature": 0
}