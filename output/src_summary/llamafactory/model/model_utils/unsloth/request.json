{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nfrom typing import TYPE_CHECKING, Any, Dict, Optional\n\nfrom ...extras.logging import get_logger\nfrom ...extras.misc import get_current_device\n\n\nif TYPE_CHECKING:\n    from transformers import PretrainedConfig, PreTrainedModel\n\n    from ...hparams import ModelArguments\n\n\nlogger = get_logger(__name__)\n\n\ndef _get_unsloth_kwargs(\n    config: \"PretrainedConfig\", model_name_or_path: str, model_args: \"ModelArguments\"\n) -> Dict[str, Any]:\n    return {\n        \"model_name\": model_name_or_path,\n        \"max_seq_length\": model_args.model_max_length or 4096,\n        \"dtype\": model_args.compute_dtype,\n        \"load_in_4bit\": model_args.quantization_bit == 4,\n        \"token\": model_args.hf_hub_token,\n        \"device_map\": {\"\": get_current_device()},\n        \"rope_scaling\": getattr(config, \"rope_scaling\", None),\n        \"fix_tokenizer\": False,\n        \"trust_remote_code\": True,\n        \"use_gradient_checkpointing\": \"unsloth\",\n    }\n\n\ndef load_unsloth_pretrained_model(\n    config: \"PretrainedConfig\", model_args: \"ModelArguments\"\n) -> Optional[\"PreTrainedModel\"]:\n    r\n    from unsloth import FastLanguageModel\n\n    unsloth_kwargs = _get_unsloth_kwargs(config, model_args.model_name_or_path, model_args)\n    try:\n        model, _ = FastLanguageModel.from_pretrained(**unsloth_kwargs)\n    except NotImplementedError:\n        logger.warning(\"Unsloth does not support model type {}.\".format(getattr(config, \"model_type\", None)))\n        model = None\n        model_args.use_unsloth = False\n\n    return model\n\n\ndef get_unsloth_peft_model(\n    model: \"PreTrainedModel\", model_args: \"ModelArguments\", peft_kwargs: Dict[str, Any]\n) -> \"PreTrainedModel\":\n    r\n    from unsloth import FastLanguageModel\n\n    unsloth_peft_kwargs = {\n        \"model\": model,\n        \"max_seq_length\": model_args.model_max_length,\n        \"use_gradient_checkpointing\": \"unsloth\",\n    }\n    return FastLanguageModel.get_peft_model(**peft_kwargs, **unsloth_peft_kwargs)\n\n\ndef load_unsloth_peft_model(\n    config: \"PretrainedConfig\", model_args: \"ModelArguments\", is_trainable: bool\n) -> \"PreTrainedModel\":\n    r\n    from unsloth import FastLanguageModel\n\n    unsloth_kwargs = _get_unsloth_kwargs(config, model_args.adapter_name_or_path[0], model_args)\n    try:\n        if not is_trainable:\n            unsloth_kwargs[\"use_gradient_checkpointing\"] = False\n\n        model, _ = FastLanguageModel.from_pretrained(**unsloth_kwargs)\n    except NotImplementedError:\n        raise ValueError(\"Unsloth does not support model type {}.\".format(getattr(config, \"model_type\", None)))\n\n    if not is_trainable:\n        FastLanguageModel.for_inference(model)\n\n    return model\n```"
        }
    ],
    "temperature": 0
}