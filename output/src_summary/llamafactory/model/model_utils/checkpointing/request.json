{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nimport inspect\nfrom functools import partial\nfrom types import MethodType\nfrom typing import TYPE_CHECKING, Any, Dict, Optional, Tuple\n\nimport torch\n\nfrom ...extras.constants import LAYERNORM_NAMES\nfrom ...extras.logging import get_logger\n\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedModel\n\n    from ...hparams import ModelArguments\n\n\nlogger = get_logger(__name__)\n\n\ndef _gradient_checkpointing_enable(\n    self: \"PreTrainedModel\", gradient_checkpointing_kwargs: Optional[Dict[str, Any]] = None\n) -> None:\n    r\n    from torch.utils.checkpoint import checkpoint\n\n    if not self.supports_gradient_checkpointing:\n        raise ValueError(\"{} does not support gradient checkpointing.\".format(self.__class__.__name__))\n\n    if gradient_checkpointing_kwargs is None:\n        gradient_checkpointing_kwargs = {\"use_reentrant\": True}\n\n    gradient_checkpointing_func = partial(checkpoint, **gradient_checkpointing_kwargs)\n\n    def custom_gradient_checkpointing_func(func, *args, **kwargs):\n        module: \"torch.nn.Module\" = func.__self__\n\n        if any(param.requires_grad for param in module.parameters()):\n            for arg in args:\n                if torch.is_tensor(arg) and torch.is_floating_point(arg):\n                    arg.requires_grad_(True)\n\n        return gradient_checkpointing_func(func, *args, **kwargs)\n\n    if \"value\" in inspect.signature(self._set_gradient_checkpointing).parameters:  \n        self.apply(partial(self._set_gradient_checkpointing, value=True))\n        self.enable_input_require_grads()\n        logger.warning(\"You are using the old GC format, some features (e.g. BAdam) will be invalid.\")\n    else:  \n        self._set_gradient_checkpointing(enable=True, gradient_checkpointing_func=custom_gradient_checkpointing_func)\n\n\ndef _fp32_forward_post_hook(\n    module: \"torch.nn.Module\", args: Tuple[\"torch.Tensor\"], output: \"torch.Tensor\"\n) -> \"torch.Tensor\":\n    return output.to(torch.float32)\n\n\ndef prepare_model_for_training(model: \"PreTrainedModel\", model_args: \"ModelArguments\") -> None:\n    r\n    if model_args.upcast_layernorm:\n        logger.info(\"Upcasting layernorm weights in float32.\")\n        for name, param in model.named_parameters():\n            if param.ndim == 1 and any(ln_name in name for ln_name in LAYERNORM_NAMES):\n                param.data = param.data.to(torch.float32)\n\n    if not model_args.disable_gradient_checkpointing:\n        if not getattr(model, \"supports_gradient_checkpointing\", False):\n            logger.warning(\"Current model does not support gradient checkpointing.\")\n        else:\n            \n            \n            model.gradient_checkpointing_enable = MethodType(_gradient_checkpointing_enable, model)\n            model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": True})\n            setattr(model.config, \"use_cache\", False)  \n            logger.info(\"Gradient checkpointing enabled.\")\n\n    if model_args.upcast_lmhead_output:\n        output_layer = model.get_output_embeddings()\n        if isinstance(output_layer, torch.nn.Linear) and output_layer.weight.dtype != torch.float32:\n            logger.info(\"Upcasting lm_head outputs in float32.\")\n            output_layer.register_forward_hook(_fp32_forward_post_hook)\n```"
        }
    ],
    "temperature": 0
}