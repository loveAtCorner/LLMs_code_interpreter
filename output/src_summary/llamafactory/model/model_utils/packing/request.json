{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nfrom typing import TYPE_CHECKING, Tuple\n\nimport torch\nimport torch.nn.functional as F\nimport transformers.models\n\nfrom ...extras.constants import SUPPORTED_CLASS_FOR_BLOCK_DIAG_ATTN\nfrom ...extras.logging import get_logger\n\n\nif TYPE_CHECKING:\n    from transformers import PretrainedConfig\n\n    from ...hparams import ModelArguments\n\n\nlogger = get_logger(__name__)\n\n\ndef get_seqlens_in_batch(attention_mask: \"torch.Tensor\") -> \"torch.Tensor\":\n    r\n    bsz = attention_mask.size(0)\n    dtype, device = attention_mask.dtype, attention_mask.device\n    max_num = torch.max(attention_mask).item()\n    counts: \"torch.Tensor\" = torch.zeros((bsz, max_num), dtype=dtype, device=device)\n    for i in range(max_num):\n        counts[:, i] = torch.sum(attention_mask == (i + 1), dim=-1)\n\n    counts = counts.flatten()\n    seqlens = counts[counts.nonzero().squeeze(dim=-1)]\n    return seqlens\n\n\ndef get_unpad_data(attention_mask: \"torch.Tensor\") -> Tuple[\"torch.Tensor\", \"torch.Tensor\", int]:\n    r\n    seqlens_in_batch = get_seqlens_in_batch(attention_mask)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n    return indices, cu_seqlens, max_seqlen_in_batch\n\n\ndef patch_for_block_diag_attn(model_type: str) -> None:\n    if model_type == \"cohere\":\n        transformers.models.cohere.modeling_cohere._get_unpad_data = get_unpad_data\n    elif model_type == \"falcon\":\n        transformers.models.falcon.modeling_falcon._get_unpad_data = get_unpad_data\n    elif model_type == \"gemma\":\n        transformers.models.gemma.modeling_gemma._get_unpad_data = get_unpad_data\n    elif model_type == \"gemma2\":\n        transformers.models.gemma2.modeling_gemma2._get_unpad_data = get_unpad_data\n    elif model_type == \"llama\":\n        transformers.models.llama.modeling_llama._get_unpad_data = get_unpad_data\n    elif model_type == \"mistral\":\n        transformers.models.mistral.modeling_mistral._get_unpad_data = get_unpad_data\n    elif model_type == \"phi\":\n        transformers.models.phi.modeling_phi._get_unpad_data = get_unpad_data\n    elif model_type == \"phi3\":\n        transformers.models.phi3.modeling_phi3._get_unpad_data = get_unpad_data\n    elif model_type == \"qwen2\":\n        transformers.models.qwen2.modeling_qwen2._get_unpad_data = get_unpad_data\n    elif model_type == \"starcoder2\":\n        transformers.models.starcoder2.modeling_starcoder2._get_unpad_data = get_unpad_data\n\n\ndef configure_packing(config: \"PretrainedConfig\", model_args: \"ModelArguments\", is_trainable: bool) -> None:\n    if not is_trainable or not model_args.block_diag_attn:\n        return\n\n    model_type = getattr(config, \"model_type\", None)\n    if model_type in SUPPORTED_CLASS_FOR_BLOCK_DIAG_ATTN:\n        patch_for_block_diag_attn(model_type)\n        logger.info(\"Using block diagonal attention for sequence packing without cross-attention.\")\n    else:\n        raise ValueError(\"Current model does not support block diagonal attention.\")\n```"
        }
    ],
    "temperature": 0
}