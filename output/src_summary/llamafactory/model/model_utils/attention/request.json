{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nfrom typing import TYPE_CHECKING\n\nfrom transformers.utils import is_flash_attn_2_available, is_torch_sdpa_available\n\nfrom ...extras.logging import get_logger\n\n\nif TYPE_CHECKING:\n    from transformers import PretrainedConfig\n\n    from ...hparams import ModelArguments\n\n\nlogger = get_logger(__name__)\n\n\ndef configure_attn_implementation(\n    config: \"PretrainedConfig\", model_args: \"ModelArguments\", is_trainable: bool\n) -> None:\n    if getattr(config, \"model_type\", None) == \"gemma2\" and is_trainable:  \n        if model_args.flash_attn == \"auto\":\n            logger.warning(\"Gemma-2 models should use eager attention in training, change `flash_attn` to disabled.\")\n            model_args.flash_attn = \"disabled\"\n        elif model_args.flash_attn != \"disabled\":\n            logger.warning(\n                \"Gemma-2 models should use eager attention in training, but you set `flash_attn: {}`. \"\n                \"Will proceed at your own risk.\".format(model_args.flash_attn)\n            )\n\n    if model_args.flash_attn == \"auto\":\n        return\n\n    elif model_args.flash_attn == \"disabled\":\n        requested_attn_implementation = \"eager\"\n\n    elif model_args.flash_attn == \"sdpa\":\n        if not is_torch_sdpa_available():\n            logger.warning(\"torch>=2.1.1 is required for SDPA attention.\")\n            return\n\n        requested_attn_implementation = \"sdpa\"\n    elif model_args.flash_attn == \"fa2\":\n        if not is_flash_attn_2_available():\n            logger.warning(\"FlashAttention-2 is not installed.\")\n            return\n\n        requested_attn_implementation = \"flash_attention_2\"\n    else:\n        raise NotImplementedError(\"Unknown attention type: {}\".format(model_args.flash_attn))\n\n    if getattr(config, \"model_type\", None) == \"internlm2\":  \n        setattr(config, \"attn_implementation\", requested_attn_implementation)\n    else:\n        setattr(config, \"_attn_implementation\", requested_attn_implementation)\n\n\ndef print_attn_implementation(config: \"PretrainedConfig\") -> None:\n    if getattr(config, \"model_type\", None) == \"internlm2\":  \n        attn_implementation = getattr(config, \"attn_implementation\", None)\n    else:\n        attn_implementation = getattr(config, \"_attn_implementation\", None)\n\n    if attn_implementation == \"flash_attention_2\":\n        logger.info(\"Using FlashAttention-2 for faster training and inference.\")\n    elif attn_implementation == \"sdpa\":\n        logger.info(\"Using torch SDPA for faster training and inference.\")\n    else:\n        logger.info(\"Using vanilla attention implementation.\")\n```"
        }
    ],
    "temperature": 0
}