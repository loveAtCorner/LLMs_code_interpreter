{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nimport os\nfrom types import MethodType\nfrom typing import TYPE_CHECKING, Any, Dict\n\nimport torch\nfrom peft import PeftModel\nfrom transformers import PreTrainedModel, PreTrainedTokenizerBase, is_torch_npu_available\nfrom transformers.integrations import is_deepspeed_zero3_enabled\nfrom transformers.modeling_utils import is_fsdp_enabled\n\nfrom ..extras.logging import get_logger\nfrom ..extras.misc import infer_optim_dtype\nfrom .model_utils.attention import configure_attn_implementation, print_attn_implementation\nfrom .model_utils.checkpointing import prepare_model_for_training\nfrom .model_utils.embedding import resize_embedding_layer\nfrom .model_utils.longlora import configure_longlora\nfrom .model_utils.moe import add_z3_leaf_module, configure_moe\nfrom .model_utils.packing import configure_packing\nfrom .model_utils.quantization import configure_quantization\nfrom .model_utils.rope import configure_rope\nfrom .model_utils.valuehead import prepare_valuehead_model\nfrom .model_utils.visual import autocast_projector_dtype, configure_visual_model\n\n\nif TYPE_CHECKING:\n    from transformers import PretrainedConfig, PreTrainedTokenizer\n    from trl import AutoModelForCausalLMWithValueHead\n\n    from ..hparams import ModelArguments\n\n\nlogger = get_logger(__name__)\n\n\ndef patch_tokenizer(tokenizer: \"PreTrainedTokenizer\") -> None:\n    if \"PreTrainedTokenizerBase\" not in str(tokenizer._pad.__func__):\n        tokenizer._pad = MethodType(PreTrainedTokenizerBase._pad, tokenizer)\n\n\ndef patch_config(\n    config: \"PretrainedConfig\",\n    tokenizer: \"PreTrainedTokenizer\",\n    model_args: \"ModelArguments\",\n    init_kwargs: Dict[str, Any],\n    is_trainable: bool,\n) -> None:\n    if model_args.compute_dtype is None:  \n        if model_args.infer_dtype != \"auto\" and not is_trainable:\n            model_args.compute_dtype = getattr(torch, model_args.infer_dtype)\n        else:\n            model_args.compute_dtype = infer_optim_dtype(model_dtype=getattr(config, \"torch_dtype\", None))\n\n    if is_torch_npu_available():\n        use_jit_compile = os.environ.get(\"JIT_COMPILE\", \"0\").lower() in [\"true\", \"1\"]\n        torch.npu.set_compile_mode(jit_compile=use_jit_compile)\n\n    configure_attn_implementation(config, model_args, is_trainable)\n    configure_rope(config, model_args, is_trainable)\n    configure_longlora(config, model_args, is_trainable)\n    configure_quantization(config, tokenizer, model_args, init_kwargs)\n    configure_moe(config, model_args, is_trainable)\n    configure_visual_model(config)\n    configure_packing(config, model_args, is_trainable)\n\n    if model_args.use_cache and not is_trainable:\n        setattr(config, \"use_cache\", True)\n        logger.info(\"Using KV cache for faster generation.\")\n\n    if getattr(config, \"model_type\", None) == \"qwen\":\n        setattr(config, \"use_flash_attn\", model_args.flash_attn == \"fa2\")\n        for dtype_name, dtype in [(\"fp16\", torch.float16), (\"bf16\", torch.bfloat16), (\"fp32\", torch.float32)]:\n            setattr(config, dtype_name, model_args.compute_dtype == dtype)\n\n    if getattr(config, \"model_type\", None) == \"qwen2\" and is_trainable and model_args.flash_attn == \"fa2\":\n        setattr(config, \"use_cache\", False)  \n\n    \n    init_kwargs[\"low_cpu_mem_usage\"] = model_args.low_cpu_mem_usage and (not is_deepspeed_zero3_enabled())\n\n    \n    \n    \n    if (not is_deepspeed_zero3_enabled() and not is_fsdp_enabled()) or model_args.quantization_bit is not None:\n        init_kwargs[\"torch_dtype\"] = model_args.compute_dtype\n\n        if init_kwargs[\"low_cpu_mem_usage\"]:  \n            if \"device_map\" not in init_kwargs and model_args.device_map:\n                init_kwargs[\"device_map\"] = model_args.device_map\n\n            if init_kwargs.get(\"device_map\", None) == \"auto\":\n                init_kwargs[\"offload_folder\"] = model_args.offload_folder\n\n\ndef patch_model(\n    model: \"PreTrainedModel\",\n    tokenizer: \"PreTrainedTokenizer\",\n    model_args: \"ModelArguments\",\n    is_trainable: bool,\n    add_valuehead: bool,\n) -> None:\n    gen_config = model.generation_config  \n    if not gen_config.do_sample and (\n        (gen_config.temperature is not None and gen_config.temperature != 1.0)\n        or (gen_config.top_p is not None and gen_config.top_p != 1.0)\n        or (gen_config.typical_p is not None and gen_config.typical_p != 1.0)\n    ):\n        gen_config.do_sample = True\n\n    if \"GenerationMixin\" not in str(model.generate.__func__):\n        model.generate = MethodType(PreTrainedModel.generate, model)\n\n    if add_valuehead:\n        prepare_valuehead_model(model)\n\n    if model_args.resize_vocab:\n        resize_embedding_layer(model, tokenizer)\n\n    if model_args.visual_inputs:\n        autocast_projector_dtype(model, model_args)\n\n    if is_trainable:\n        prepare_model_for_training(model, model_args)\n        add_z3_leaf_module(model)\n\n    if not model_args.use_unsloth:\n        print_attn_implementation(model.config)\n\n    try:\n        model.add_model_tags([\"llama-factory\"])\n    except Exception:\n        logger.warning(\"Cannot properly tag the model.\")\n\n\ndef patch_valuehead_model(model: \"AutoModelForCausalLMWithValueHead\") -> None:\n    def tie_weights(self: \"AutoModelForCausalLMWithValueHead\") -> None:\n        if isinstance(self.pretrained_model, PreTrainedModel):\n            self.pretrained_model.tie_weights()\n\n    def get_input_embeddings(self: \"AutoModelForCausalLMWithValueHead\") -> torch.nn.Module:\n        if isinstance(self.pretrained_model, PreTrainedModel):\n            return self.pretrained_model.get_input_embeddings()\n\n    def get_output_embeddings(self: \"AutoModelForCausalLMWithValueHead\") -> torch.nn.Module:\n        if isinstance(self.pretrained_model, PreTrainedModel):\n            return self.pretrained_model.get_output_embeddings()\n\n    def create_or_update_model_card(self: \"AutoModelForCausalLMWithValueHead\", output_dir: str) -> None:\n        if isinstance(self.pretrained_model, PeftModel):\n            self.pretrained_model.create_or_update_model_card(output_dir)\n\n    ignore_modules = [name for name, _ in model.named_parameters() if \"pretrained_model\" in name]\n    setattr(model, \"_keys_to_ignore_on_save\", ignore_modules)\n    setattr(model, \"tie_weights\", MethodType(tie_weights, model))\n    setattr(model, \"get_input_embeddings\", MethodType(get_input_embeddings, model))\n    setattr(model, \"get_output_embeddings\", MethodType(get_output_embeddings, model))\n    setattr(model, \"create_or_update_model_card\", MethodType(create_or_update_model_card, model))\n```"
        }
    ],
    "temperature": 0
}