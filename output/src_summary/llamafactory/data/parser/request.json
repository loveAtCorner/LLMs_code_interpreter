{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nimport json\nimport os\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Any, Dict, List, Literal, Optional\n\nfrom ..extras.constants import DATA_CONFIG\nfrom ..extras.misc import use_modelscope\n\n\nif TYPE_CHECKING:\n    from ..hparams import DataArguments\n\n\n@dataclass\nclass DatasetAttr:\n    r\n\n    \n    load_from: Literal[\"hf_hub\", \"ms_hub\", \"script\", \"file\"]\n    dataset_name: str\n    formatting: Literal[\"alpaca\", \"sharegpt\"] = \"alpaca\"\n    ranking: bool = False\n    \n    subset: Optional[str] = None\n    folder: Optional[str] = None\n    num_samples: Optional[int] = None\n    \n    system: Optional[str] = None\n    tools: Optional[str] = None\n    images: Optional[str] = None\n    \n    chosen: Optional[str] = None\n    rejected: Optional[str] = None\n    kto_tag: Optional[str] = None\n    \n    prompt: Optional[str] = \"instruction\"\n    query: Optional[str] = \"input\"\n    response: Optional[str] = \"output\"\n    history: Optional[str] = None\n    \n    messages: Optional[str] = \"conversations\"\n    \n    role_tag: Optional[str] = \"from\"\n    content_tag: Optional[str] = \"value\"\n    user_tag: Optional[str] = \"human\"\n    assistant_tag: Optional[str] = \"gpt\"\n    observation_tag: Optional[str] = \"observation\"\n    function_tag: Optional[str] = \"function_call\"\n    system_tag: Optional[str] = \"system\"\n\n    def __repr__(self) -> str:\n        return self.dataset_name\n\n    def set_attr(self, key: str, obj: Dict[str, Any], default: Optional[Any] = None) -> None:\n        setattr(self, key, obj.get(key, default))\n\n\ndef get_dataset_list(data_args: \"DataArguments\") -> List[\"DatasetAttr\"]:\n    if data_args.dataset is not None:\n        dataset_names = [ds.strip() for ds in data_args.dataset.split(\",\")]\n    else:\n        dataset_names = []\n\n    if data_args.dataset_dir == \"ONLINE\":\n        dataset_info = None\n    else:\n        try:\n            with open(os.path.join(data_args.dataset_dir, DATA_CONFIG), \"r\") as f:\n                dataset_info = json.load(f)\n        except Exception as err:\n            if len(dataset_names) != 0:\n                raise ValueError(\n                    \"Cannot open {} due to {}.\".format(os.path.join(data_args.dataset_dir, DATA_CONFIG), str(err))\n                )\n            dataset_info = None\n\n    if data_args.interleave_probs is not None:\n        data_args.interleave_probs = [float(prob.strip()) for prob in data_args.interleave_probs.split(\",\")]\n\n    dataset_list: List[DatasetAttr] = []\n    for name in dataset_names:\n        if dataset_info is None:\n            load_from = \"ms_hub\" if use_modelscope() else \"hf_hub\"\n            dataset_attr = DatasetAttr(load_from, dataset_name=name)\n            dataset_list.append(dataset_attr)\n            continue\n\n        if name not in dataset_info:\n            raise ValueError(\"Undefined dataset {} in {}.\".format(name, DATA_CONFIG))\n\n        has_hf_url = \"hf_hub_url\" in dataset_info[name]\n        has_ms_url = \"ms_hub_url\" in dataset_info[name]\n\n        if has_hf_url or has_ms_url:\n            if (use_modelscope() and has_ms_url) or (not has_hf_url):\n                dataset_attr = DatasetAttr(\"ms_hub\", dataset_name=dataset_info[name][\"ms_hub_url\"])\n            else:\n                dataset_attr = DatasetAttr(\"hf_hub\", dataset_name=dataset_info[name][\"hf_hub_url\"])\n        elif \"script_url\" in dataset_info[name]:\n            dataset_attr = DatasetAttr(\"script\", dataset_name=dataset_info[name][\"script_url\"])\n        else:\n            dataset_attr = DatasetAttr(\"file\", dataset_name=dataset_info[name][\"file_name\"])\n\n        dataset_attr.set_attr(\"formatting\", dataset_info[name], default=\"alpaca\")\n        dataset_attr.set_attr(\"ranking\", dataset_info[name], default=False)\n        dataset_attr.set_attr(\"subset\", dataset_info[name])\n        dataset_attr.set_attr(\"folder\", dataset_info[name])\n        dataset_attr.set_attr(\"num_samples\", dataset_info[name])\n\n        if \"columns\" in dataset_info[name]:\n            column_names = [\"system\", \"tools\", \"images\", \"chosen\", \"rejected\", \"kto_tag\"]\n            if dataset_attr.formatting == \"alpaca\":\n                column_names.extend([\"prompt\", \"query\", \"response\", \"history\"])\n            else:\n                column_names.extend([\"messages\"])\n\n            for column_name in column_names:\n                dataset_attr.set_attr(column_name, dataset_info[name][\"columns\"])\n\n        if dataset_attr.formatting == \"sharegpt\" and \"tags\" in dataset_info[name]:\n            tag_names = (\n                \"role_tag\",\n                \"content_tag\",\n                \"user_tag\",\n                \"assistant_tag\",\n                \"observation_tag\",\n                \"function_tag\",\n                \"system_tag\",\n            )\n            for tag in tag_names:\n                dataset_attr.set_attr(tag, dataset_info[name][\"tags\"])\n\n        dataset_list.append(dataset_attr)\n\n    return dataset_list\n```"
        }
    ],
    "temperature": 0
}