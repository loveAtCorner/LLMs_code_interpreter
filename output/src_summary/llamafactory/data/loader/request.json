{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nimport inspect\nimport os\nimport sys\nfrom typing import TYPE_CHECKING, Literal, Optional, Union\n\nimport numpy as np\nfrom datasets import load_dataset, load_from_disk\n\nfrom ..extras.constants import FILEEXT2TYPE\nfrom ..extras.logging import get_logger\nfrom ..extras.misc import has_tokenized_data\nfrom .aligner import align_dataset\nfrom .data_utils import merge_dataset\nfrom .parser import get_dataset_list\nfrom .preprocess import get_preprocess_and_print_func\nfrom .template import get_template_and_fix_tokenizer\n\n\nif TYPE_CHECKING:\n    from datasets import Dataset, IterableDataset\n    from transformers import PreTrainedTokenizer, ProcessorMixin, Seq2SeqTrainingArguments\n\n    from ..hparams import DataArguments, ModelArguments\n    from .parser import DatasetAttr\n\n\nlogger = get_logger(__name__)\n\n\ndef load_single_dataset(\n    dataset_attr: \"DatasetAttr\",\n    model_args: \"ModelArguments\",\n    data_args: \"DataArguments\",\n    training_args: \"Seq2SeqTrainingArguments\",\n) -> Union[\"Dataset\", \"IterableDataset\"]:\n    logger.info(\"Loading dataset {}...\".format(dataset_attr))\n    data_path, data_name, data_dir, data_files = None, None, None, None\n    if dataset_attr.load_from in [\"hf_hub\", \"ms_hub\"]:\n        data_path = dataset_attr.dataset_name\n        data_name = dataset_attr.subset\n        data_dir = dataset_attr.folder\n\n    elif dataset_attr.load_from == \"script\":\n        data_path = os.path.join(data_args.dataset_dir, dataset_attr.dataset_name)\n        data_name = dataset_attr.subset\n        data_dir = dataset_attr.folder\n\n    elif dataset_attr.load_from == \"file\":\n        data_files = []\n        local_path = os.path.join(data_args.dataset_dir, dataset_attr.dataset_name)\n        if os.path.isdir(local_path):  \n            for file_name in os.listdir(local_path):\n                data_files.append(os.path.join(local_path, file_name))\n                if data_path is None:\n                    data_path = FILEEXT2TYPE.get(file_name.split(\".\")[-1], None)\n                elif data_path != FILEEXT2TYPE.get(file_name.split(\".\")[-1], None):\n                    raise ValueError(\"File types should be identical.\")\n        elif os.path.isfile(local_path):  \n            data_files.append(local_path)\n            data_path = FILEEXT2TYPE.get(local_path.split(\".\")[-1], None)\n        else:\n            raise ValueError(\"File {} not found.\".format(local_path))\n\n        if data_path is None:\n            raise ValueError(\"Allowed file types: {}.\".format(\",\".join(FILEEXT2TYPE.keys())))\n    else:\n        raise NotImplementedError(\"Unknown load type: {}.\".format(dataset_attr.load_from))\n\n    if dataset_attr.load_from == \"ms_hub\":\n        try:\n            from modelscope import MsDataset\n            from modelscope.utils.config_ds import MS_DATASETS_CACHE\n\n            cache_dir = model_args.cache_dir or MS_DATASETS_CACHE\n            dataset = MsDataset.load(\n                dataset_name=data_path,\n                subset_name=data_name,\n                data_dir=data_dir,\n                data_files=data_files,\n                split=data_args.split,\n                cache_dir=cache_dir,\n                token=model_args.ms_hub_token,\n                use_streaming=(data_args.streaming and (dataset_attr.load_from != \"file\")),\n            )\n            if isinstance(dataset, MsDataset):\n                dataset = dataset.to_hf_dataset()\n        except ImportError:\n            raise ImportError(\"Please install modelscope via `pip install modelscope -U`\")\n    else:\n        if \"trust_remote_code\" in inspect.signature(load_dataset).parameters:  \n            kwargs = {\"trust_remote_code\": True}\n        else:\n            kwargs = {}\n\n        dataset = load_dataset(\n            path=data_path,\n            name=data_name,\n            data_dir=data_dir,\n            data_files=data_files,\n            split=data_args.split,\n            cache_dir=model_args.cache_dir,\n            token=model_args.hf_hub_token,\n            streaming=(data_args.streaming and (dataset_attr.load_from != \"file\")),\n            **kwargs,\n        )\n\n    if data_args.streaming and (dataset_attr.load_from == \"file\"):  \n        dataset = dataset.to_iterable_dataset()  \n\n    if dataset_attr.num_samples is not None and not data_args.streaming:\n        target_num = dataset_attr.num_samples\n        indexes = np.random.permutation(len(dataset))[:target_num]\n        target_num -= len(indexes)\n        if target_num > 0:\n            expand_indexes = np.random.choice(len(dataset), target_num)\n            indexes = np.concatenate((indexes, expand_indexes), axis=0)\n\n        assert len(indexes) == dataset_attr.num_samples, \"Sample num mismatched.\"\n        dataset = dataset.select(indexes)\n        logger.info(\"Sampled {} examples from dataset {}.\".format(dataset_attr.num_samples, dataset_attr))\n\n    if data_args.max_samples is not None:  \n        max_samples = min(data_args.max_samples, len(dataset))\n        dataset = dataset.select(range(max_samples))\n\n    return align_dataset(dataset, dataset_attr, data_args, training_args)\n\n\ndef get_dataset(\n    model_args: \"ModelArguments\",\n    data_args: \"DataArguments\",\n    training_args: \"Seq2SeqTrainingArguments\",\n    stage: Literal[\"pt\", \"sft\", \"rm\", \"ppo\", \"kto\"],\n    tokenizer: \"PreTrainedTokenizer\",\n    processor: Optional[\"ProcessorMixin\"] = None,\n) -> Union[\"Dataset\", \"IterableDataset\"]:\n    template = get_template_and_fix_tokenizer(tokenizer, data_args.template, data_args.tool_format)\n    if data_args.train_on_prompt and template.efficient_eos:\n        raise ValueError(\"Current template does not support `train_on_prompt`.\")\n\n    \n    if data_args.tokenized_path is not None:\n        if has_tokenized_data(data_args.tokenized_path):\n            logger.warning(\"Loading dataset from disk will ignore other data arguments.\")\n            dataset = load_from_disk(data_args.tokenized_path)\n            logger.info(\"Loaded tokenized dataset from {}.\".format(data_args.tokenized_path))\n            if data_args.streaming:\n                dataset = dataset.to_iterable_dataset()\n            return dataset\n\n        if data_args.streaming:\n            raise ValueError(\"Turn off `streaming` when saving dataset to disk.\")\n\n    with training_args.main_process_first(desc=\"load dataset\"):\n        all_datasets = []\n        for dataset_attr in get_dataset_list(data_args):\n            if (stage == \"rm\" and dataset_attr.ranking is False) or (stage != \"rm\" and dataset_attr.ranking is True):\n                raise ValueError(\"The dataset is not applicable in the current training stage.\")\n\n            all_datasets.append(load_single_dataset(dataset_attr, model_args, data_args, training_args))\n\n        dataset = merge_dataset(all_datasets, data_args, training_args)\n\n    with training_args.main_process_first(desc=\"pre-process dataset\"):\n        preprocess_func, print_function = get_preprocess_and_print_func(\n            data_args, training_args, stage, template, tokenizer, processor\n        )\n        column_names = list(next(iter(dataset)).keys())\n        kwargs = {}\n        if not data_args.streaming:\n            kwargs = dict(\n                num_proc=data_args.preprocessing_num_workers,\n                load_from_cache_file=(not data_args.overwrite_cache) or (training_args.local_process_index != 0),\n                desc=\"Running tokenizer on dataset\",\n            )\n\n        dataset = dataset.map(preprocess_func, batched=True, remove_columns=column_names, **kwargs)\n\n        if data_args.tokenized_path is not None:\n            if training_args.should_save:\n                dataset.save_to_disk(data_args.tokenized_path)\n                logger.info(\"Tokenized dataset saved at {}.\".format(data_args.tokenized_path))\n                logger.info(\"Please restart the training with `tokenized_path: {}`.\".format(data_args.tokenized_path))\n\n            sys.exit(0)\n\n        if training_args.should_log:\n            try:\n                print_function(next(iter(dataset)))\n            except StopIteration:\n                if stage == \"pt\":\n                    raise RuntimeError(\"Cannot find sufficient samples, consider increasing dataset size.\")\n                else:\n                    raise RuntimeError(\"Cannot find valid samples, check `data/README.md` for the data format.\")\n\n        return dataset\n```"
        }
    ],
    "temperature": 0
}