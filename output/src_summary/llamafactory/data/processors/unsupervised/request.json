{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, Tuple\n\nfrom ...extras.logging import get_logger\nfrom ..data_utils import Role\nfrom .processor_utils import get_paligemma_token_type_ids, get_pixel_values, infer_seqlen\n\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedTokenizer, ProcessorMixin\n\n    from ...hparams import DataArguments\n    from ..template import Template\n\n\nlogger = get_logger(__name__)\n\n\ndef _encode_unsupervised_example(\n    prompt: Sequence[Dict[str, str]],\n    response: Sequence[Dict[str, str]],\n    system: Optional[str],\n    tools: Optional[str],\n    template: \"Template\",\n    tokenizer: \"PreTrainedTokenizer\",\n    processor: Optional[\"ProcessorMixin\"],\n    data_args: \"DataArguments\",\n) -> Tuple[List[int], List[int]]:\n    if processor is not None and not hasattr(processor, \"image_seq_length\"):  \n        prompt[0][\"content\"] = template.image_token + prompt[0][\"content\"]\n\n    if len(response) == 1:\n        messages = prompt + response\n    else:\n        messages = prompt + [{\"role\": Role.ASSISTANT.value, \"content\": \"\"}]\n\n    input_ids, labels = template.encode_oneturn(tokenizer, messages, system, tools)\n    if template.efficient_eos:\n        labels += [tokenizer.eos_token_id]\n\n    if processor is not None and hasattr(processor, \"image_seq_length\"):  \n        image_token_id = tokenizer.convert_tokens_to_ids(template.image_token)\n        input_ids = [image_token_id] * getattr(processor, \"image_seq_length\") + input_ids\n\n    source_len, target_len = infer_seqlen(len(input_ids), len(labels), data_args.cutoff_len)\n    input_ids = input_ids[:source_len]\n    labels = labels[:target_len]\n    return input_ids, labels\n\n\ndef preprocess_unsupervised_dataset(\n    examples: Dict[str, List[Any]],\n    template: \"Template\",\n    tokenizer: \"PreTrainedTokenizer\",\n    processor: Optional[\"ProcessorMixin\"],\n    data_args: \"DataArguments\",\n) -> Dict[str, List[List[int]]]:\n    \n    model_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n    if processor is not None:\n        model_inputs[\"pixel_values\"] = []\n        if hasattr(processor, \"image_seq_length\"):  \n            model_inputs[\"token_type_ids\"] = []\n\n    for i in range(len(examples[\"prompt\"])):\n        if len(examples[\"prompt\"][i]) % 2 != 1:\n            logger.warning(\"Dropped invalid example: {}\".format(examples[\"prompt\"][i] + examples[\"response\"][i]))\n            continue\n\n        input_ids, labels = _encode_unsupervised_example(\n            prompt=examples[\"prompt\"][i],\n            response=examples[\"response\"][i],\n            system=examples[\"system\"][i],\n            tools=examples[\"tools\"][i],\n            template=template,\n            tokenizer=tokenizer,\n            processor=processor,\n            data_args=data_args,\n        )\n        model_inputs[\"input_ids\"].append(input_ids)\n        model_inputs[\"attention_mask\"].append([1] * len(input_ids))\n        model_inputs[\"labels\"].append(labels)\n        if processor is not None:\n            model_inputs[\"pixel_values\"].append(get_pixel_values(examples[\"images\"][i], processor))\n            if hasattr(processor, \"image_seq_length\"):  \n                model_inputs[\"token_type_ids\"].append(get_paligemma_token_type_ids(len(input_ids), processor))\n\n    return model_inputs\n\n\ndef print_unsupervised_dataset_example(example: Dict[str, List[int]], tokenizer: \"PreTrainedTokenizer\") -> None:\n    print(\"input_ids:\\n{}\".format(example[\"input_ids\"]))\n    print(\"inputs:\\n{}\".format(tokenizer.decode(example[\"input_ids\"], skip_special_tokens=False)))\n```"
        }
    ],
    "temperature": 0
}