{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nfrom collections import defaultdict\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Sequence, Tuple\n\nfrom ...extras.constants import IGNORE_INDEX\nfrom ...extras.logging import get_logger\nfrom .processor_utils import get_paligemma_token_type_ids, get_pixel_values, greedy_knapsack, infer_seqlen\n\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedTokenizer, ProcessorMixin\n\n    from ...hparams import DataArguments\n    from ..template import Template\n\n\nlogger = get_logger(__name__)\n\n\ndef _encode_supervised_example(\n    prompt: Sequence[Dict[str, str]],\n    response: Sequence[Dict[str, str]],\n    system: Optional[str],\n    tools: Optional[str],\n    template: \"Template\",\n    tokenizer: \"PreTrainedTokenizer\",\n    processor: Optional[\"ProcessorMixin\"],\n    data_args: \"DataArguments\",\n) -> Tuple[List[int], List[int]]:\n    if processor is not None and not hasattr(processor, \"image_seq_length\"):  \n        prompt[0][\"content\"] = template.image_token + prompt[0][\"content\"]\n\n    messages = prompt + response\n    input_ids, labels = [], []\n\n    if processor is not None and hasattr(processor, \"image_seq_length\"):  \n        image_token_id = tokenizer.convert_tokens_to_ids(template.image_token)\n        input_ids += [image_token_id] * getattr(processor, \"image_seq_length\")\n        labels += [IGNORE_INDEX] * getattr(processor, \"image_seq_length\")\n\n    encoded_pairs = template.encode_multiturn(tokenizer, messages, system, tools)\n    total_length = 1 if template.efficient_eos else 0\n    for turn_idx, (source_ids, target_ids) in enumerate(encoded_pairs):\n        if total_length >= data_args.cutoff_len:\n            break\n\n        source_len, target_len = infer_seqlen(len(source_ids), len(target_ids), data_args.cutoff_len - total_length)\n        source_ids = source_ids[:source_len]\n        target_ids = target_ids[:target_len]\n        total_length += source_len + target_len\n\n        if data_args.train_on_prompt:\n            source_mask = source_ids\n        elif turn_idx != 0 and template.efficient_eos:\n            source_mask = [tokenizer.eos_token_id] + [IGNORE_INDEX] * (source_len - 1)\n        else:\n            source_mask = [IGNORE_INDEX] * source_len\n\n        input_ids += source_ids + target_ids\n        labels += source_mask + target_ids\n\n    if template.efficient_eos:\n        input_ids += [tokenizer.eos_token_id]\n        labels += [tokenizer.eos_token_id]\n\n    return input_ids, labels\n\n\ndef preprocess_supervised_dataset(\n    examples: Dict[str, List[Any]],\n    template: \"Template\",\n    tokenizer: \"PreTrainedTokenizer\",\n    processor: Optional[\"ProcessorMixin\"],\n    data_args: \"DataArguments\",\n) -> Dict[str, List[List[int]]]:\n    \n    \n    model_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n    if processor is not None:\n        model_inputs[\"pixel_values\"] = []\n        if hasattr(processor, \"image_seq_length\"):  \n            model_inputs[\"token_type_ids\"] = []\n\n    for i in range(len(examples[\"prompt\"])):\n        if len(examples[\"prompt\"][i]) % 2 != 1 or len(examples[\"response\"][i]) != 1:\n            logger.warning(\"Dropped invalid example: {}\".format(examples[\"prompt\"][i] + examples[\"response\"][i]))\n            continue\n\n        input_ids, labels = _encode_supervised_example(\n            prompt=examples[\"prompt\"][i],\n            response=examples[\"response\"][i],\n            system=examples[\"system\"][i],\n            tools=examples[\"tools\"][i],\n            template=template,\n            tokenizer=tokenizer,\n            processor=processor,\n            data_args=data_args,\n        )\n        model_inputs[\"input_ids\"].append(input_ids)\n        model_inputs[\"attention_mask\"].append([1] * len(input_ids))\n        model_inputs[\"labels\"].append(labels)\n        if processor is not None:\n            model_inputs[\"pixel_values\"].append(get_pixel_values(examples[\"images\"][i], processor))\n            if hasattr(processor, \"image_seq_length\"):  \n                model_inputs[\"token_type_ids\"].append(get_paligemma_token_type_ids(len(input_ids), processor))\n\n    return model_inputs\n\n\ndef preprocess_packed_supervised_dataset(\n    examples: Dict[str, List[Any]],\n    template: \"Template\",\n    tokenizer: \"PreTrainedTokenizer\",\n    data_args: \"DataArguments\",\n) -> Dict[str, List[List[int]]]:\n    \n    \n    valid_num = 0\n    batch_input_ids, batch_labels = [], []\n    lengths = []\n    length2indexes = defaultdict(list)\n    for i in range(len(examples[\"prompt\"])):\n        if len(examples[\"prompt\"][i]) % 2 != 1 or len(examples[\"response\"][i]) != 1:\n            logger.warning(\"Dropped invalid example: {}\".format(examples[\"prompt\"][i] + examples[\"response\"][i]))\n            continue\n\n        input_ids, labels = _encode_supervised_example(\n            prompt=examples[\"prompt\"][i],\n            response=examples[\"response\"][i],\n            system=examples[\"system\"][i],\n            tools=examples[\"tools\"][i],\n            template=template,\n            tokenizer=tokenizer,\n            processor=None,\n            data_args=data_args,\n        )\n        length = len(input_ids)\n        if length > data_args.cutoff_len:\n            logger.warning(\"Dropped lengthy example with length {} > {}.\".format(length, data_args.cutoff_len))\n        else:\n            lengths.append(length)\n            length2indexes[length].append(valid_num)\n            batch_input_ids.append(input_ids)\n            batch_labels.append(labels)\n            valid_num += 1\n\n    model_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n    knapsacks = greedy_knapsack(lengths, data_args.cutoff_len)\n    for knapsack in knapsacks:\n        packed_input_ids, packed_attention_masks, packed_labels = [], [], []\n        for i, length in enumerate(knapsack):\n            index = length2indexes[length].pop()\n            packed_input_ids += batch_input_ids[index]\n            packed_labels += batch_labels[index]\n            if data_args.neat_packing:\n                packed_attention_masks += [i + 1] * len(batch_input_ids[index])  \n            else:\n                packed_attention_masks += [1] * len(batch_input_ids[index])\n\n        if len(packed_input_ids) < data_args.cutoff_len:\n            pad_length = data_args.cutoff_len - len(packed_input_ids)\n            packed_input_ids += [tokenizer.pad_token_id] * pad_length\n            packed_labels += [IGNORE_INDEX] * pad_length\n            if data_args.neat_packing:\n                packed_attention_masks += [0] * pad_length\n            else:\n                packed_attention_masks += [1] * pad_length  \n\n        if len(packed_input_ids) != data_args.cutoff_len:\n            raise ValueError(\"The length of packed example should be identical to the cutoff length.\")\n\n        model_inputs[\"input_ids\"].append(packed_input_ids)\n        model_inputs[\"attention_mask\"].append(packed_attention_masks)\n        model_inputs[\"labels\"].append(packed_labels)\n\n    return model_inputs\n\n\ndef print_supervised_dataset_example(example: Dict[str, List[int]], tokenizer: \"PreTrainedTokenizer\") -> None:\n    valid_labels = list(filter(lambda x: x != IGNORE_INDEX, example[\"labels\"]))\n    print(\"input_ids:\\n{}\".format(example[\"input_ids\"]))\n    print(\"inputs:\\n{}\".format(tokenizer.decode(example[\"input_ids\"], skip_special_tokens=False)))\n    print(\"label_ids:\\n{}\".format(example[\"labels\"]))\n    print(\"labels:\\n{}\".format(tokenizer.decode(valid_labels, skip_special_tokens=False)))\n```"
        }
    ],
    "temperature": 0
}