{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nimport uuid\nfrom typing import TYPE_CHECKING, Any, AsyncGenerator, AsyncIterator, Dict, List, Optional, Sequence, Union\n\nfrom ..data import get_template_and_fix_tokenizer\nfrom ..extras.logging import get_logger\nfrom ..extras.misc import get_device_count\nfrom ..extras.packages import is_vllm_available, is_vllm_version_greater_than_0_5, is_vllm_version_greater_than_0_5_1\nfrom ..model import load_config, load_tokenizer\nfrom ..model.model_utils.quantization import QuantizationMethod\nfrom ..model.model_utils.visual import LlavaMultiModalProjectorForYiVLForVLLM\nfrom .base_engine import BaseEngine, Response\n\n\nif is_vllm_available():\n    from vllm import AsyncEngineArgs, AsyncLLMEngine, RequestOutput, SamplingParams\n    from vllm.lora.request import LoRARequest\n\n    if is_vllm_version_greater_than_0_5_1():\n        pass\n    elif is_vllm_version_greater_than_0_5():\n        from vllm.multimodal.image import ImagePixelData\n    else:\n        from vllm.sequence import MultiModalData\n\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n    from transformers.image_processing_utils import BaseImageProcessor\n\n    from ..hparams import DataArguments, FinetuningArguments, GeneratingArguments, ModelArguments\n\n\nlogger = get_logger(__name__)\n\n\nclass VllmEngine(BaseEngine):\n    def __init__(\n        self,\n        model_args: \"ModelArguments\",\n        data_args: \"DataArguments\",\n        finetuning_args: \"FinetuningArguments\",\n        generating_args: \"GeneratingArguments\",\n    ) -> None:\n        config = load_config(model_args)  \n        if getattr(config, \"quantization_config\", None):  \n            quantization_config: Dict[str, Any] = getattr(config, \"quantization_config\", None)\n            quant_method = quantization_config.get(\"quant_method\", \"\")\n            if quant_method == QuantizationMethod.GPTQ and model_args.infer_dtype == \"auto\":\n                model_args.infer_dtype = \"float16\"\n\n        self.can_generate = finetuning_args.stage == \"sft\"\n        tokenizer_module = load_tokenizer(model_args)\n        self.tokenizer = tokenizer_module[\"tokenizer\"]\n        self.processor = tokenizer_module[\"processor\"]\n        self.tokenizer.padding_side = \"left\"\n        self.template = get_template_and_fix_tokenizer(self.tokenizer, data_args.template, data_args.tool_format)\n        self.generating_args = generating_args.to_dict()\n\n        engine_args = {\n            \"model\": model_args.model_name_or_path,\n            \"trust_remote_code\": True,\n            \"download_dir\": model_args.cache_dir,\n            \"dtype\": model_args.infer_dtype,\n            \"max_model_len\": model_args.vllm_maxlen,\n            \"tensor_parallel_size\": get_device_count() or 1,\n            \"gpu_memory_utilization\": model_args.vllm_gpu_util,\n            \"disable_log_stats\": True,\n            \"disable_log_requests\": True,\n            \"enforce_eager\": model_args.vllm_enforce_eager,\n            \"enable_lora\": model_args.adapter_name_or_path is not None,\n            \"max_lora_rank\": model_args.vllm_max_lora_rank,\n        }\n\n        if model_args.visual_inputs:\n            image_size = config.vision_config.image_size\n            patch_size = config.vision_config.patch_size\n            self.image_feature_size = (image_size // patch_size) ** 2\n            engine_args[\"image_input_type\"] = \"pixel_values\"\n            engine_args[\"image_token_id\"] = self.tokenizer.convert_tokens_to_ids(self.template.image_token)\n            engine_args[\"image_input_shape\"] = \"1,3,{},{}\".format(image_size, image_size)\n            engine_args[\"image_feature_size\"] = self.image_feature_size\n            if getattr(config, \"is_yi_vl_derived_model\", None):\n                import vllm.model_executor.models.llava\n\n                logger.info(\"Detected Yi-VL model, applying projector patch.\")\n                vllm.model_executor.models.llava.LlavaMultiModalProjector = LlavaMultiModalProjectorForYiVLForVLLM\n\n        self.model = AsyncLLMEngine.from_engine_args(AsyncEngineArgs(**engine_args))\n        if model_args.adapter_name_or_path is not None:\n            self.lora_request = LoRARequest(\"default\", 1, model_args.adapter_name_or_path[0])\n        else:\n            self.lora_request = None\n\n    async def _generate(\n        self,\n        messages: Sequence[Dict[str, str]],\n        system: Optional[str] = None,\n        tools: Optional[str] = None,\n        image: Optional[\"NDArray\"] = None,\n        **input_kwargs,\n    ) -> AsyncIterator[\"RequestOutput\"]:\n        request_id = \"chatcmpl-{}\".format(uuid.uuid4().hex)\n\n        if (\n            self.processor is not None\n            and image is not None\n            and not hasattr(self.processor, \"image_seq_length\")\n            and self.template.image_token not in messages[0][\"content\"]\n        ):  \n            messages[0][\"content\"] = self.template.image_token * self.image_feature_size + messages[0][\"content\"]\n\n        paired_messages = messages + [{\"role\": \"assistant\", \"content\": \"\"}]\n        system = system or self.generating_args[\"default_system\"]\n        prompt_ids, _ = self.template.encode_oneturn(\n            tokenizer=self.tokenizer, messages=paired_messages, system=system, tools=tools\n        )\n\n        if self.processor is not None and image is not None:  \n            image_processor: \"BaseImageProcessor\" = getattr(self.processor, \"image_processor\")\n            pixel_values = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"]\n            if is_vllm_version_greater_than_0_5_1():\n                multi_modal_data = {\"image\": pixel_values}\n            elif is_vllm_version_greater_than_0_5():\n                multi_modal_data = ImagePixelData(image=pixel_values)   \n            else:  \n                multi_modal_data = MultiModalData(type=MultiModalData.Type.IMAGE, data=pixel_values)\n        else:\n            multi_modal_data = None\n\n        prompt_length = len(prompt_ids)\n\n        use_beam_search: bool = self.generating_args[\"num_beams\"] > 1\n        temperature: Optional[float] = input_kwargs.pop(\"temperature\", None)\n        top_p: Optional[float] = input_kwargs.pop(\"top_p\", None)\n        top_k: Optional[float] = input_kwargs.pop(\"top_k\", None)\n        num_return_sequences: int = input_kwargs.pop(\"num_return_sequences\", 1)\n        repetition_penalty: Optional[float] = input_kwargs.pop(\"repetition_penalty\", None)\n        length_penalty: Optional[float] = input_kwargs.pop(\"length_penalty\", None)\n        max_length: Optional[int] = input_kwargs.pop(\"max_length\", None)\n        max_new_tokens: Optional[int] = input_kwargs.pop(\"max_new_tokens\", None)\n        stop: Optional[Union[str, List[str]]] = input_kwargs.pop(\"stop\", None)\n\n        if \"max_new_tokens\" in self.generating_args:\n            max_tokens = self.generating_args[\"max_new_tokens\"]\n        elif \"max_length\" in self.generating_args:\n            if self.generating_args[\"max_length\"] > prompt_length:\n                max_tokens = self.generating_args[\"max_length\"] - prompt_length\n            else:\n                max_tokens = 1\n\n        if max_length:\n            max_tokens = max_length - prompt_length if max_length > prompt_length else 1\n\n        if max_new_tokens:\n            max_tokens = max_new_tokens\n\n        sampling_params = SamplingParams(\n            n=num_return_sequences,\n            repetition_penalty=(\n                repetition_penalty if repetition_penalty is not None else self.generating_args[\"repetition_penalty\"]\n            )\n            or 1.0,  \n            temperature=temperature if temperature is not None else self.generating_args[\"temperature\"],\n            top_p=(top_p if top_p is not None else self.generating_args[\"top_p\"]) or 1.0,  \n            top_k=top_k if top_k is not None else self.generating_args[\"top_k\"],\n            use_beam_search=use_beam_search,\n            length_penalty=length_penalty if length_penalty is not None else self.generating_args[\"length_penalty\"],\n            stop=stop,\n            stop_token_ids=[self.tokenizer.eos_token_id] + self.tokenizer.additional_special_tokens_ids,\n            max_tokens=max_tokens,\n            skip_special_tokens=True,\n        )\n\n        result_generator = self.model.generate(\n            inputs={\"prompt_token_ids\": prompt_ids, \"multi_modal_data\": multi_modal_data},\n            sampling_params=sampling_params,\n            request_id=request_id,\n            lora_request=self.lora_request,\n        )\n        return result_generator\n\n    async def chat(\n        self,\n        messages: Sequence[Dict[str, str]],\n        system: Optional[str] = None,\n        tools: Optional[str] = None,\n        image: Optional[\"NDArray\"] = None,\n        **input_kwargs,\n    ) -> List[\"Response\"]:\n        final_output = None\n        generator = await self._generate(messages, system, tools, image, **input_kwargs)\n        async for request_output in generator:\n            final_output = request_output\n\n        results = []\n        for output in final_output.outputs:\n            results.append(\n                Response(\n                    response_text=output.text,\n                    response_length=len(output.token_ids),\n                    prompt_length=len(final_output.prompt_token_ids),\n                    finish_reason=output.finish_reason,\n                )\n            )\n\n        return results\n\n    async def stream_chat(\n        self,\n        messages: Sequence[Dict[str, str]],\n        system: Optional[str] = None,\n        tools: Optional[str] = None,\n        image: Optional[\"NDArray\"] = None,\n        **input_kwargs,\n    ) -> AsyncGenerator[str, None]:\n        generated_text = \"\"\n        generator = await self._generate(messages, system, tools, image, **input_kwargs)\n        async for result in generator:\n            delta_text = result.outputs[0].text[len(generated_text) :]\n            generated_text = result.outputs[0].text\n            yield delta_text\n\n    async def get_scores(\n        self,\n        batch_input: List[str],\n        **input_kwargs,\n    ) -> List[float]:\n        raise NotImplementedError(\"vLLM engine does not support get_scores.\")\n```"
        }
    ],
    "temperature": 0
}