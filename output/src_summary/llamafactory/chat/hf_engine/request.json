{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nimport asyncio\nimport concurrent.futures\nimport os\nfrom threading import Thread\nfrom typing import TYPE_CHECKING, Any, AsyncGenerator, Callable, Dict, List, Optional, Sequence, Tuple, Union\n\nimport torch\nfrom transformers import GenerationConfig, TextIteratorStreamer\n\nfrom ..data import get_template_and_fix_tokenizer\nfrom ..extras.logging import get_logger\nfrom ..extras.misc import get_logits_processor\nfrom ..model import load_model, load_tokenizer\nfrom .base_engine import BaseEngine, Response\n\n\nif TYPE_CHECKING:\n    from numpy.typing import NDArray\n    from transformers import PreTrainedModel, PreTrainedTokenizer, ProcessorMixin\n    from transformers.image_processing_utils import BaseImageProcessor\n    from trl import PreTrainedModelWrapper\n\n    from ..data import Template\n    from ..hparams import DataArguments, FinetuningArguments, GeneratingArguments, ModelArguments\n\n\nlogger = get_logger(__name__)\n\n\nclass HuggingfaceEngine(BaseEngine):\n    def __init__(\n        self,\n        model_args: \"ModelArguments\",\n        data_args: \"DataArguments\",\n        finetuning_args: \"FinetuningArguments\",\n        generating_args: \"GeneratingArguments\",\n    ) -> None:\n        self.can_generate = finetuning_args.stage == \"sft\"\n        tokenizer_module = load_tokenizer(model_args)\n        self.tokenizer = tokenizer_module[\"tokenizer\"]\n        self.processor = tokenizer_module[\"processor\"]\n        self.tokenizer.padding_side = \"left\" if self.can_generate else \"right\"\n        self.template = get_template_and_fix_tokenizer(self.tokenizer, data_args.template, data_args.tool_format)\n        self.model = load_model(\n            self.tokenizer, model_args, finetuning_args, is_trainable=False, add_valuehead=(not self.can_generate)\n        )  \n        self.generating_args = generating_args.to_dict()\n        try:\n            asyncio.get_event_loop()\n        except RuntimeError:\n            logger.warning(\"There is no current event loop, creating a new one.\")\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n\n        self.semaphore = asyncio.Semaphore(int(os.environ.get(\"MAX_CONCURRENT\", \"1\")))\n\n    @staticmethod\n    def _process_args(\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n        processor: Optional[\"ProcessorMixin\"],\n        template: \"Template\",\n        generating_args: Dict[str, Any],\n        messages: Sequence[Dict[str, str]],\n        system: Optional[str] = None,\n        tools: Optional[str] = None,\n        image: Optional[\"NDArray\"] = None,\n        input_kwargs: Optional[Dict[str, Any]] = {},\n    ) -> Tuple[Dict[str, Any], int]:\n        if (\n            processor is not None\n            and image is not None\n            and not hasattr(processor, \"image_seq_length\")\n            and template.image_token not in messages[0][\"content\"]\n        ):  \n            messages[0][\"content\"] = template.image_token + messages[0][\"content\"]\n\n        paired_messages = messages + [{\"role\": \"assistant\", \"content\": \"\"}]\n        system = system or generating_args[\"default_system\"]\n        pixel_values = None\n        prompt_ids, _ = template.encode_oneturn(\n            tokenizer=tokenizer, messages=paired_messages, system=system, tools=tools\n        )\n        if processor is not None and image is not None:  \n            image_processor: \"BaseImageProcessor\" = getattr(processor, \"image_processor\")\n            batch_feature = image_processor(image, return_tensors=\"pt\")\n            pixel_values = batch_feature.to(model.device)[\"pixel_values\"]  \n            if hasattr(processor, \"image_seq_length\"):  \n                image_token_id = tokenizer.convert_tokens_to_ids(template.image_token)\n                prompt_ids = [image_token_id] * getattr(processor, \"image_seq_length\") + prompt_ids\n\n        prompt_length = len(prompt_ids)\n        inputs = torch.tensor([prompt_ids], device=model.device)\n        attention_mask = torch.ones_like(inputs, dtype=torch.bool)\n\n        do_sample: Optional[bool] = input_kwargs.pop(\"do_sample\", None)\n        temperature: Optional[float] = input_kwargs.pop(\"temperature\", None)\n        top_p: Optional[float] = input_kwargs.pop(\"top_p\", None)\n        top_k: Optional[float] = input_kwargs.pop(\"top_k\", None)\n        num_return_sequences: int = input_kwargs.pop(\"num_return_sequences\", 1)\n        repetition_penalty: Optional[float] = input_kwargs.pop(\"repetition_penalty\", None)\n        length_penalty: Optional[float] = input_kwargs.pop(\"length_penalty\", None)\n        max_length: Optional[int] = input_kwargs.pop(\"max_length\", None)\n        max_new_tokens: Optional[int] = input_kwargs.pop(\"max_new_tokens\", None)\n        stop: Optional[Union[str, List[str]]] = input_kwargs.pop(\"stop\", None)\n\n        if stop is not None:\n            logger.warning(\"Stop parameter is not supported by the huggingface engine yet.\")\n\n        generating_args = generating_args.copy()\n        generating_args.update(\n            dict(\n                do_sample=do_sample if do_sample is not None else generating_args[\"do_sample\"],\n                temperature=temperature if temperature is not None else generating_args[\"temperature\"],\n                top_p=top_p if top_p is not None else generating_args[\"top_p\"],\n                top_k=top_k if top_k is not None else generating_args[\"top_k\"],\n                num_return_sequences=num_return_sequences,\n                repetition_penalty=repetition_penalty\n                if repetition_penalty is not None\n                else generating_args[\"repetition_penalty\"],\n                length_penalty=length_penalty if length_penalty is not None else generating_args[\"length_penalty\"],\n                eos_token_id=[tokenizer.eos_token_id] + tokenizer.additional_special_tokens_ids,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        )\n\n        if isinstance(num_return_sequences, int) and num_return_sequences > 1:  \n            generating_args[\"do_sample\"] = True\n            generating_args[\"temperature\"] = generating_args[\"temperature\"] or 1.0\n\n        if not generating_args[\"temperature\"]:\n            generating_args[\"do_sample\"] = False\n\n        if not generating_args[\"do_sample\"]:\n            generating_args.pop(\"temperature\", None)\n            generating_args.pop(\"top_p\", None)\n\n        if max_length:\n            generating_args.pop(\"max_new_tokens\", None)\n            generating_args[\"max_length\"] = max_length\n\n        if max_new_tokens:\n            generating_args.pop(\"max_length\", None)\n            generating_args[\"max_new_tokens\"] = max_new_tokens\n\n        gen_kwargs = dict(\n            inputs=inputs,\n            attention_mask=attention_mask,\n            generation_config=GenerationConfig(**generating_args),\n            logits_processor=get_logits_processor(),\n        )\n\n        if pixel_values is not None:\n            gen_kwargs[\"pixel_values\"] = pixel_values\n\n        return gen_kwargs, prompt_length\n\n    @staticmethod\n    @torch.inference_mode()\n    def _chat(\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n        processor: Optional[\"ProcessorMixin\"],\n        template: \"Template\",\n        generating_args: Dict[str, Any],\n        messages: Sequence[Dict[str, str]],\n        system: Optional[str] = None,\n        tools: Optional[str] = None,\n        image: Optional[\"NDArray\"] = None,\n        input_kwargs: Optional[Dict[str, Any]] = {},\n    ) -> List[\"Response\"]:\n        gen_kwargs, prompt_length = HuggingfaceEngine._process_args(\n            model, tokenizer, processor, template, generating_args, messages, system, tools, image, input_kwargs\n        )\n        generate_output = model.generate(**gen_kwargs)\n        response_ids = generate_output[:, prompt_length:]\n        response = tokenizer.batch_decode(response_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n        results = []\n        for i in range(len(response)):\n            eos_index = (response_ids[i] == tokenizer.eos_token_id).nonzero()\n            response_length = (eos_index[0].item() + 1) if len(eos_index) else len(response_ids[i])\n            results.append(\n                Response(\n                    response_text=response[i],\n                    response_length=response_length,\n                    prompt_length=prompt_length,\n                    finish_reason=\"stop\" if len(eos_index) else \"length\",\n                )\n            )\n\n        return results\n\n    @staticmethod\n    @torch.inference_mode()\n    def _stream_chat(\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n        processor: Optional[\"ProcessorMixin\"],\n        template: \"Template\",\n        generating_args: Dict[str, Any],\n        messages: Sequence[Dict[str, str]],\n        system: Optional[str] = None,\n        tools: Optional[str] = None,\n        image: Optional[\"NDArray\"] = None,\n        input_kwargs: Optional[Dict[str, Any]] = {},\n    ) -> Callable[[], str]:\n        gen_kwargs, _ = HuggingfaceEngine._process_args(\n            model, tokenizer, processor, template, generating_args, messages, system, tools, image, input_kwargs\n        )\n        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n        gen_kwargs[\"streamer\"] = streamer\n        thread = Thread(target=model.generate, kwargs=gen_kwargs, daemon=True)\n        thread.start()\n\n        def stream():\n            try:\n                return streamer.__next__()\n            except StopIteration:\n                raise StopAsyncIteration()\n\n        return stream\n\n    @staticmethod\n    @torch.inference_mode()\n    def _get_scores(\n        model: \"PreTrainedModelWrapper\",\n        tokenizer: \"PreTrainedTokenizer\",\n        batch_input: List[str],\n        input_kwargs: Optional[Dict[str, Any]] = {},\n    ) -> List[float]:\n        max_length = input_kwargs.pop(\"max_length\", None)\n        device = getattr(model.pretrained_model, \"device\", \"cuda\")\n        inputs = tokenizer(\n            batch_input,\n            padding=True,\n            truncation=True,\n            max_length=max_length or getattr(model.config, \"max_position_embeddings\", 1024),\n            return_tensors=\"pt\",\n            add_special_tokens=True,\n        ).to(device)\n\n        input_ids: torch.Tensor = inputs[\"input_ids\"]\n        _, _, values = model(**inputs, output_hidden_states=True, return_dict=True)\n\n        if getattr(model.config, \"model_type\", None) == \"chatglm\":\n            values = torch.transpose(values, 0, 1)\n\n        scores = []\n        for i in range(input_ids.size(0)):\n            end_indexes = (input_ids[i] != tokenizer.pad_token_id).nonzero()\n            end_index = end_indexes[-1].item() if len(end_indexes) else 0\n            scores.append(values[i, end_index].nan_to_num().item())\n\n        return scores\n\n    async def chat(\n        self,\n        messages: Sequence[Dict[str, str]],\n        system: Optional[str] = None,\n        tools: Optional[str] = None,\n        image: Optional[\"NDArray\"] = None,\n        **input_kwargs,\n    ) -> List[\"Response\"]:\n        if not self.can_generate:\n            raise ValueError(\"The current model does not support `chat`.\")\n\n        loop = asyncio.get_running_loop()\n        input_args = (\n            self.model,\n            self.tokenizer,\n            self.processor,\n            self.template,\n            self.generating_args,\n            messages,\n            system,\n            tools,\n            image,\n            input_kwargs,\n        )\n        async with self.semaphore:\n            with concurrent.futures.ThreadPoolExecutor() as pool:\n                return await loop.run_in_executor(pool, self._chat, *input_args)\n\n    async def stream_chat(\n        self,\n        messages: Sequence[Dict[str, str]],\n        system: Optional[str] = None,\n        tools: Optional[str] = None,\n        image: Optional[\"NDArray\"] = None,\n        **input_kwargs,\n    ) -> AsyncGenerator[str, None]:\n        if not self.can_generate:\n            raise ValueError(\"The current model does not support `stream_chat`.\")\n\n        loop = asyncio.get_running_loop()\n        input_args = (\n            self.model,\n            self.tokenizer,\n            self.processor,\n            self.template,\n            self.generating_args,\n            messages,\n            system,\n            tools,\n            image,\n            input_kwargs,\n        )\n        async with self.semaphore:\n            with concurrent.futures.ThreadPoolExecutor() as pool:\n                stream = self._stream_chat(*input_args)\n                while True:\n                    try:\n                        yield await loop.run_in_executor(pool, stream)\n                    except StopAsyncIteration:\n                        break\n\n    async def get_scores(\n        self,\n        batch_input: List[str],\n        **input_kwargs,\n    ) -> List[float]:\n        if self.can_generate:\n            raise ValueError(\"Cannot get scores using an auto-regressive model.\")\n\n        loop = asyncio.get_running_loop()\n        input_args = (self.model, self.tokenizer, batch_input, input_kwargs)\n        async with self.semaphore:\n            with concurrent.futures.ThreadPoolExecutor() as pool:\n                return await loop.run_in_executor(pool, self._get_scores, *input_args)\n```"
        }
    ],
    "temperature": 0
}