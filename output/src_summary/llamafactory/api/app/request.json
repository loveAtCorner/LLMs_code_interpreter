{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nimport os\nfrom contextlib import asynccontextmanager\nfrom typing import Optional\n\nfrom typing_extensions import Annotated\n\nfrom ..chat import ChatModel\nfrom ..extras.misc import torch_gc\nfrom ..extras.packages import is_fastapi_available, is_starlette_available, is_uvicorn_available\nfrom .chat import (\n    create_chat_completion_response,\n    create_score_evaluation_response,\n    create_stream_chat_completion_response,\n)\nfrom .protocol import (\n    ChatCompletionRequest,\n    ChatCompletionResponse,\n    ModelCard,\n    ModelList,\n    ScoreEvaluationRequest,\n    ScoreEvaluationResponse,\n)\n\n\nif is_fastapi_available():\n    from fastapi import Depends, FastAPI, HTTPException, status\n    from fastapi.middleware.cors import CORSMiddleware\n    from fastapi.security.http import HTTPAuthorizationCredentials, HTTPBearer\n\n\nif is_starlette_available():\n    from sse_starlette import EventSourceResponse\n\n\nif is_uvicorn_available():\n    import uvicorn\n\n\n@asynccontextmanager\nasync def lifespan(app: \"FastAPI\"):  \n    yield\n    torch_gc()\n\n\ndef create_app(chat_model: \"ChatModel\") -> \"FastAPI\":\n    app = FastAPI(lifespan=lifespan)\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n    api_key = os.environ.get(\"API_KEY\")\n    security = HTTPBearer(auto_error=False)\n\n    async def verify_api_key(auth: Annotated[Optional[HTTPAuthorizationCredentials], Depends(security)]):\n        if api_key and (auth is None or auth.credentials != api_key):\n            raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid API key.\")\n\n    @app.get(\n        \"/v1/models\",\n        response_model=ModelList,\n        status_code=status.HTTP_200_OK,\n        dependencies=[Depends(verify_api_key)],\n    )\n    async def list_models():\n        model_card = ModelCard(id=\"gpt-3.5-turbo\")\n        return ModelList(data=[model_card])\n\n    @app.post(\n        \"/v1/chat/completions\",\n        response_model=ChatCompletionResponse,\n        status_code=status.HTTP_200_OK,\n        dependencies=[Depends(verify_api_key)],\n    )\n    async def create_chat_completion(request: ChatCompletionRequest):\n        if not chat_model.engine.can_generate:\n            raise HTTPException(status_code=status.HTTP_405_METHOD_NOT_ALLOWED, detail=\"Not allowed\")\n\n        if request.stream:\n            generate = create_stream_chat_completion_response(request, chat_model)\n            return EventSourceResponse(generate, media_type=\"text/event-stream\")\n        else:\n            return await create_chat_completion_response(request, chat_model)\n\n    @app.post(\n        \"/v1/score/evaluation\",\n        response_model=ScoreEvaluationResponse,\n        status_code=status.HTTP_200_OK,\n        dependencies=[Depends(verify_api_key)],\n    )\n    async def create_score_evaluation(request: ScoreEvaluationRequest):\n        if chat_model.engine.can_generate:\n            raise HTTPException(status_code=status.HTTP_405_METHOD_NOT_ALLOWED, detail=\"Not allowed\")\n\n        return await create_score_evaluation_response(request, chat_model)\n\n    return app\n\n\ndef run_api() -> None:\n    chat_model = ChatModel()\n    app = create_app(chat_model)\n    api_host = os.environ.get(\"API_HOST\", \"0.0.0.0\")\n    api_port = int(os.environ.get(\"API_PORT\", \"8000\"))\n    print(\"Visit http://localhost:{}/docs for API document.\".format(api_port))\n    uvicorn.run(app, host=api_host, port=api_port)\n```"
        }
    ],
    "temperature": 0
}