{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nimport json\nimport os\nfrom types import MethodType\nfrom typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n\nimport torch\nfrom transformers import Trainer\n\nfrom ...extras.logging import get_logger\nfrom ..callbacks import FixValueHeadModelCallback, PissaConvertCallback, SaveProcessorCallback\nfrom ..trainer_utils import create_custom_optimzer, create_custom_scheduler\n\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedModel, ProcessorMixin\n    from transformers.trainer import PredictionOutput\n\n    from ...hparams import FinetuningArguments\n\n\nlogger = get_logger(__name__)\n\n\nclass PairwiseTrainer(Trainer):\n    r\n\n    def __init__(\n        self, finetuning_args: \"FinetuningArguments\", processor: Optional[\"ProcessorMixin\"], **kwargs\n    ) -> None:\n        super().__init__(**kwargs)\n        self.finetuning_args = finetuning_args\n        self.can_return_loss = True  \n        self.add_callback(FixValueHeadModelCallback)\n\n        if processor is not None:\n            self.add_callback(SaveProcessorCallback(processor))\n\n        if finetuning_args.pissa_convert:\n            self.add_callback(PissaConvertCallback)\n\n        if finetuning_args.use_badam:\n            from badam import BAdamCallback, clip_grad_norm_old_version\n\n            self.accelerator.clip_grad_norm_ = MethodType(clip_grad_norm_old_version, self.accelerator)\n            self.add_callback(BAdamCallback)\n\n    def create_optimizer(self) -> \"torch.optim.Optimizer\":\n        if self.optimizer is None:\n            self.optimizer = create_custom_optimzer(self.model, self.args, self.finetuning_args)\n        return super().create_optimizer()\n\n    def create_scheduler(\n        self, num_training_steps: int, optimizer: Optional[\"torch.optim.Optimizer\"] = None\n    ) -> \"torch.optim.lr_scheduler.LRScheduler\":\n        create_custom_scheduler(self.args, num_training_steps, optimizer)\n        return super().create_scheduler(num_training_steps, optimizer)\n\n    def compute_loss(\n        self, model: \"PreTrainedModel\", inputs: Dict[str, torch.Tensor], return_outputs: bool = False\n    ) -> Union[torch.Tensor, Tuple[torch.Tensor, List[torch.Tensor]]]:\n        r\n        _, _, values = model(**inputs, output_hidden_states=True, return_dict=True, use_cache=False)\n        batch_size = inputs[\"input_ids\"].size(0) // 2\n        chosen_masks, rejected_masks = torch.split(inputs[\"attention_mask\"], batch_size, dim=0)\n        chosen_rewards, rejected_rewards = torch.split(values, batch_size, dim=0)\n        chosen_scores = chosen_rewards.gather(dim=-1, index=(chosen_masks.sum(dim=-1, keepdim=True) - 1))\n        rejected_scores = rejected_rewards.gather(dim=-1, index=(rejected_masks.sum(dim=-1, keepdim=True) - 1))\n        chosen_scores, rejected_scores = chosen_scores.squeeze(), rejected_scores.squeeze()\n\n        loss = -torch.nn.functional.logsigmoid(chosen_scores.float() - rejected_scores.float()).mean()\n        if return_outputs:\n            return loss, (loss, chosen_scores, rejected_scores)\n        else:\n            return loss\n\n    def save_predictions(self, predict_results: \"PredictionOutput\") -> None:\n        r\n        if not self.is_world_process_zero():\n            return\n\n        output_prediction_file = os.path.join(self.args.output_dir, \"generated_predictions.jsonl\")\n        logger.info(f\"Saving prediction results to {output_prediction_file}\")\n        chosen_scores, rejected_scores = predict_results.predictions\n\n        with open(output_prediction_file, \"w\", encoding=\"utf-8\") as writer:\n            res: List[str] = []\n            for c_score, r_score in zip(chosen_scores, rejected_scores):\n                res.append(json.dumps({\"chosen\": round(float(c_score), 2), \"rejected\": round(float(r_score), 2)}))\n\n            writer.write(\"\\n\".join(res))\n```"
        }
    ],
    "temperature": 0
}