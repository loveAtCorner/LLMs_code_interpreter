{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nimport warnings\nfrom collections import defaultdict\nfrom contextlib import nullcontext\nfrom types import MethodType\nfrom typing import TYPE_CHECKING, Dict, Literal, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nfrom transformers import Trainer\nfrom trl import DPOTrainer\nfrom trl.trainer import disable_dropout_in_model\n\nfrom ...extras.constants import IGNORE_INDEX\nfrom ..callbacks import PissaConvertCallback, SaveProcessorCallback\nfrom ..trainer_utils import create_custom_optimzer, create_custom_scheduler, get_batch_logps\n\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedModel, ProcessorMixin\n\n    from ...hparams import FinetuningArguments\n\n\nclass CustomDPOTrainer(DPOTrainer):\n    def __init__(\n        self,\n        model: Union[\"PreTrainedModel\", torch.nn.Module],\n        ref_model: Optional[Union[\"PreTrainedModel\", torch.nn.Module]],\n        finetuning_args: \"FinetuningArguments\",\n        processor: Optional[\"ProcessorMixin\"],\n        disable_dropout: bool = True,\n        **kwargs,\n    ):\n        if disable_dropout:\n            disable_dropout_in_model(model)\n            if ref_model is not None:\n                disable_dropout_in_model(ref_model)\n\n        self.finetuning_args = finetuning_args\n        self.reference_free = False\n        self.use_dpo_data_collator = True  \n        self.generate_during_eval = False  \n        self.label_pad_token_id = IGNORE_INDEX\n        self.padding_value = 0\n        self.is_encoder_decoder = model.config.is_encoder_decoder\n        self.precompute_ref_log_probs = False\n        self._precomputed_train_ref_log_probs = False\n        self._precomputed_eval_ref_log_probs = False\n        self._peft_has_been_casted_to_bf16 = False\n\n        self.ref_model = ref_model\n        self._stored_metrics = defaultdict(lambda: defaultdict(list))\n\n        \n        self.beta = finetuning_args.pref_beta\n        self.loss_type = finetuning_args.pref_loss\n        self.ftx_gamma = finetuning_args.pref_ftx\n        self.label_smoothing = finetuning_args.dpo_label_smoothing\n        self.simpo_gamma = finetuning_args.simpo_gamma\n\n        Trainer.__init__(self, model=model, **kwargs)\n        if not hasattr(self, \"accelerator\"):\n            raise AttributeError(\"Please update `transformers`.\")\n\n        warnings.simplefilter(\"ignore\")  \n\n        if ref_model is not None:\n            if self.is_deepspeed_enabled:\n                if not (\n                    getattr(ref_model, \"is_loaded_in_8bit\", False) or getattr(ref_model, \"is_loaded_in_4bit\", False)\n                ):  \n                    self.ref_model = self._prepare_deepspeed(self.ref_model)\n            else:\n                self.ref_model = self.accelerator.prepare_model(self.ref_model, evaluation_mode=True)\n                self.ref_model.eval()\n\n        if processor is not None:\n            self.add_callback(SaveProcessorCallback(processor))\n\n        if finetuning_args.pissa_convert:\n            self.callback_handler.add_callback(PissaConvertCallback)\n\n        if finetuning_args.use_badam:\n            from badam import BAdamCallback, clip_grad_norm_old_version\n\n            self.accelerator.clip_grad_norm_ = MethodType(clip_grad_norm_old_version, self.accelerator)\n            self.add_callback(BAdamCallback)\n\n    def create_optimizer(self) -> \"torch.optim.Optimizer\":\n        if self.optimizer is None:\n            self.optimizer = create_custom_optimzer(self.model, self.args, self.finetuning_args)\n        return super().create_optimizer()\n\n    def create_scheduler(\n        self, num_training_steps: int, optimizer: Optional[\"torch.optim.Optimizer\"] = None\n    ) -> \"torch.optim.lr_scheduler.LRScheduler\":\n        create_custom_scheduler(self.args, num_training_steps, optimizer)\n        return super().create_scheduler(num_training_steps, optimizer)\n\n    def odds_ratio_loss(self, chosen_logps: \"torch.Tensor\", rejected_logps: \"torch.Tensor\") -> \"torch.Tensor\":\n        r\n        log_odds = (chosen_logps - rejected_logps) - (\n            torch.log1p(-torch.exp(chosen_logps)) - torch.log1p(-torch.exp(rejected_logps))\n        )\n        sft_loss = -chosen_logps\n        odds_ratio_loss = -F.logsigmoid(log_odds)\n        orpo_loss = sft_loss + self.beta * odds_ratio_loss\n        return orpo_loss\n\n    def simpo_loss(self, chosen_logps: \"torch.Tensor\", rejected_logps: \"torch.Tensor\") -> \"torch.Tensor\":\n        r\n        pi_logratios = chosen_logps - rejected_logps\n        gamma_logratios = self.simpo_gamma / self.beta\n        logits = pi_logratios - gamma_logratios\n        simpo_loss = -F.logsigmoid(self.beta * logits)\n        return simpo_loss\n\n    def compute_preference_loss(\n        self,\n        policy_chosen_logps: \"torch.Tensor\",\n        policy_rejected_logps: \"torch.Tensor\",\n        reference_chosen_logps: Optional[\"torch.Tensor\"],\n        reference_rejected_logps: Optional[\"torch.Tensor\"],\n    ) -> Tuple[\"torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\"]:\n        r\n        if not self.finetuning_args.use_ref_model:\n            if self.loss_type == \"orpo\":\n                losses = self.odds_ratio_loss(policy_chosen_logps, policy_rejected_logps)\n            elif self.loss_type == \"simpo\":\n                losses = self.simpo_loss(policy_chosen_logps, policy_rejected_logps)\n            else:\n                raise NotImplementedError(\"Unknown loss type: {}.\".format(self.loss_type))\n\n            chosen_rewards = self.beta * policy_chosen_logps.to(self.accelerator.device).detach()\n            rejected_rewards = self.beta * policy_rejected_logps.to(self.accelerator.device).detach()\n        else:\n            losses, chosen_rewards, rejected_rewards = self.dpo_loss(\n                policy_chosen_logps, policy_rejected_logps, reference_chosen_logps, reference_rejected_logps\n            )\n\n        return losses, chosen_rewards, rejected_rewards\n\n    def concatenated_forward(\n        self, model: \"PreTrainedModel\", batch: Dict[str, \"torch.Tensor\"]\n    ) -> Tuple[\"torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\"]:\n        r\n        if self.finetuning_args.use_ref_model:\n            batch = {k: v.detach().clone() for k, v in batch.items()}  \n\n        all_logits: \"torch.Tensor\" = model(**batch, return_dict=True, use_cache=False).logits.to(torch.float32)\n\n        all_logps, valid_length = get_batch_logps(logits=all_logits, labels=batch[\"labels\"])\n        if self.loss_type in [\"ipo\", \"orpo\", \"simpo\"]:\n            all_logps = all_logps / valid_length\n\n        batch_size = batch[\"input_ids\"].size(0) // 2\n        chosen_logps, rejected_logps = all_logps.split(batch_size, dim=0)\n        chosen_logits, rejected_logits = all_logits.split(batch_size, dim=0)\n        chosen_length, _ = valid_length.split(batch_size, dim=0)\n        return chosen_logps, rejected_logps, chosen_logits, rejected_logits, chosen_logps / chosen_length\n\n    def compute_reference_log_probs(\n        self, model: \"PreTrainedModel\", batch: Dict[str, \"torch.Tensor\"]\n    ) -> Tuple[Optional[\"torch.Tensor\"], Optional[\"torch.Tensor\"]]:\n        r\n        if not self.finetuning_args.use_ref_model:\n            return None, None\n\n        if self.ref_model is None:\n            ref_model = model\n            ref_context = self.accelerator.unwrap_model(model).disable_adapter()\n        else:\n            ref_model = self.ref_model\n            ref_context = nullcontext()\n\n        with torch.no_grad(), ref_context:\n            reference_chosen_logps, reference_rejected_logps, *_ = self.concatenated_forward(ref_model, batch)\n\n        return reference_chosen_logps, reference_rejected_logps\n\n    def get_batch_loss_metrics(\n        self,\n        model: \"PreTrainedModel\",\n        batch: Dict[str, \"torch.Tensor\"],\n        train_eval: Literal[\"train\", \"eval\"] = \"train\",\n    ) -> Tuple[\"torch.Tensor\", Dict[str, \"torch.Tensor\"]]:\n        r\n        metrics = {}\n        (\n            policy_chosen_logps,\n            policy_rejected_logps,\n            policy_chosen_logits,\n            policy_rejected_logits,\n            policy_chosen_logps_avg,\n        ) = self.concatenated_forward(model, batch)\n\n        reference_chosen_logps, reference_rejected_logps = self.compute_reference_log_probs(model, batch)\n        losses, chosen_rewards, rejected_rewards = self.compute_preference_loss(\n            policy_chosen_logps,\n            policy_rejected_logps,\n            reference_chosen_logps,\n            reference_rejected_logps,\n        )\n        sft_loss = -policy_chosen_logps_avg\n        if self.ftx_gamma > 1e-6:\n            losses += self.ftx_gamma * sft_loss\n\n        reward_accuracies = (chosen_rewards > rejected_rewards).float()\n\n        prefix = \"eval_\" if train_eval == \"eval\" else \"\"\n        metrics[\"{}rewards/chosen\".format(prefix)] = chosen_rewards.mean().cpu()\n        metrics[\"{}rewards/rejected\".format(prefix)] = rejected_rewards.mean().cpu()\n        metrics[\"{}rewards/accuracies\".format(prefix)] = reward_accuracies.mean().cpu()\n        metrics[\"{}rewards/margins\".format(prefix)] = (chosen_rewards - rejected_rewards).mean().cpu()\n        metrics[\"{}logps/rejected\".format(prefix)] = policy_rejected_logps.detach().mean().cpu()\n        metrics[\"{}logps/chosen\".format(prefix)] = policy_chosen_logps.detach().mean().cpu()\n        metrics[\"{}logits/rejected\".format(prefix)] = policy_rejected_logits.detach().mean().cpu()\n        metrics[\"{}logits/chosen\".format(prefix)] = policy_chosen_logits.detach().mean().cpu()\n        if self.loss_type == \"orpo\":\n            metrics[\"{}sft_loss\".format(prefix)] = sft_loss.detach().mean().cpu()\n            metrics[\"{}odds_ratio_loss\".format(prefix)] = ((losses - sft_loss) / self.beta).detach().mean().cpu()\n\n        return losses.mean(), metrics\n```"
        }
    ],
    "temperature": 0
}