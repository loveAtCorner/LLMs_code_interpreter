{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nimport os\nimport shutil\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional\n\nimport torch\nfrom transformers import PreTrainedModel\n\nfrom ..data import get_template_and_fix_tokenizer\nfrom ..extras.constants import V_HEAD_SAFE_WEIGHTS_NAME, V_HEAD_WEIGHTS_NAME\nfrom ..extras.logging import get_logger\nfrom ..hparams import get_infer_args, get_train_args\nfrom ..model import load_model, load_tokenizer\nfrom .callbacks import LogCallback\nfrom .dpo import run_dpo\nfrom .kto import run_kto\nfrom .ppo import run_ppo\nfrom .pt import run_pt\nfrom .rm import run_rm\nfrom .sft import run_sft\n\n\nif TYPE_CHECKING:\n    from transformers import TrainerCallback\n\n\nlogger = get_logger(__name__)\n\n\ndef run_exp(args: Optional[Dict[str, Any]] = None, callbacks: List[\"TrainerCallback\"] = []) -> None:\n    callbacks.append(LogCallback())\n    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n\n    if finetuning_args.stage == \"pt\":\n        run_pt(model_args, data_args, training_args, finetuning_args, callbacks)\n    elif finetuning_args.stage == \"sft\":\n        run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n    elif finetuning_args.stage == \"rm\":\n        run_rm(model_args, data_args, training_args, finetuning_args, callbacks)\n    elif finetuning_args.stage == \"ppo\":\n        run_ppo(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n    elif finetuning_args.stage == \"dpo\":\n        run_dpo(model_args, data_args, training_args, finetuning_args, callbacks)\n    elif finetuning_args.stage == \"kto\":\n        run_kto(model_args, data_args, training_args, finetuning_args, callbacks)\n    else:\n        raise ValueError(\"Unknown task: {}.\".format(finetuning_args.stage))\n\n\ndef export_model(args: Optional[Dict[str, Any]] = None) -> None:\n    model_args, data_args, finetuning_args, _ = get_infer_args(args)\n\n    if model_args.export_dir is None:\n        raise ValueError(\"Please specify `export_dir` to save model.\")\n\n    if model_args.adapter_name_or_path is not None and model_args.export_quantization_bit is not None:\n        raise ValueError(\"Please merge adapters before quantizing the model.\")\n\n    tokenizer_module = load_tokenizer(model_args)\n    tokenizer = tokenizer_module[\"tokenizer\"]\n    processor = tokenizer_module[\"processor\"]\n    get_template_and_fix_tokenizer(tokenizer, data_args.template)\n    model = load_model(tokenizer, model_args, finetuning_args)  \n\n    if getattr(model, \"quantization_method\", None) and model_args.adapter_name_or_path is not None:\n        raise ValueError(\"Cannot merge adapters to a quantized model.\")\n\n    if not isinstance(model, PreTrainedModel):\n        raise ValueError(\"The model is not a `PreTrainedModel`, export aborted.\")\n\n    if getattr(model, \"quantization_method\", None) is None:  \n        output_dtype = getattr(model.config, \"torch_dtype\", torch.float16)\n        setattr(model.config, \"torch_dtype\", output_dtype)\n        model = model.to(output_dtype)\n    else:\n        setattr(model.config, \"torch_dtype\", torch.float16)\n\n    model.save_pretrained(\n        save_directory=model_args.export_dir,\n        max_shard_size=\"{}GB\".format(model_args.export_size),\n        safe_serialization=(not model_args.export_legacy_format),\n    )\n    if model_args.export_hub_model_id is not None:\n        model.push_to_hub(\n            model_args.export_hub_model_id,\n            token=model_args.hf_hub_token,\n            max_shard_size=\"{}GB\".format(model_args.export_size),\n            safe_serialization=(not model_args.export_legacy_format),\n        )\n\n    if finetuning_args.stage == \"rm\":\n        if model_args.adapter_name_or_path is not None:\n            vhead_path = model_args.adapter_name_or_path[-1]\n        else:\n            vhead_path = model_args.model_name_or_path\n\n        if os.path.exists(os.path.join(vhead_path, V_HEAD_SAFE_WEIGHTS_NAME)):\n            shutil.copy(\n                os.path.join(vhead_path, V_HEAD_SAFE_WEIGHTS_NAME),\n                os.path.join(model_args.export_dir, V_HEAD_SAFE_WEIGHTS_NAME),\n            )\n            logger.info(\"Copied valuehead to {}.\".format(model_args.export_dir))\n        elif os.path.exists(os.path.join(vhead_path, V_HEAD_WEIGHTS_NAME)):\n            shutil.copy(\n                os.path.join(vhead_path, V_HEAD_WEIGHTS_NAME),\n                os.path.join(model_args.export_dir, V_HEAD_WEIGHTS_NAME),\n            )\n            logger.info(\"Copied valuehead to {}.\".format(model_args.export_dir))\n\n    try:\n        tokenizer.padding_side = \"left\"  \n        tokenizer.init_kwargs[\"padding_side\"] = \"left\"\n        tokenizer.save_pretrained(model_args.export_dir)\n        if model_args.export_hub_model_id is not None:\n            tokenizer.push_to_hub(model_args.export_hub_model_id, token=model_args.hf_hub_token)\n\n        if model_args.visual_inputs and processor is not None:\n            getattr(processor, \"image_processor\").save_pretrained(model_args.export_dir)\n            if model_args.export_hub_model_id is not None:\n                getattr(processor, \"image_processor\").push_to_hub(\n                    model_args.export_hub_model_id, token=model_args.hf_hub_token\n                )\n\n    except Exception:\n        logger.warning(\"Cannot save tokenizer, please copy the files manually.\")\n```"
        }
    ],
    "temperature": 0
}