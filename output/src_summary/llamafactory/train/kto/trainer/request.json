{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nimport warnings\nfrom collections import defaultdict\nfrom contextlib import nullcontext\nfrom types import MethodType\nfrom typing import TYPE_CHECKING, Dict, Literal, Optional, Tuple, Union\n\nimport torch\nfrom transformers import Trainer\nfrom trl import KTOTrainer\nfrom trl.trainer import disable_dropout_in_model\n\nfrom ...extras.constants import IGNORE_INDEX\nfrom ..callbacks import SaveProcessorCallback\nfrom ..trainer_utils import create_custom_optimzer, create_custom_scheduler, get_batch_logps\n\n\nif TYPE_CHECKING:\n    import torch.utils.data\n    from transformers import PreTrainedModel, ProcessorMixin\n\n    from ...hparams import FinetuningArguments\n\n\nclass CustomKTOTrainer(KTOTrainer):\n    def __init__(\n        self,\n        model: Union[\"PreTrainedModel\", torch.nn.Module],\n        ref_model: Optional[Union[\"PreTrainedModel\", torch.nn.Module]],\n        finetuning_args: \"FinetuningArguments\",\n        processor: Optional[\"ProcessorMixin\"],\n        disable_dropout: bool = True,\n        **kwargs,\n    ):\n        if disable_dropout:\n            disable_dropout_in_model(model)\n            if ref_model is not None:\n                disable_dropout_in_model(ref_model)\n\n        self.finetuning_args = finetuning_args\n        self.reference_free = False\n        self.use_dpo_data_collator = True  \n        self.generate_during_eval = False  \n        self.label_pad_token_id = IGNORE_INDEX\n        self.padding_value = 0\n        self.is_encoder_decoder = model.config.is_encoder_decoder\n        self.precompute_ref_log_probs = False\n        self._precomputed_train_ref_log_probs = False\n        self._precomputed_eval_ref_log_probs = False\n        self._peft_has_been_casted_to_bf16 = False\n\n        self.ref_model = ref_model\n        self._stored_metrics = defaultdict(lambda: defaultdict(list))\n\n        \n        self.beta = finetuning_args.pref_beta\n        self.desirable_weight = finetuning_args.kto_chosen_weight\n        self.undesirable_weight = finetuning_args.kto_rejected_weight\n        self.ftx_gamma = finetuning_args.pref_ftx\n\n        Trainer.__init__(self, model=model, **kwargs)\n        if not hasattr(self, \"accelerator\"):\n            raise AttributeError(\"Please update `transformers`.\")\n\n        warnings.simplefilter(\"ignore\")  \n\n        if ref_model is not None:\n            if self.is_deepspeed_enabled:\n                if not (\n                    getattr(ref_model, \"is_loaded_in_8bit\", False) or getattr(ref_model, \"is_loaded_in_4bit\", False)\n                ):  \n                    self.ref_model = self._prepare_deepspeed(self.ref_model)\n            else:\n                self.ref_model = self.accelerator.prepare_model(self.ref_model, evaluation_mode=True)\n                self.ref_model.eval()\n\n        if processor is not None:\n            self.add_callback(SaveProcessorCallback(processor))\n\n        if finetuning_args.use_badam:\n            from badam import BAdamCallback, clip_grad_norm_old_version\n\n            self.accelerator.clip_grad_norm_ = MethodType(clip_grad_norm_old_version, self.accelerator)\n            self.add_callback(BAdamCallback)\n\n    def create_optimizer(self) -> \"torch.optim.Optimizer\":\n        if self.optimizer is None:\n            self.optimizer = create_custom_optimzer(self.model, self.args, self.finetuning_args)\n        return super().create_optimizer()\n\n    def create_scheduler(\n        self, num_training_steps: int, optimizer: Optional[\"torch.optim.Optimizer\"] = None\n    ) -> \"torch.optim.lr_scheduler.LRScheduler\":\n        create_custom_scheduler(self.args, num_training_steps, optimizer)\n        return super().create_scheduler(num_training_steps, optimizer)\n\n    def _get_train_sampler(self) -> Optional[\"torch.utils.data.Sampler\"]:\n        r\n        return Trainer._get_train_sampler(self)\n\n    def forward(\n        self, model: \"PreTrainedModel\", batch: Dict[str, \"torch.Tensor\"], prefix: Literal[\"\", \"kl_\"] = \"\"\n    ) -> Tuple[\"torch.Tensor\", \"torch.Tensor\"]:\n        r\n        batch = {k: v.detach().clone() for k, v in batch.items()}  \n        model_inputs = {\n            \"input_ids\": batch[\"{}input_ids\".format(prefix)],\n            \"attention_mask\": batch[\"{}attention_mask\".format(prefix)],\n        }\n        if \"pixel_values\" in batch:\n            model_inputs[\"pixel_values\"] = batch[\"pixel_values\"]\n\n        if \"{}token_type_ids\".format(prefix) in batch:\n            model_inputs[\"token_type_ids\"] = batch[\"{}token_type_ids\".format(prefix)]\n\n        logits = model(**model_inputs, return_dict=True, use_cache=False).logits.to(torch.float32)\n\n        logps, valid_length = get_batch_logps(logits=logits, labels=batch[\"{}labels\".format(prefix)])\n        return logps, logps / valid_length\n\n    def concatenated_forward(\n        self, model: \"PreTrainedModel\", batch: Dict[str, \"torch.Tensor\"]\n    ) -> Tuple[\"torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\"]:\n        target_logps, target_logps_avg = self.forward(model, batch)\n        with torch.no_grad():\n            kl_logps, _ = self.forward(model, batch, prefix=\"kl_\")\n\n        if len(target_logps) != len(batch[\"kto_tags\"]):\n            raise ValueError(\"Mismatched shape of inputs and labels.\")\n\n        chosen_logps = target_logps[batch[\"kto_tags\"]]\n        rejected_logps = target_logps[~batch[\"kto_tags\"]]\n        chosen_logps_avg = target_logps_avg[batch[\"kto_tags\"]]\n        return chosen_logps, rejected_logps, kl_logps, chosen_logps_avg\n\n    def compute_reference_log_probs(\n        self, model: \"PreTrainedModel\", batch: Dict[str, \"torch.Tensor\"]\n    ) -> Tuple[\"torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\"]:\n        r\n        if self.ref_model is None:\n            ref_model = model\n            ref_context = self.accelerator.unwrap_model(model).disable_adapter()\n        else:\n            ref_model = self.ref_model\n            ref_context = nullcontext()\n\n        with torch.no_grad(), ref_context:\n            reference_chosen_logps, reference_rejected_logps, reference_kl_logps, _ = self.concatenated_forward(\n                ref_model, batch\n            )\n\n        return reference_chosen_logps, reference_rejected_logps, reference_kl_logps\n\n    def get_batch_loss_metrics(\n        self,\n        model: \"PreTrainedModel\",\n        batch: Dict[str, \"torch.Tensor\"],\n    ) -> Tuple[\"torch.Tensor\", Dict[str, \"torch.Tensor\"]]:\n        r\n        metrics = {}\n        policy_chosen_logps, policy_rejected_logps, policy_kl_logps, policy_chosen_logps_avg = (\n            self.concatenated_forward(model, batch)\n        )\n        reference_chosen_logps, reference_rejected_logps, reference_kl_logps = self.compute_reference_log_probs(\n            model, batch\n        )\n        losses, chosen_rewards, rejected_rewards, kl = self.kto_loss(\n            policy_chosen_logps,\n            policy_rejected_logps,\n            policy_kl_logps,\n            reference_chosen_logps,\n            reference_rejected_logps,\n            reference_kl_logps,\n        )\n        losses = losses.nanmean()\n\n        if self.ftx_gamma > 1e-6 and len(policy_chosen_logps) > 0:  \n            sft_loss = -policy_chosen_logps_avg\n            losses += self.ftx_gamma * sft_loss.nanmean() / len(policy_chosen_logps) * len(batch[\"labels\"])\n\n        num_chosen = torch.Tensor([len(chosen_rewards)]).to(self.accelerator.device)\n        num_rejected = torch.Tensor([len(rejected_rewards)]).to(self.accelerator.device)\n\n        all_num_chosen = self.accelerator.gather(num_chosen).sum().item()\n        all_num_rejected = self.accelerator.gather(num_rejected).sum().item()\n\n        if all_num_chosen > 0:\n            metrics[\"rewards/chosen_sum\"] = self.accelerator.gather(chosen_rewards.nansum()).nansum().item()\n            metrics[\"logps/chosen_sum\"] = self.accelerator.gather(policy_chosen_logps.nansum()).nansum().item()\n            metrics[\"count/chosen\"] = all_num_chosen\n\n        if all_num_rejected > 0:\n            metrics[\"rewards/rejected_sum\"] = self.accelerator.gather(rejected_rewards.nansum()).nansum().item()\n            metrics[\"logps/rejected_sum\"] = self.accelerator.gather(policy_rejected_logps.nansum()).nansum().item()\n            metrics[\"count/rejected\"] = all_num_rejected\n\n        metrics[\"kl\"] = kl.item()\n\n        return losses, metrics\n```"
        }
    ],
    "temperature": 0
}