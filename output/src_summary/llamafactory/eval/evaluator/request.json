{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nimport inspect\nimport json\nimport os\nfrom typing import Any, Dict, List, Optional\n\nimport numpy as np\nimport torch\nfrom datasets import load_dataset\nfrom tqdm import tqdm, trange\nfrom transformers.utils import cached_file\n\nfrom ..data import get_template_and_fix_tokenizer\nfrom ..extras.constants import CHOICES, SUBJECTS\nfrom ..hparams import get_eval_args\nfrom ..model import load_model, load_tokenizer\nfrom .template import get_eval_template\n\n\nclass Evaluator:\n    def __init__(self, args: Optional[Dict[str, Any]] = None) -> None:\n        self.model_args, self.data_args, self.eval_args, finetuning_args = get_eval_args(args)\n        self.tokenizer = load_tokenizer(self.model_args)[\"tokenizer\"]\n        self.tokenizer.padding_side = \"right\"  \n        self.template = get_template_and_fix_tokenizer(self.tokenizer, self.data_args.template)\n        self.model = load_model(self.tokenizer, self.model_args, finetuning_args)\n        self.eval_template = get_eval_template(self.eval_args.lang)\n        self.choice_inputs = [self.tokenizer.encode(ch, add_special_tokens=False)[-1] for ch in CHOICES]\n\n    @torch.inference_mode()\n    def batch_inference(self, batch_input: Dict[str, torch.Tensor]) -> List[str]:\n        logits = self.model(**batch_input).logits\n        lengths = torch.sum(batch_input[\"attention_mask\"], dim=-1)\n        word_probs = torch.stack([logits[i, lengths[i] - 1] for i in range(len(lengths))], dim=0)\n        choice_probs = torch.nn.functional.softmax(word_probs[:, self.choice_inputs], dim=-1).detach()\n        return [chr(ord(\"A\") + offset.item()) for offset in torch.argmax(choice_probs, dim=-1)]\n\n    def eval(self) -> None:\n        mapping = cached_file(\n            path_or_repo_id=os.path.join(self.eval_args.task_dir, self.eval_args.task),\n            filename=\"mapping.json\",\n            cache_dir=self.model_args.cache_dir,\n            token=self.model_args.hf_hub_token,\n        )\n\n        with open(mapping, \"r\", encoding=\"utf-8\") as f:\n            categorys: Dict[str, Dict[str, str]] = json.load(f)\n\n        category_corrects = {subj: np.array([], dtype=\"bool\") for subj in SUBJECTS}\n        pbar = tqdm(categorys.keys(), desc=\"Processing subjects\", position=0)\n        results = {}\n        for subject in pbar:\n            if \"trust_remote_code\" in inspect.signature(load_dataset).parameters:  \n                kwargs = {\"trust_remote_code\": True}\n            else:\n                kwargs = {}\n\n            dataset = load_dataset(\n                path=os.path.join(self.eval_args.task_dir, self.eval_args.task),\n                name=subject,\n                cache_dir=self.model_args.cache_dir,\n                download_mode=self.eval_args.download_mode,\n                token=self.model_args.hf_hub_token,\n                **kwargs,\n            )\n            pbar.set_postfix_str(categorys[subject][\"name\"])\n            inputs, outputs, labels = [], [], []\n            for i in trange(len(dataset[self.data_args.split]), desc=\"Formatting batches\", position=1, leave=False):\n                support_set = (\n                    dataset[\"train\"].shuffle().select(range(min(self.eval_args.n_shot, len(dataset[\"train\"]))))\n                )\n                messages = self.eval_template.format_example(\n                    target_data=dataset[self.data_args.split][i],\n                    support_set=support_set,\n                    subject_name=categorys[subject][\"name\"],\n                )\n\n                input_ids, _ = self.template.encode_oneturn(tokenizer=self.tokenizer, messages=messages)\n                inputs.append({\"input_ids\": input_ids, \"attention_mask\": [1] * len(input_ids)})\n                labels.append(messages[-1][\"content\"])\n\n            for i in trange(\n                0, len(inputs), self.eval_args.batch_size, desc=\"Predicting batches\", position=1, leave=False\n            ):\n                batch_input = self.tokenizer.pad(\n                    inputs[i : i + self.eval_args.batch_size], return_attention_mask=True, return_tensors=\"pt\"\n                ).to(self.model.device)\n                preds = self.batch_inference(batch_input)\n                outputs += preds\n\n            corrects = np.array(outputs) == np.array(labels)\n            category_name = categorys[subject][\"category\"]\n            category_corrects[category_name] = np.concatenate([category_corrects[category_name], corrects], axis=0)\n            category_corrects[\"Average\"] = np.concatenate([category_corrects[\"Average\"], corrects], axis=0)\n            results[subject] = {str(i): outputs[i] for i in range(len(outputs))}\n\n        pbar.close()\n        self._save_results(category_corrects, results)\n\n    def _save_results(self, category_corrects: Dict[str, np.ndarray], results: Dict[str, Dict[int, str]]) -> None:\n        score_info = \"\\n\".join(\n            [\n                \"{:>15}: {:.2f}\".format(category_name, 100 * np.mean(category_correct))\n                for category_name, category_correct in category_corrects.items()\n                if len(category_correct)\n            ]\n        )\n        print(score_info)\n        if self.eval_args.save_dir is not None:\n            os.makedirs(self.eval_args.save_dir, exist_ok=False)\n            with open(os.path.join(self.eval_args.save_dir, \"results.json\"), \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n                json.dump(results, f, indent=2)\n\n            with open(os.path.join(self.eval_args.save_dir, \"results.log\"), \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n                f.write(score_info)\n\n\ndef run_eval() -> None:\n    Evaluator().eval()\n```"
        }
    ],
    "temperature": 0
}