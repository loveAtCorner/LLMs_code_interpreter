{
    "model": "qwen1.5-32b-chat-int4",
    "messages": [
        {
            "role": "system",
            "content": "你是python源码结构和逻辑的解析专家，非常擅长剖析核心代码的功能"
        },
        {
            "role": "user",
            "content": "请概括下面这个脚本的核心功能，然后该功能是否和大语言模型的指令精调任务有关```python\nimport logging\nimport os\nimport sys\nfrom typing import Any, Dict, Optional, Tuple\n\nimport torch\nimport transformers\nfrom transformers import HfArgumentParser, Seq2SeqTrainingArguments\nfrom transformers.integrations import is_deepspeed_zero3_enabled\nfrom transformers.trainer_utils import get_last_checkpoint\nfrom transformers.training_args import ParallelMode\nfrom transformers.utils import is_torch_bf16_gpu_available\nfrom transformers.utils.versions import require_version\n\nfrom ..extras.constants import CHECKPOINT_NAMES\nfrom ..extras.logging import get_logger\nfrom ..extras.misc import check_dependencies, get_current_device\nfrom .data_args import DataArguments\nfrom .evaluation_args import EvaluationArguments\nfrom .finetuning_args import FinetuningArguments\nfrom .generating_args import GeneratingArguments\nfrom .model_args import ModelArguments\n\n\nlogger = get_logger(__name__)\n\n\ncheck_dependencies()\n\n\n_TRAIN_ARGS = [ModelArguments, DataArguments, Seq2SeqTrainingArguments, FinetuningArguments, GeneratingArguments]\n_TRAIN_CLS = Tuple[ModelArguments, DataArguments, Seq2SeqTrainingArguments, FinetuningArguments, GeneratingArguments]\n_INFER_ARGS = [ModelArguments, DataArguments, FinetuningArguments, GeneratingArguments]\n_INFER_CLS = Tuple[ModelArguments, DataArguments, FinetuningArguments, GeneratingArguments]\n_EVAL_ARGS = [ModelArguments, DataArguments, EvaluationArguments, FinetuningArguments]\n_EVAL_CLS = Tuple[ModelArguments, DataArguments, EvaluationArguments, FinetuningArguments]\n\n\ndef _parse_args(parser: \"HfArgumentParser\", args: Optional[Dict[str, Any]] = None) -> Tuple[Any]:\n    if args is not None:\n        return parser.parse_dict(args)\n\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".yaml\"):\n        return parser.parse_yaml_file(os.path.abspath(sys.argv[1]))\n\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        return parser.parse_json_file(os.path.abspath(sys.argv[1]))\n\n    (*parsed_args, unknown_args) = parser.parse_args_into_dataclasses(return_remaining_strings=True)\n\n    if unknown_args:\n        print(parser.format_help())\n        print(\"Got unknown args, potentially deprecated arguments: {}\".format(unknown_args))\n        raise ValueError(\"Some specified arguments are not used by the HfArgumentParser: {}\".format(unknown_args))\n\n    return (*parsed_args,)\n\n\ndef _set_transformers_logging(log_level: Optional[int] = logging.INFO) -> None:\n    transformers.utils.logging.set_verbosity(log_level)\n    transformers.utils.logging.enable_default_handler()\n    transformers.utils.logging.enable_explicit_format()\n\n\ndef _verify_model_args(model_args: \"ModelArguments\", finetuning_args: \"FinetuningArguments\") -> None:\n    if model_args.adapter_name_or_path is not None and finetuning_args.finetuning_type != \"lora\":\n        raise ValueError(\"Adapter is only valid for the LoRA method.\")\n\n    if model_args.quantization_bit is not None:\n        if finetuning_args.finetuning_type != \"lora\":\n            raise ValueError(\"Quantization is only compatible with the LoRA method.\")\n\n        if finetuning_args.pissa_init:\n            raise ValueError(\"Please use scripts/pissa_init.py to initialize PiSSA for a quantized model.\")\n\n        if model_args.resize_vocab:\n            raise ValueError(\"Cannot resize embedding layers of a quantized model.\")\n\n        if model_args.adapter_name_or_path is not None and finetuning_args.create_new_adapter:\n            raise ValueError(\"Cannot create new adapter upon a quantized model.\")\n\n        if model_args.adapter_name_or_path is not None and len(model_args.adapter_name_or_path) != 1:\n            raise ValueError(\"Quantized model only accepts a single adapter. Merge them first.\")\n\n\ndef _check_extra_dependencies(\n    model_args: \"ModelArguments\",\n    finetuning_args: \"FinetuningArguments\",\n    training_args: Optional[\"Seq2SeqTrainingArguments\"] = None,\n) -> None:\n    if model_args.use_unsloth:\n        require_version(\"unsloth\", \"Please install unsloth: https://github.com/unslothai/unsloth\")\n\n    if model_args.mixture_of_depths is not None:\n        require_version(\"mixture-of-depth>=1.1.6\", \"To fix: pip install mixture-of-depth>=1.1.6\")\n\n    if model_args.infer_backend == \"vllm\":\n        require_version(\"vllm>=0.4.3\", \"To fix: pip install vllm>=0.4.3\")\n\n    if finetuning_args.use_galore:\n        require_version(\"galore_torch\", \"To fix: pip install galore_torch\")\n\n    if finetuning_args.use_badam:\n        require_version(\"badam>=1.2.1\", \"To fix: pip install badam>=1.2.1\")\n\n    if finetuning_args.plot_loss:\n        require_version(\"matplotlib\", \"To fix: pip install matplotlib\")\n\n    if training_args is not None and training_args.predict_with_generate:\n        require_version(\"jieba\", \"To fix: pip install jieba\")\n        require_version(\"nltk\", \"To fix: pip install nltk\")\n        require_version(\"rouge_chinese\", \"To fix: pip install rouge-chinese\")\n\n\ndef _parse_train_args(args: Optional[Dict[str, Any]] = None) -> _TRAIN_CLS:\n    parser = HfArgumentParser(_TRAIN_ARGS)\n    return _parse_args(parser, args)\n\n\ndef _parse_infer_args(args: Optional[Dict[str, Any]] = None) -> _INFER_CLS:\n    parser = HfArgumentParser(_INFER_ARGS)\n    return _parse_args(parser, args)\n\n\ndef _parse_eval_args(args: Optional[Dict[str, Any]] = None) -> _EVAL_CLS:\n    parser = HfArgumentParser(_EVAL_ARGS)\n    return _parse_args(parser, args)\n\n\ndef get_train_args(args: Optional[Dict[str, Any]] = None) -> _TRAIN_CLS:\n    model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n\n    \n    if training_args.should_log:\n        _set_transformers_logging()\n\n    \n    if finetuning_args.stage != \"pt\" and data_args.template is None:\n        raise ValueError(\"Please specify which `template` to use.\")\n\n    if finetuning_args.stage != \"sft\" and training_args.predict_with_generate:\n        raise ValueError(\"`predict_with_generate` cannot be set as True except SFT.\")\n\n    if finetuning_args.stage != \"sft\" and data_args.neat_packing:\n        raise ValueError(\"`neat_packing` cannot be set as True except SFT.\")\n\n    if finetuning_args.stage == \"sft\" and training_args.do_predict and not training_args.predict_with_generate:\n        raise ValueError(\"Please enable `predict_with_generate` to save model predictions.\")\n\n    if finetuning_args.stage in [\"rm\", \"ppo\"] and training_args.load_best_model_at_end:\n        raise ValueError(\"RM and PPO stages do not support `load_best_model_at_end`.\")\n\n    if finetuning_args.stage == \"ppo\" and not training_args.do_train:\n        raise ValueError(\"PPO training does not support evaluation, use the SFT stage to evaluate models.\")\n\n    if finetuning_args.stage == \"ppo\" and model_args.shift_attn:\n        raise ValueError(\"PPO training is incompatible with S^2-Attn.\")\n\n    if finetuning_args.stage == \"ppo\" and finetuning_args.reward_model_type == \"lora\" and model_args.use_unsloth:\n        raise ValueError(\"Unsloth does not support lora reward model.\")\n\n    if (\n        finetuning_args.stage == \"ppo\"\n        and training_args.report_to\n        and training_args.report_to[0] not in [\"wandb\", \"tensorboard\"]\n    ):\n        raise ValueError(\"PPO only accepts wandb or tensorboard logger.\")\n\n    if training_args.parallel_mode == ParallelMode.NOT_DISTRIBUTED:\n        raise ValueError(\"Please launch distributed training with `llamafactory-cli` or `torchrun`.\")\n\n    if training_args.deepspeed and training_args.parallel_mode != ParallelMode.DISTRIBUTED:\n        raise ValueError(\"Please use `FORCE_TORCHRUN=1` to launch DeepSpeed training.\")\n\n    if training_args.max_steps == -1 and data_args.streaming:\n        raise ValueError(\"Please specify `max_steps` in streaming mode.\")\n\n    if training_args.do_train and training_args.predict_with_generate:\n        raise ValueError(\"`predict_with_generate` cannot be set as True while training.\")\n\n    if training_args.do_train and model_args.quantization_device_map == \"auto\":\n        raise ValueError(\"Cannot use device map for quantized models in training.\")\n\n    if finetuning_args.pissa_init and is_deepspeed_zero3_enabled():\n        raise ValueError(\"PiSSA is incompatible with DeepSpeed ZeRO-3.\")\n\n    if finetuning_args.pure_bf16:\n        if not is_torch_bf16_gpu_available():\n            raise ValueError(\"This device does not support `pure_bf16`.\")\n\n        if is_deepspeed_zero3_enabled():\n            raise ValueError(\"`pure_bf16` is incompatible with DeepSpeed ZeRO-3.\")\n\n    if (\n        finetuning_args.use_galore\n        and finetuning_args.galore_layerwise\n        and training_args.parallel_mode == ParallelMode.DISTRIBUTED\n    ):\n        raise ValueError(\"Distributed training does not support layer-wise GaLore.\")\n\n    if finetuning_args.use_badam and training_args.parallel_mode == ParallelMode.DISTRIBUTED:\n        if finetuning_args.badam_mode == \"ratio\":\n            raise ValueError(\"Radio-based BAdam does not yet support distributed training, use layer-wise BAdam.\")\n        elif not is_deepspeed_zero3_enabled():\n            raise ValueError(\"Layer-wise BAdam only supports DeepSpeed ZeRO-3 training.\")\n\n    if finetuning_args.use_galore and training_args.deepspeed is not None:\n        raise ValueError(\"GaLore is incompatible with DeepSpeed yet.\")\n\n    if model_args.infer_backend == \"vllm\":\n        raise ValueError(\"vLLM backend is only available for API, CLI and Web.\")\n\n    if model_args.visual_inputs and data_args.packing:\n        raise ValueError(\"Cannot use packing in MLLM fine-tuning.\")\n\n    if model_args.use_unsloth and is_deepspeed_zero3_enabled():\n        raise ValueError(\"Unsloth is incompatible with DeepSpeed ZeRO-3.\")\n\n    if data_args.neat_packing and not data_args.packing:\n        logger.warning(\"`neat_packing` requires `packing` is True. Change it to True.\")\n        data_args.packing = True\n\n    _verify_model_args(model_args, finetuning_args)\n    _check_extra_dependencies(model_args, finetuning_args, training_args)\n\n    if (\n        training_args.do_train\n        and finetuning_args.finetuning_type == \"lora\"\n        and model_args.quantization_bit is None\n        and model_args.resize_vocab\n        and finetuning_args.additional_target is None\n    ):\n        logger.warning(\"Remember to add embedding layers to `additional_target` to make the added tokens trainable.\")\n\n    if training_args.do_train and model_args.quantization_bit is not None and (not model_args.upcast_layernorm):\n        logger.warning(\"We recommend enable `upcast_layernorm` in quantized training.\")\n\n    if training_args.do_train and (not training_args.fp16) and (not training_args.bf16):\n        logger.warning(\"We recommend enable mixed precision training.\")\n\n    if training_args.do_train and finetuning_args.use_galore and not finetuning_args.pure_bf16:\n        logger.warning(\"Using GaLore with mixed precision training may significantly increases GPU memory usage.\")\n\n    if (not training_args.do_train) and model_args.quantization_bit is not None:\n        logger.warning(\"Evaluating model in 4/8-bit mode may cause lower scores.\")\n\n    if (not training_args.do_train) and finetuning_args.stage == \"dpo\" and finetuning_args.ref_model is None:\n        logger.warning(\"Specify `ref_model` for computing rewards at evaluation.\")\n\n    \n    if (\n        training_args.parallel_mode == ParallelMode.DISTRIBUTED\n        and training_args.ddp_find_unused_parameters is None\n        and finetuning_args.finetuning_type == \"lora\"\n    ):\n        logger.warning(\"`ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\")\n        training_args.ddp_find_unused_parameters = False\n\n    if finetuning_args.stage in [\"rm\", \"ppo\"] and finetuning_args.finetuning_type in [\"full\", \"freeze\"]:\n        can_resume_from_checkpoint = False\n        if training_args.resume_from_checkpoint is not None:\n            logger.warning(\"Cannot resume from checkpoint in current stage.\")\n            training_args.resume_from_checkpoint = None\n    else:\n        can_resume_from_checkpoint = True\n\n    if (\n        training_args.resume_from_checkpoint is None\n        and training_args.do_train\n        and os.path.isdir(training_args.output_dir)\n        and not training_args.overwrite_output_dir\n        and can_resume_from_checkpoint\n    ):\n        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n        if last_checkpoint is None and any(\n            os.path.isfile(os.path.join(training_args.output_dir, name)) for name in CHECKPOINT_NAMES\n        ):\n            raise ValueError(\"Output directory already exists and is not empty. Please set `overwrite_output_dir`.\")\n\n        if last_checkpoint is not None:\n            training_args.resume_from_checkpoint = last_checkpoint\n            logger.info(\"Resuming training from {}.\".format(training_args.resume_from_checkpoint))\n            logger.info(\"Change `output_dir` or use `overwrite_output_dir` to avoid.\")\n\n    if (\n        finetuning_args.stage in [\"rm\", \"ppo\"]\n        and finetuning_args.finetuning_type == \"lora\"\n        and training_args.resume_from_checkpoint is not None\n    ):\n        logger.warning(\n            \"Add {} to `adapter_name_or_path` to resume training from checkpoint.\".format(\n                training_args.resume_from_checkpoint\n            )\n        )\n\n    \n    if training_args.bf16 or finetuning_args.pure_bf16:\n        model_args.compute_dtype = torch.bfloat16\n    elif training_args.fp16:\n        model_args.compute_dtype = torch.float16\n\n    model_args.device_map = {\"\": get_current_device()}\n    model_args.model_max_length = data_args.cutoff_len\n    model_args.block_diag_attn = data_args.neat_packing\n    data_args.packing = data_args.packing if data_args.packing is not None else finetuning_args.stage == \"pt\"\n\n    \n    logger.info(\n        \"Process rank: {}, device: {}, n_gpu: {}, distributed training: {}, compute dtype: {}\".format(\n            training_args.local_rank,\n            training_args.device,\n            training_args.n_gpu,\n            training_args.parallel_mode == ParallelMode.DISTRIBUTED,\n            str(model_args.compute_dtype),\n        )\n    )\n\n    transformers.set_seed(training_args.seed)\n\n    return model_args, data_args, training_args, finetuning_args, generating_args\n\n\ndef get_infer_args(args: Optional[Dict[str, Any]] = None) -> _INFER_CLS:\n    model_args, data_args, finetuning_args, generating_args = _parse_infer_args(args)\n\n    _set_transformers_logging()\n\n    if data_args.template is None:\n        raise ValueError(\"Please specify which `template` to use.\")\n\n    if model_args.infer_backend == \"vllm\":\n        if finetuning_args.stage != \"sft\":\n            raise ValueError(\"vLLM engine only supports auto-regressive models.\")\n\n        if model_args.quantization_bit is not None:\n            raise ValueError(\"vLLM engine does not support bnb quantization (GPTQ and AWQ are supported).\")\n\n        if model_args.rope_scaling is not None:\n            raise ValueError(\"vLLM engine does not support RoPE scaling.\")\n\n        if model_args.adapter_name_or_path is not None and len(model_args.adapter_name_or_path) != 1:\n            raise ValueError(\"vLLM only accepts a single adapter. Merge them first.\")\n\n    if finetuning_args.stage == \"rm\" and model_args.visual_inputs:\n        raise ValueError(\"Reward server does not support MLLM yet. Stay tuned.\")\n\n    _verify_model_args(model_args, finetuning_args)\n    _check_extra_dependencies(model_args, finetuning_args)\n\n    if model_args.export_dir is not None and model_args.export_device == \"cpu\":\n        model_args.device_map = {\"\": torch.device(\"cpu\")}\n        model_args.model_max_length = data_args.cutoff_len\n    else:\n        model_args.device_map = \"auto\"\n\n    return model_args, data_args, finetuning_args, generating_args\n\n\ndef get_eval_args(args: Optional[Dict[str, Any]] = None) -> _EVAL_CLS:\n    model_args, data_args, eval_args, finetuning_args = _parse_eval_args(args)\n\n    _set_transformers_logging()\n\n    if data_args.template is None:\n        raise ValueError(\"Please specify which `template` to use.\")\n\n    if model_args.infer_backend == \"vllm\":\n        raise ValueError(\"vLLM backend is only available for API, CLI and Web.\")\n\n    _verify_model_args(model_args, finetuning_args)\n    _check_extra_dependencies(model_args, finetuning_args)\n\n    model_args.device_map = \"auto\"\n\n    transformers.set_seed(eval_args.seed)\n\n    return model_args, data_args, eval_args, finetuning_args\n```"
        }
    ],
    "temperature": 0
}